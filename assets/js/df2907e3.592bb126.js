"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6577],{152:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>i,toc:()=>h});var i=t(3041),r=t(4848),s=t(8453);const a={title:"Project: Deep Q-Learning Agent",description:"Reinforcement learning agent achieving 30\xd7 higher performance in custom Gym environment",slug:"deep-q-learning-agent",tags:["Project","Python","Gymnasium","Reinforcement Learning","Deep Learning"],image:"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715"},o='Deep Q-Learning Shooter Game Project "SnowFight"',l={authorsImageUrls:[]},h=[{value:"Overview",id:"overview",level:2},{value:"Game Design",id:"game-design",level:2},{value:"Model",id:"model",level:2},{value:"Results",id:"results",level:2},{value:"Notable Behaviors (1)",id:"notable-behaviors-1",level:2},{value:"Notable Behaviors (2)",id:"notable-behaviors-2",level:2},{value:"Notable Behaviors (3)",id:"notable-behaviors-3",level:2},{value:"Future Directions",id:"future-directions",level:2}];function g(e){const n={a:"a",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/ash3327/SnowFight",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/View_Project-Deep%20Q--Learning%20Agent-4285F4?style=flat&logo=github&logoColor=white",alt:"View Project"})})," ",(0,r.jsx)(n.a,{href:"https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Report-4285F4?style=flat&logo=github&logoColor=white&link=https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf",alt:"Report"})})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://www.python.org/",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white",alt:"Python"})}),"\r\n",(0,r.jsx)(n.a,{href:"https://gymnasium.farama.org/index.html",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Gymnasium-8B9467?style=flat&logo=openai",alt:"Gymnasium"})}),"\r\n",(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Reinforcement_Learning-00BFFF?style=flat",alt:"Reinforcement Learning"}),"\r\n",(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Group_Project-FF9900?style=flat",alt:"Group Project"}),"\r\n",(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Last_Updated-December_2022-green?style=flat",alt:"Last Updated"})]}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Created a Gym environment of a simple third-person shooter game in Python"}),"\n",(0,r.jsx)(n.li,{children:"Implemented a simple Deep-Q Network with PyTorch to train agents to master at the game (left image)"}),"\n",(0,r.jsx)(n.li,{children:"Fine-tuned the hyperparameters of the agent, achieving average kill streak of 7 (right image, top) and lengthend the survival duration by 4 times (right image, bottom), which significantly better than the random baseline of 0.22 kills on average."}),"\n",(0,r.jsx)(n.li,{children:"Explored how deep-Q learning models handle a variable quantity of moving objects, i.e. the bullets and enemies, and relevant adjustments to the reward functions and representations of the observation space needed."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Report:"}),"\xa0",(0,r.jsx)(n.a,{href:"https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Report-4285F4?style=flat&logo=github&logoColor=white&link=https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf",alt:"Report"})})]}),"\n",(0,r.jsx)("img",{src:"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715",width:"300",height:"300"}),"\n",(0,r.jsx)("img",{src:"https://github.com/ash3327/ash3327/assets/86100752/9ac9a3e3-8e36-436c-bbd9-48b80c06e2d6",width:"400"}),"\n",(0,r.jsx)(n.h2,{id:"game-design",children:"Game Design"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Survive a zombie apocalypse by controlling a snowball-throwing character."}),"\n",(0,r.jsx)(n.li,{children:"Goal: Survive as long as possible while killing zombies."}),"\n",(0,r.jsx)(n.li,{children:"Player has limited vision range."}),"\n",(0,r.jsx)(n.li,{children:"Game ends when a zombie touches the player."}),"\n",(0,r.jsx)(n.li,{children:"Image: Human Gameplay; Art: Myself; AI training: Myself; Observation and Reward Design: Myself & Jerry; Game Code: Jerry & Myself"}),"\n"]}),"\n",(0,r.jsx)("img",{src:"/personalblog/img/docs/snowfight/gameplay-human-1.gif",width:"300",height:"300"}),"\n",(0,r.jsx)(n.h2,{id:"model",children:"Model"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Deep Q-Network (DQN) model for agent training."}),"\n",(0,r.jsx)(n.li,{children:"3-layer feedforward neural network in TensorFlow."}),"\n",(0,r.jsx)(n.li,{children:"Uses experience replay with batch training."}),"\n",(0,r.jsx)(n.li,{children:"Epsilon-decay strategy for interaction."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Tested multiple decay factors (gamma)."}),"\n",(0,r.jsx)(n.li,{children:"Adjusted rewards to improve agent's learning on long-term dependencies."}),"\n",(0,r.jsx)(n.li,{children:"Achieving an average kill streak of 7 and quadrupling survival time, far surpassing the random baseline of 0.22 kills on average."}),"\n"]}),"\n",(0,r.jsx)("img",{src:"/personalblog/img/docs/snowfight/results-1.png",width:"400",height:"400"}),"\n",(0,r.jsx)(n.h2,{id:"notable-behaviors-1",children:"Notable Behaviors (1)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AI learns to control its orientation for precise shooting."}),"\n",(0,r.jsx)(n.li,{children:"AI refines orientation control to improve shooting accuracy."}),"\n"]}),"\n",(0,r.jsx)("img",{src:"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715",width:"300",height:"300"}),"\n",(0,r.jsx)(n.h2,{id:"notable-behaviors-2",children:"Notable Behaviors (2)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AI learns to evade zombies by retreating to the map corner."}),"\n",(0,r.jsx)(n.li,{children:"AI retreats to a corner for better firing coverage."}),"\n"]}),"\n",(0,r.jsx)("img",{src:"/personalblog/img/docs/snowfight/results-2.gif",width:"300",height:"300"}),"\n",(0,r.jsx)(n.h2,{id:"notable-behaviors-3",children:"Notable Behaviors (3)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AI adopts a spinning and frequent shooting strategy."}),"\n",(0,r.jsx)(n.li,{children:"AI spins and shoots frequently to maximize hits."}),"\n"]}),"\n",(0,r.jsx)("img",{src:"/personalblog/img/docs/snowfight/results-3.gif",width:"300",height:"300"}),"\n",(0,r.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Further training needed due to time constraints of this project."}),"\n",(0,r.jsx)(n.li,{children:"Interest in refining rewards and exploring new mechanics in near future."}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(g,{...e})}):g(e)}},3041:e=>{e.exports=JSON.parse('{"permalink":"/personalblog/blog/deep-q-learning-agent","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-12-01-deep-q-learning-agent.md","source":"@site/blog/2022-12-01-deep-q-learning-agent.md","title":"Project: Deep Q-Learning Agent","description":"Reinforcement learning agent achieving 30\xd7 higher performance in custom Gym environment","date":"2022-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"Python","permalink":"/personalblog/blog/tags/python"},{"inline":true,"label":"Gymnasium","permalink":"/personalblog/blog/tags/gymnasium"},{"inline":true,"label":"Reinforcement Learning","permalink":"/personalblog/blog/tags/reinforcement-learning"},{"inline":true,"label":"Deep Learning","permalink":"/personalblog/blog/tags/deep-learning"}],"readingTime":1.915,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Project: Deep Q-Learning Agent","description":"Reinforcement learning agent achieving 30\xd7 higher performance in custom Gym environment","slug":"deep-q-learning-agent","tags":["Project","Python","Gymnasium","Reinforcement Learning","Deep Learning"],"image":"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715"},"unlisted":false,"prevItem":{"title":"Project: YOLO Object Tracking","permalink":"/personalblog/blog/yolo-object-tracking"},"nextItem":{"title":"Project: GAN Generation","permalink":"/personalblog/blog/gan-generation"}}')},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);