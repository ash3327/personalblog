"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[170],{1078:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/05/20/intro","metadata":{"permalink":"/personalblog/blog/2025/05/20/intro","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-05-20-intro.md","source":"@site/blog/2025-05-20-intro.md","title":"5\u6708\u898f\u5283","description":"\u7d93\u904e\u4e86\u4e8c\u6708\u5230\u4e94\u6708\u671f\u9593\u8846\u591a\u7684 Projects\u3001Reports \u7684\u56f0\u64fe\u4e4b\u5f8c\uff0c\u5230\u4e86\u4e94\u6708\u672b\u4f3c\u4e4e\u7d42\u65bc\u6709\u4e9b\u6642\u9593\u80fd\u5920\u8b93\u81ea\u5df1\u6c89\u6fb1\u4e00\u4e0b\u3001\u91cd\u6574\u4e00\u4e0b\u81ea\u5df1\u5c0d\u672a\u4f86\u7684\u898f\u5283\u3002","date":"2025-05-20T00:00:00.000Z","tags":[{"inline":true,"label":"Daily \u65e5\u5e38","permalink":"/personalblog/blog/tags/daily-\u65e5\u5e38"}],"readingTime":0.805,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"5\u6708\u898f\u5283","tags":["Daily \u65e5\u5e38"]},"unlisted":false,"nextItem":{"title":"Project: Event-Planning App \\"Oasis\\"","permalink":"/personalblog/blog/oasis-event-planning-app"}},"content":"\u7d93\u904e\u4e86\u4e8c\u6708\u5230\u4e94\u6708\u671f\u9593\u8846\u591a\u7684 Projects\u3001Reports \u7684\u56f0\u64fe\u4e4b\u5f8c\uff0c\u5230\u4e86\u4e94\u6708\u672b\u4f3c\u4e4e\u7d42\u65bc\u6709\u4e9b\u6642\u9593\u80fd\u5920\u8b93\u81ea\u5df1\u6c89\u6fb1\u4e00\u4e0b\u3001\u91cd\u6574\u4e00\u4e0b\u81ea\u5df1\u5c0d\u672a\u4f86\u7684\u898f\u5283\u3002\\r\\n\\r\\n\u672c\u8eab\u81ea\u5df1\u5c0d Reinforcement Learning \u7684\u8208\u8da3\u5c31\u6bd4\u8f03\u5927\uff0c\u4f46\u524d\u9663\u5b50\u6709\u4e9b\u9677\u5165\u4e86\u8207\u4ed6\u4eba\u7684\u6bd4\u8f03\u4e4b\u4e2d\uff0c\u53cd\u800c\u5931\u53bb\u4e86\u672c\u4f86\u5c0d\u77e5\u8b58\u7684\u71b1\u60c5\u3002\\r\\n\\r\\n\u9019\u5468\u6253\u7b97\u91cd\u6eab\u66fe\u7d93\u5927\u4e8c\u6642\u4e0a\u904e\u7684 RL\uff0c\u4e26\u91cd\u65b0\u7ffb\u65b0 SnowFight \u9019\u4e00\u500b Project\u3002\\r\\n\\r\\n\u5e0c\u671b\u80fd\u5920\u907f\u514d\u81ea\u5df1\u9032\u5165\u62d6\u5ef6\u75c7\u7684\u72c0\u614b\u4e2d\u3002\\r\\n\\r\\n---\\r\\n\\r\\n\u4eca\u5929\u66ab\u6642\u5fa9\u7fd2\u4e86 RL \u7684\u57fa\u790e\\r\\n\u95dc\u65bc Stanford, Sutton Bartol \u7684 Reinforcement Learning \u7684 Ex 1"},{"id":"oasis-event-planning-app","metadata":{"permalink":"/personalblog/blog/oasis-event-planning-app","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-06-07-oasis-event-planning-app.md","source":"@site/blog/2024-06-07-oasis-event-planning-app.md","title":"Project: Event-Planning App \\"Oasis\\"","description":"Android event planning app with robust notification system and SQL database","date":"2024-06-07T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"Java","permalink":"/personalblog/blog/tags/java"},{"inline":true,"label":"Android","permalink":"/personalblog/blog/tags/android"},{"inline":true,"label":"RoomDB","permalink":"/personalblog/blog/tags/room-db"},{"inline":true,"label":"Mobile Development","permalink":"/personalblog/blog/tags/mobile-development"}],"readingTime":1.46,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: Event-Planning App \\"Oasis\\"","description":"Android event planning app with robust notification system and SQL database","slug":"oasis-event-planning-app","tags":["Project","Java","Android","RoomDB","Mobile Development"],"image":"/img/docs/oasis/image.png"},"unlisted":false,"prevItem":{"title":"5\u6708\u898f\u5283","permalink":"/personalblog/blog/2025/05/20/intro"},"nextItem":{"title":"Project: P2P Communication App","permalink":"/personalblog/blog/p2p-communication-app"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-Event--Planning%20App%20\'Oasis\'-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/OasisPlanner/tree/development)\\r\\n\\r\\n\\r\\n# OasisPlanner \\r\\n_Calendar App Project_ (Developing Phase)\\r\\n\\r\\n> [!INFO]\\r\\n> * Still under development and the functionalities are not completed yet.\\r\\n> * Distributable version not delivered yet.\\r\\n\\r\\n## Our Goals\\r\\n\\r\\nOasis: where you find calm _in the past, present, and future._\\r\\n\\r\\nWe aim to provide a convenient and simple interface for users to effortlessly jot down and analyze their everyday productive work, without putting too much stress on yourself. \\r\\n\\r\\nWe believe:\\r\\n- Visualizing past work done can improve confidence\\r\\n- Striking a balance between work and relaxing is crucial to boosting long-term productivity\\r\\n\\r\\n## Environment Setup\\r\\n\\r\\n- **JDK**: Version 1.8 (Java 8). Download from [Oracle JDK 8](https://www.oracle.com/java/technologies/javase-jdk8-downloads.html) or use [SDKMAN!](https://sdkman.io/).\\r\\n- **Gradle**: Use the included Gradle Wrapper (v7.3.3). Run `./gradlew` (Unix) or `gradlew.bat` (Windows).\\r\\n- **OS**: Tested on Windows 11. Should work on macOS/Linux with JDK installed.\\r\\n\\r\\n## Code Maintenance\\r\\n_Most part of the features are still under development._\\r\\n- [Design Reference on Code Structure](https://github.com/ash3327/OasisPlanner/tree/development/markdowns/code_structure.md)\\r\\n\\r\\n## Design\\r\\n- [Design Document](https://github.com/ash3327/OasisPlanner/tree/development/markdowns/first_draft_design_doc.png)\\r\\n- Note: In the deisgn document, some feature designs are copied from existing apps. Such designs will ONLY BE USED AS A BASIC REFERENCE, and the final product WILL NOT include the exact same design.\\r\\n\\r\\n## Our Plan\\r\\n- Complete basic functionalities as described in our goals before September 2024.\\r\\n- We are currently working on:\\r\\n  - Basic Functionalities\\r\\n\\r\\n## Recent Updates\\r\\n- Our recent update added support to devices of Android version 13+. [16 JUN 2024]\\r\\n- Added \\"Quick Add\\" and \\"Quick Edit\\" function - users can now add new events with just one click!\\r\\n- Added UI for Home and Projects fragments.\\r\\n\\r\\n## Functionalities\\r\\n- Preview of work:\\r\\n\\r\\n  <img src=\\"https://github.com/ash3327/ash3327/assets/86100752/3548ccde-c41b-440f-af3d-4f35303066e4\\" width=\\"200\\"/>\\r\\n  <img src=\\"https://github.com/ash3327/ash3327/assets/86100752/73996de9-525e-4c91-a27d-f76b8054de93\\" width=\\"200\\"/>\\r\\n  <img src=\\"https://github.com/ash3327/ash3327/assets/86100752/43f12dbf-4ab5-45ec-9f50-6086b3f7e601\\" width=\\"200\\"/>\\r\\n  <img src=\\"https://github.com/ash3327/ash3327/assets/86100752/82471662-dfac-44f0-bbc5-c06190d2a21e\\" width=\\"200\\"/>\\r\\n\\r\\n- Relevant documents will be uploaded later."},{"id":"p2p-communication-app","metadata":{"permalink":"/personalblog/blog/p2p-communication-app","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-08-p2p-communication-app.md","source":"@site/blog/2024-04-08-p2p-communication-app.md","title":"Project: P2P Communication App","description":"Real-time audio/video streaming with optimized packet synchronization","date":"2024-04-08T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"Python","permalink":"/personalblog/blog/tags/python"},{"inline":true,"label":"Networking","permalink":"/personalblog/blog/tags/networking"},{"inline":true,"label":"Real-time Communication","permalink":"/personalblog/blog/tags/real-time-communication"},{"inline":true,"label":"P2P","permalink":"/personalblog/blog/tags/p-2-p"}],"readingTime":4.83,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: P2P Communication App","description":"Real-time audio/video streaming with optimized packet synchronization","slug":"p2p-communication-app","tags":["Project","Python","Networking","Real-time Communication","P2P"],"image":"/img//img/docs/p2p/p2p/image.png"},"unlisted":false,"prevItem":{"title":"Project: Event-Planning App \\"Oasis\\"","permalink":"/personalblog/blog/oasis-event-planning-app"},"nextItem":{"title":"Project: Vision Transformer Analysis","permalink":"/personalblog/blog/vision-transformer-analysis"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-P2P%20Communication%20App-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/Peer-to-Peer-Communication-App)\\r\\n\\r\\n# Peer-to-Peer Communication App\\r\\n\\r\\n[![GitHub](https://img.shields.io/badge/public%20repo-github-%2324292e.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ash3327/Peer-to-Peer-Communication-App)\\r\\n![Last Commit](https://img.shields.io/badge/last%20commit-Apr%202024-%23FFA500.svg?style=for-the-badge&logo=git&logoColor=white)\\r\\n\\r\\nA fork of the project in the semester 2023-24 Term 2, creating a peer-to-peer communication app supporting audio recording, waveform display and editing, and also screen share function. \\r\\n\\r\\n## Key Features\\r\\n- Peer-to-peer communication within local area network\\r\\n- Functions:\\r\\n  - Create, join and leave chat rooms\\r\\n  - Audio recording\\r\\n  - Screen sharing\\r\\n- Synchronization and handling of audio and video streams from multiple users so that they do not hear their own voices\\r\\n\\r\\n<table>\\r\\n  <tr>\\r\\n    <td>Chatroom Creation &amp; User Alias</td>\\r\\n    <td>Audio Recording</td>\\r\\n  </tr>\\r\\n  <tr>\\r\\n    <td><img src=\\"/img/docs/p2p/image-1.png\\" alt=\\"Chatroom Creation &amp; User Alias\\"/></td>\\r\\n    <td><img src=\\"/img/docs/p2p/image-2.png\\" alt=\\"Audio Recording\\"/></td>\\r\\n  </tr>\\r\\n  <tr>\\r\\n    <td colspan=\\"2\\">Screen Sharing</td>\\r\\n  </tr>\\r\\n  <tr>\\r\\n    <td colspan=\\"2\\"><img src=\\"/img/docs/p2p/image-3.png\\" alt=\\"Screen Sharing\\"/></td>\\r\\n  </tr>\\r\\n</table>\\r\\n\\r\\n## Architecture\\r\\n- Consists of ONE server and multiple clients\\r\\n- The server and clients can be runned on the same machine, or on different machines within the same local area network\\r\\n\\r\\nThe calls between server and clients, visualized:\\r\\n![](/img/docs/p2p/gui.jpeg)\\r\\n\\r\\n## Notes\\r\\n\\r\\n- Functions that allows usage over the internet are not implemented yet due to problems in port forwarding that requires extra care\\r\\n\\r\\n## Installation\\r\\n\\r\\nTo install this package, perform the following:\\r\\n\\r\\n1) Execute ```pip3 install -r requirements.txt``` in command prompt.\\r\\n\\r\\n> [!CAUTION]\\r\\n>\\r\\n> 2) To allow the usage of the audio-to-text function, you need to manually install the ffmpeg package. (Note that we only allow English transcription for now)\\r\\n>\\r\\n>    If you\'re using conda, then do ```conda install ffmpeg``` (advised).\\r\\n>\\r\\n>       \\r\\n\\r\\n<details>\\r\\n<summary>More:</summary>\\r\\n\\r\\nIf your OS is linux, then do ```sudo apt install ffmpeg```.\\r\\n\\r\\nIf your OS is windows, you can download FFMPEG from the official download page: https://ffmpeg.org/download.html#build-windows. (Personally, we used the version ffmpeg-master-latest-win64-gpl.zip provided in https://github.com/BtbN/FFmpeg-Builds/releases)\\r\\n\\r\\nIf your OS is MacOS, you can also download it from the official download page: https://ffmpeg.org/download.html#build-mac.\\r\\n\\r\\nNote that if you downloaded the executable from the website, you\'ll have to manually move it (```ffmpeg.exe``` in the unzipped ```bin``` subfolder) to your script\'s root directory (in the SAME layer as other .py files) (or add it to PATH).\\r\\n</details>\\r\\n\\r\\nNote:\\r\\n\\r\\n1) The application currently only work with computers _in the same local area network_ due to port forwarding. \\r\\n\\r\\n> [!CAUTION]\\r\\n>\\r\\n> 2) Please TURN OFF Windows Defender Firewall AND Windows Firewall (or any firewalls) in the private network before using.\\r\\n\\r\\n## Server-Side\\r\\nTo start the server, perform the following:\\r\\n\\r\\n1) Run ```chat_server.py``` by calling ```python chat_server.py --port <port>```. \\r\\n\\r\\n    - Use the token ```-h``` to get hints on the arguments.\\r\\n    - Use the token ```-l``` to show logs of all communication between server and client.\\r\\n    - Use the token ```-r``` to change the sampling frequency of all audio messages sent to and received from this server.\\r\\n    - Press ```Ctrl+C``` for the following:\\r\\n        - check the ip and port of the server, or\\r\\n        - terminate the server\\r\\n\\r\\nNotes:\\r\\n\\r\\n1) You should be able to read the server IP and the port as follows: \\r\\n`Initializing Chat Server at IP [10.13.252.5] and port [12345]`\\r\\n\\r\\n2) You can terminate the server end program and all its associated connections by pressing ```Ctrl+C``` in the command prompt.\\r\\n\\r\\n3) Sampling Frequency can be set with `-r <frequency>` (Default = 5000). It is advised to lower your sampling frequency if your computer cannot handle the default sampling frequency.\\r\\n\\r\\n## Client-Side\\r\\nTo start the client-side software, perform the following: \\r\\n\\r\\n1) Run ```chat_client.py``` by calling ```python chat_client.py --ip <ip> --port <port>```, where the IP and port can be read from the server side.\\r\\n\\r\\n    - Use the token ```-h``` to get hints on the arguments.\\r\\n    - Use the token ```-l``` to show logs of all communication between server and client.\\r\\n\\r\\n<details>\\r\\n<summary>Functionalities:</summary>\\r\\n\\r\\n1) Users can click on \'new room\' button to enter the name of the room now.\\r\\n![](/img/docs/p2p/readme_res/image-1.png)\\r\\n2) users can click on the room names to join the room directly now.\\r\\n![](/img/docs/p2p/readme_res/image.png)\\r\\n3) Users can mute/unmute his/her voices, and also quit room now.\\r\\n![](/img/docs/p2p/readme_res/image-2.png)\\r\\n\\r\\n</details>\\r\\n\\r\\n## Logs\\r\\n\\r\\n\\r\\n<details>\\r\\n<summary>Example of server side log:</summary>\\r\\n```\\r\\nInitializing Chat Server at IP [10.13.252.5] and port [12345]\\r\\nStarting server...\\r\\nAccepted request from: 10.13.252.5 port 1749\\r\\nI/list                  : 10.13.252.5 1749      : {\'action\': \'list\'}\\r\\nO/list_rooms            : 10.13.252.5 1749      : {\'rooms\': []}\\r\\nI/create                : 10.13.252.5 1749      : {\'action\': \'create\', \'room\': \'hello\'}\\r\\nO/created_room          : 10.13.252.5 1749      : {\'status\': \'ok\', \'room\': \'hello\'}\\r\\nI/list                  : 10.13.252.5 1749      : {\'action\': \'list\'}\\r\\nO/list_rooms            : 10.13.252.5 1749      : {\'rooms\': [\'hello\']}\\r\\nI/create                : 10.13.252.5 1749      : {\'action\': \'create\', \'room\': \'world\'}\\r\\nO/created_room          : 10.13.252.5 1749      : {\'status\': \'ok\', \'room\': \'world\'}\\r\\nI/list                  : 10.13.252.5 1749      : {\'action\': \'list\'}\\r\\nO/list_rooms            : 10.13.252.5 1749      : {\'rooms\': [\'hello\', \'world\']}\\r\\nI/join                  : 10.13.252.5 1749      : {\'action\': \'join\', \'room\': \'world\'}\\r\\nO/join_room             : 10.13.252.5 1749      : {\'status\': \'room already joined\', \'room\': \'world\'}\\r\\nI/exit                  : 10.13.252.5 1749      : {\'action\': \'exit\'}\\r\\nEnded request from: 10.13.252.5 port 1749\\r\\nAccepted request from: 10.13.252.5 port 1819\\r\\nI/list                  : 10.13.252.5 1819      : {\'action\': \'list\'}\\r\\nO/list_rooms            : 10.13.252.5 1819      : {\'rooms\': [\'hello\', \'world\']}\\r\\nAccepted request from: 10.13.252.5 port 1829\\r\\nI/list                  : 10.13.252.5 1829      : {\'action\': \'list\'}\\r\\nO/list_rooms            : 10.13.252.5 1829      : {\'rooms\': [\'hello\', \'world\']}\\r\\nI/exit                  : 10.13.252.5 1829      : {\'action\': \'exit\'}\\r\\nEnded request from: 10.13.252.5 port 1829\\r\\nI/exit                  : 10.13.252.5 1819      : {\'action\': \'exit\'}\\r\\nEnded request from: 10.13.252.5 port 1819\\r\\n```\\r\\n</details>\\r\\n\\r\\n<details>\\r\\n<summary>Example of client side log:</summary>\\r\\n```\\r\\nO/list                  : {\'action\': \'list\'}\\r\\nI/list_rooms            : {\'rooms\': [\'hello\', \'world\']}\\r\\nO/create                : {\'action\': \'create\', \'room\': \'room 4\'}\\r\\nO/list                  : {\'action\': \'list\'}\\r\\nI/created_room          : {\'status\': \'ok\', \'room\': \'room 4\'}\\r\\nRoom \'room 4\' created successfully.\\r\\nI/list_rooms            : {\'rooms\': [\'hello\', \'world\', \'room 4\']}\\r\\nO/join                  : {\'action\': \'join\', \'room\': \'world\'}\\r\\nI/join_room             : {\'status\': \'ok\', \'room\': \'world\'}\\r\\nJoined room \'world\' successfully.\\r\\nO/join                  : {\'action\': \'join\', \'room\': \'world\'}\\r\\nI/join_room             : {\'status\': \'room already joined\', \'room\': \'world\'}\\r\\nRoom already joined.\\r\\nO/create                : {\'action\': \'create\', \'room\': \'room 4\'}\\r\\nO/list                  : {\'action\': \'list\'}\\r\\nI/created_room          : {\'status\': \'room already exists\', \'room\': \'room 4\'}\\r\\nFailed to create room.\\r\\nI/list_rooms            : {\'rooms\': [\'hello\', \'world\', \'room 4\']}\\r\\n```\\r\\n</details>"},{"id":"vision-transformer-analysis","metadata":{"permalink":"/personalblog/blog/vision-transformer-analysis","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-01-vision-transformer-analysis.md","source":"@site/blog/2024-04-01-vision-transformer-analysis.md","title":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","date":"2024-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"PyTorch","permalink":"/personalblog/blog/tags/py-torch"},{"inline":true,"label":"Computer Vision","permalink":"/personalblog/blog/tags/computer-vision"},{"inline":true,"label":"Machine Learning","permalink":"/personalblog/blog/tags/machine-learning"},{"inline":true,"label":"Vision Transformers","permalink":"/personalblog/blog/tags/vision-transformers"}],"readingTime":3.605,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","slug":"vision-transformer-analysis","tags":["Project","PyTorch","Computer Vision","Machine Learning","Vision Transformers"],"image":"/img//img/docs/vit/vit/image4.png"},"unlisted":false,"prevItem":{"title":"Project: P2P Communication App","permalink":"/personalblog/blog/p2p-communication-app"},"nextItem":{"title":"Project: ARG Prediction with Transformers","permalink":"/personalblog/blog/arg-prediction-transformers"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/proj-vision-transformer)\\r\\n\\r\\n# Project Vision Transformer\\r\\n\\r\\n[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) \\r\\n[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/) \\r\\n[![UNet](https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&color=green)](https://arxiv.org/abs/1505.04597)\\r\\n[![ResNet](https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&color=green)](https://arxiv.org/abs/1512.03385)\\r\\n[![ViT](https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&color=green)](https://arxiv.org/abs/2010.11929)\\r\\n[![DeiT](https://img.shields.io/badge/Model-DeiT-orange?logo=github&color=orange)](https://github.com/facebookresearch/deit)\\r\\n[![T2T](https://img.shields.io/badge/Model-T2T-orange?logo=github&color=orange)](https://github.com/yitu-opensource/T2T-ViT)\\r\\n[![Dataset | CIFAR10](https://img.shields.io/badge/Dataset-CIFAR10-blue.svg)](https://www.cs.toronto.edu/~kriz/cifar.html)\\r\\n[![Dataset | STL10](https://img.shields.io/badge/Dataset-STL10-blue.svg)](https://cs.stanford.edu/~acoates/stl10/)\\r\\n[![Dataset | Cityscapes](https://img.shields.io/badge/Dataset-Cityscapes-blue.svg)](https://www.cityscapes-dataset.com/)\\r\\n![Last Update | April 2024](https://img.shields.io/badge/Last%20Update-April%202024-green.svg)\\r\\n\\r\\nThis is the final project for the course **AIST4010**. More details on the project can be found in the report. This project is done in April 2024.\\r\\n\\r\\n**Report**: [![Report](https://img.shields.io/badge/Final%20Report-blue.svg)](https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf)\\r\\n\\r\\n---\\r\\n\\r\\n## Overview\\r\\n\\r\\n### Project Goals\\r\\nThe project investigates the **generalizability of Vision Transformers (ViTs)** compared to Convolutional Neural Networks (CNNs) for **small-scale computer vision tasks**. While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets.\\r\\n\\r\\n### Key Contributions\\r\\n1. **Scalability Analysis**: Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets.\\r\\n2. **Computational Efficiency**: Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets.\\r\\n3. **Comparison of Architectures**: Implemented and trained models with similar parameter counts for fair performance evaluations.\\r\\n\\r\\nNote: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report. \\r\\n\\r\\nSections below are not generated by ChatGPT.\\r\\n\\r\\n---\\r\\n\\r\\n## Installation\\r\\n\\r\\n1. Run all commands in `commands.txt`. Ensure that the CUDA version is **\\\\<=11.x**.\\r\\n\\r\\n> [!NOTE]\\r\\n> Execute Jupyter notebooks **from the root folder** of the project to avoid import issues.\\r\\n\\r\\n---\\r\\n\\r\\n## Data Loading\\r\\n\\r\\nDownload datasets and place them in the `data/` folder. The structure should match the following diagram:\\r\\n\\r\\n<img src=\\"/img/docs/vit/image.png\\" alt=\\"drawing\\" height=\\"400\\"/> \\r\\n\\r\\n---\\r\\n\\r\\n## Models Used\\r\\n\\r\\n![alt text](/img/docs/vit/image3.png)\\r\\n\\r\\nThe models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme.\\r\\n\\r\\n## Experimental Results\\r\\n\\r\\n### Scalability Performance\\r\\n\\r\\n![alt text](/img/docs/vit/image2.png)\\r\\n\\r\\n**Findings:** Transformer-based models perform poorly on small datasets.\\r\\n\\r\\n* For models with the same input size, transformer-based models achieve significantly lower accuracy.\\r\\n* The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green).\\r\\n\\r\\n---\\r\\n\\r\\n### Computational Efficiency\\r\\n\\r\\n| Against #iterations | Against time in second-P100 |\\r\\n| --- | --- |\\r\\n| <img src=\\"/img/docs/vit/image4.png\\" width=\\"300\\" /> | <img src=\\"/img/docs/vit/image5.png\\" width=\\"300\\" /> |\\r\\n\\r\\n**Findings:** Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets.\\r\\n* Note that it is an unfair comparison if we compare all models directly since they don\'t have the same accuracy.\\r\\n* We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge.\\r\\n\\r\\n---\\r\\n\\r\\n## Image Classification Task\\r\\n\\r\\n### File Structure\\r\\n\\r\\nRelevant code and logs are located in the `support/` folder.\\r\\n\\r\\n```python\\r\\nsupport/\\r\\n\u251c\u2500 commands.txt          # Commands for running the project.\\r\\n\u251c\u2500 main_code.ipynb       # Main training code.\\r\\n\u251c\u2500 models/\\r\\n\u2502   \u251c\u2500 <id>_<dataset>_<model>_<input_size>/\\r\\n\u2502       \u251c\u2500 <epoch>/\\r\\n\u2502           \u251c\u2500 model_<timestamp>.h5          # Trained model.\\r\\n\u2502           \u251c\u2500 model_<timestamp>.h5_accs.png # Accuracy history.\\r\\n\u2502           \u251c\u2500 model_<timestamp>.h5_lrs.png  # Learning rate history.\\r\\n\u2502           \u251c\u2500 model_<timestamp>.h5_details.txt # Model details.\\r\\n\u2514\u2500 requirements.txt       # Python dependencies.\\r\\n```\\r\\n\\r\\n---\\r\\n\\r\\n### Training\\r\\n\\r\\nIn **main_code.ipynb**, users can modify the following cells, following that order:\\r\\n\\r\\n1. **Cells 1, 3, and 4** immediately below Main Function:\\r\\n   - Change batch size, image size, patch size (not changed throughout the experiment).\\r\\n   - Change data augmentation (train_transform).\\r\\n   - Change dataset and \u201cfraction\u201d (proportion of subset).\\r\\n\\r\\n2. The cell that imports `torchsummary` and those below it, before \\"Some Other Utility Function\\":\\r\\n   - Change the model used and output directory.\\r\\n   - Change `INITIAL_LR`, `DECAY`, `GAMMA`, `kwargs` (arguments for the scheduler).\\r\\n   - Change `LOAD_PATH` (if not None, then the weights `<id>.h5` will be loaded if the model matches the description).\\r\\n\\r\\n3. The cell immediately after \\"The Training\\":\\r\\n   - Change `NUM_EPOCHS` and `NUM_EPOCHS_TO_SAVE`.\\r\\n\\r\\nThen, you can watch the results in the cell that follows the cell in (3). The outputs by default are in the path `models/`.\\r\\n\\r\\n---\\r\\n\\r\\n## Image Segmentation Task\\r\\n\\r\\nThis segment explores **UNet-based architectures** for image segmentation tasks. Related code is in the `models_archive/` folder. Not part of the final report.\\r\\n\\r\\n<table>\\r\\n  <tr>\\r\\n    <th>mAP</th>\\r\\n    <th>IoU</th>\\r\\n  </tr>\\r\\n  <tr>\\r\\n    <td><img src=\\"/img/docs/vit/acc/model_1711376232.h5_accs.png\\" width=\\"300\\"/></td>\\r\\n    <td><img src=\\"/img/docs/vit/acc/model_1711376232.h5_ious.png\\" width=\\"300\\"/></td>\\r\\n  </tr>\\r\\n</table>\\r\\n\\r\\nResultant output:\\r\\n\\r\\n![](/img/docs/vit/image7.png)"},{"id":"arg-prediction-transformers","metadata":{"permalink":"/personalblog/blog/arg-prediction-transformers","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-03-01-arg-prediction-transformers.md","source":"@site/blog/2024-03-01-arg-prediction-transformers.md","title":"Project: ARG Prediction with Transformers","description":"Fine-tuned ProtTrans model for antibiotic resistance gene classification with 0.94 F-score","date":"2024-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"Transformers","permalink":"/personalblog/blog/tags/transformers"},{"inline":true,"label":"Bioinformatics","permalink":"/personalblog/blog/tags/bioinformatics"},{"inline":true,"label":"Machine Learning","permalink":"/personalblog/blog/tags/machine-learning"},{"inline":true,"label":"Protein Analysis","permalink":"/personalblog/blog/tags/protein-analysis"}],"readingTime":1.745,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Project: ARG Prediction with Transformers","description":"Fine-tuned ProtTrans model for antibiotic resistance gene classification with 0.94 F-score","slug":"arg-prediction-transformers","tags":["Project","Transformers","Bioinformatics","Machine Learning","Protein Analysis"],"image":"/img/docs/prottrans/image.png"},"unlisted":false,"prevItem":{"title":"Project: Vision Transformer Analysis","permalink":"/personalblog/blog/vision-transformer-analysis"},"nextItem":{"title":"Project: YOLO Object Tracking","permalink":"/personalblog/blog/yolo-object-tracking"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-ARG%20Prediction%20with%20Transformers-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer)\\r\\n\\r\\n# ARG Prediction with Transformers\\r\\n\\r\\nFine-tuned ProtTrans model for antibiotic resistance gene classification achieving 0.94 F-score.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n## Overview\\r\\n\\r\\n* An assignment.\\r\\n* Competition Link and Resources: https://www.kaggle.com/competitions/aist4010-spring2024-a2/leaderboard?tab=public\\r\\n\\r\\n* This repository is posted just for reference of myself.\\r\\n* Code style may not be nice if you\'re trying to use this as your own reference for learning.\\r\\n\\r\\n* Here is the report: [Report](https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer/blob/main/report.pdf)\\r\\n\\r\\n## Key Achievements\\r\\n\\r\\n- 0.94 F-score on ARG classification\\r\\n- Fine-tuned ProtTrans model\\r\\n- Robust bioinformatics pipeline\\r\\n\\r\\n## Technologies Used\\r\\n\\r\\n- Transformers\\r\\n- ProtTrans\\r\\n- Bioinformatics tools\\r\\n- Python\\r\\n\\r\\n## Procedures\\r\\n\\r\\n1. Download the dataset either by direct downloading, or through kaggle by the following:\\r\\n\\r\\n    a. Generate the Kaggle api key from kaggle / accounts / generate api key.\\r\\n    \\r\\n    b. Put the kaggle.json generated into the folder specified by the error message generated when you execute ```kaggle competitions download -c aist4010-spring2024-a2```.\\r\\n\\r\\n    c. Execute ```kaggle competitions download -c aist4010-spring2024-a2``` in command prompt and unzip the file in any manner. Make sure you unzipped it with root folder containing the directory ```aist4010-spring2024-a2/data```.\\r\\n\\r\\n    *Note: For replacement, you can also place the ```data/``` directory from the dataset under the directory ```(root)/aist4010-spring2024-a2```. You may also change the ```paths``` variable under the section ```Parameters and Settings```.\\r\\n\\r\\n2. Execute ```pip3 install -r requirements.txt``` in command prompt, and also install PyTorch that matches your needs. You may want to install PyTorch versions compatible with the CUDA and GPU you\'re using.\\r\\n\\r\\n3. Open the Jupyter notebook main.ipynb.\\r\\n\\r\\n    a. For the first time of training, you would have to prepare the embeddings by setting ```LOAD = False```. This way, the embeddings generated are placed in the directory ```(root)/cache``` or ```(root)/cache_2```. You can then load the embeddings generated by setting ```LOAD = True```.\\r\\n\\r\\n    b. You can change any parameters under the sections with header ```Parameters and Settings```, and you are NOT advised to change any code from other sections. The names of the variables in the section that users can modify should be self-explanatory.\\r\\n\\r\\n    c. Run the code.\\r\\n\\r\\n## GitHub Repository\\r\\n\\r\\nYou can find the complete source code and implementation details on [GitHub](https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer)."},{"id":"yolo-object-tracking","metadata":{"permalink":"/personalblog/blog/yolo-object-tracking","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-06-05-yolo-object-tracking.md","source":"@site/blog/2023-06-05-yolo-object-tracking.md","title":"Project: YOLO Object Tracking","description":"Improved instance tracking with custom algorithm from outputs of YOLOv8","date":"2023-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"YOLO","permalink":"/personalblog/blog/tags/yolo"},{"inline":true,"label":"Object Detection","permalink":"/personalblog/blog/tags/object-detection"},{"inline":true,"label":"Computer Vision","permalink":"/personalblog/blog/tags/computer-vision"},{"inline":true,"label":"Tracking","permalink":"/personalblog/blog/tags/tracking"}],"readingTime":2.27,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: YOLO Object Tracking","description":"Improved instance tracking with custom algorithm from outputs of YOLOv8","slug":"yolo-object-tracking","tags":["Project","YOLO","Object Detection","Computer Vision","Tracking"],"image":"/img/img/docs/yolo-1/yolo-1/vid3.gif"},"unlisted":false,"prevItem":{"title":"Project: ARG Prediction with Transformers","permalink":"/personalblog/blog/arg-prediction-transformers"},"nextItem":{"title":"Project: GAN Generation","permalink":"/personalblog/blog/gan-generation"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-YOLO%20Object%20Tracking-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/ObjectDetection-v1)\\r\\n\\r\\n\\r\\n# ObjectDetection-v1\\r\\n[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) \\r\\n[![Ultralytics](https://img.shields.io/badge/Ultralytics-00875A.svg?logo=ultralytics&logoColor=white)](https://github.com/ultralytics) \\r\\n[![YOLO](https://img.shields.io/badge/YOLO-FF69B4.svg?logo=yolo&logoColor=white)](https://github.com/ultralytics/yolov5)\\r\\n![Artificial Intelligence (AI)](https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&logoColor=white)\\r\\n![Object Detection](https://img.shields.io/badge/Object%20Detection-EE4C2C.svg?logo=object-detection&logoColor=white)\\r\\n![Last Updated](https://img.shields.io/badge/Last%20Updated-June%202023-green.svg)\\r\\n\\r\\n## Backup of Old Project (June 2023)\\r\\n\\r\\nThis is a backup of an old project that focused on object detection and tracking over videos using **YOLOv8**. The project was based on the following tutorials:\\r\\n\\r\\n- [Murtaza\'s Workshop - Robotics and AI\'s Object Detection Tutorial](https://www.youtube.com/watch?v=WgPbbWmnXJ8&ab_channel=Murtaza%27sWorkshop-RoboticsandAI)\\r\\n- [Computer Vision Zone\'s Object Detection Course](https://www.computervision.zone/courses/object-detection-course/)\\r\\n\\r\\nThe project was written in Python and uses the YOLOv8 object detection model.\\r\\n\\r\\n## Execution\\r\\n\\r\\nStart a new virtual environment:\\r\\n```bash\\r\\npython3 -m venv venv\\r\\nsource venv/bin/activate # linux or windows WSL\\r\\n.\\\\venv\\\\Scripts\\\\activate # windows cmd\\r\\n```\\r\\nThen install the requirements:\\r\\n```bash\\r\\npip install -r requirements.txt\\r\\n```\\r\\nFinally, run the jupyter notebook `Object Detection.ipynb` within the virtual environment.\\r\\n\\r\\n## About the Project / Key Features\\r\\n\\r\\n### **Static Image Detection** ![Section 3.2](https://img.shields.io/badge/Section%203.2-228B22.svg?logo=section&logoColor=white)\\r\\n- Applying YOLOv8 on static image given by path specified in the \\"Parameters\\" section.\\r\\n\\r\\n![alt text](/img/docs/yolo-1/image.png)\\r\\n\\r\\n### **Video Detection** ![Section 3.4](https://img.shields.io/badge/Section%203.4-228B22.svg?logo=section&logoColor=white)\\r\\n- Applying YOLOv8 on video given by path specified in the \\"Parameters\\" section.\\r\\n\\r\\n![alt text](/img/docs/yolo-1/vid1.gif)\\r\\n\\r\\n### **Instance Tracking with Abrewlay Sort** ![Section 3.5.1](https://img.shields.io/badge/Section%203.5.1-228B22.svg?logo=section&logoColor=white) [![Abrewlay Sort](https://img.shields.io/badge/Abrewlay%20Sort-007EC6.svg?logo=github&logoColor=white)](https://github.com/abewley/sort)\\r\\n- Instance identification with Abrewlay Sort library for tracking objects.\\r\\n\\r\\n![alt text](/img/docs/yolo-1/vid2.gif)\\r\\n\\r\\n**Notes**\\r\\n- The color of the box indicates the INSTANCE ID\\r\\n\\r\\n**Problems**\\r\\n1. **Inconsistent IDs**: Occurs under occlusion or label changes\\r\\n   - Observe that the id of the truck 457 on the rightmost lane (cyan, with label \\"car\\") changed to id 473 (purple, with label \\"truck\\")\\r\\n   - Indicates that the library cannot provide a consistent ID for the same object across frames\\r\\n2. **Multiple bounding boxes for the same object**\\r\\n   - As artifacts of the original YOLOv8 detection (due to using the nano model)\\r\\n\\r\\n### **Custom Tracking Algorithm** ![Section 3.5.2](https://img.shields.io/badge/Section%203.5.2-228B22.svg?logo=section&logoColor=white)\\r\\n- Instance identification with custom-implemented algorithm for tracking objects.\\r\\n\\r\\n![alt text](/img/docs/yolo-1/vid3.gif)\\r\\n\\r\\n**Goals**\\r\\n- Implementing a version of the object tracking algorithm that is more resistent to lost frames and flickering compared to the Abrewlay Sort library.\\r\\n\\r\\n**Features**\\r\\n1. **Label Accumulation**\\r\\n   - Can accumulate labels from previous frames.\\r\\n   - The ids (colors of the frames) of the objects are more consistent, which can be checked visually\\r\\n   - The same truck now got a consistent id (no change in color indicates that)\\r\\n2. **Bounding Box Merging**\\r\\n   - Finds the closest bounding box to the previous frames (stored in a dictionary)\\r\\n   - Dictionary is cleared after a certain number of frames\\r\\n   - Only bounding boxes within a certain change in size and aspect ratio are merged by overwritting the same instance id\\r\\n\\r\\n### **Car Counter** ![Section 3.6](https://img.shields.io/badge/Section%203.6-228B22.svg?logo=section&logoColor=white)\\r\\n\\r\\n![alt text](/img/docs/yolo-1/vid4.gif)\\r\\n\\r\\n- Simple algorithm for counting cars across the line by masking the image before detection.\\r\\n- Utilized the algorithm in sectino 3.5.2 for tracking objects to ensure that a car is not detected twice."},{"id":"gan-generation","metadata":{"permalink":"/personalblog/blog/gan-generation","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-06-01-gan-generation.md","source":"@site/blog/2023-06-01-gan-generation.md","title":"Project: GAN Generation","description":"WGAN implementation on MNIST dataset","date":"2023-06-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"PyTorch","permalink":"/personalblog/blog/tags/py-torch"},{"inline":true,"label":"GAN","permalink":"/personalblog/blog/tags/gan"},{"inline":true,"label":"Deep Learning","permalink":"/personalblog/blog/tags/deep-learning"},{"inline":true,"label":"Generative Models","permalink":"/personalblog/blog/tags/generative-models"}],"readingTime":1.135,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: GAN Generation","description":"WGAN implementation on MNIST dataset","slug":"gan-generation","tags":["Project","PyTorch","GAN","Deep Learning","Generative Models"],"image":"/img/docs/gan/v5.gif"},"unlisted":false,"prevItem":{"title":"Project: YOLO Object Tracking","permalink":"/personalblog/blog/yolo-object-tracking"},"nextItem":{"title":"Project: Deep Q-Learning Agent","permalink":"/personalblog/blog/deep-q-learning-agent"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-GAN%20Generation-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/GAN-self-learn-v1)\\r\\n\\r\\n\\r\\n# GAN Self-Learning Project\\r\\n[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/)\\r\\n[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/)\\r\\n![Generative Adversarial Networks](https://img.shields.io/badge/GAN-Generative%20Adversarial%20Networks-blueviolet.svg)\\r\\n![MNIST Dataset](https://img.shields.io/badge/Dataset-MNIST-blue.svg)\\r\\n![Last Updated: August 2022](https://img.shields.io/badge/Last%20Updated-August%202022-green.svg)\\r\\n\\r\\n## Backup of GAN Learning Project (August 2022)\\r\\n\\r\\n> [!NOTE]\\r\\n> The project explores various GAN architectures and improvements through iterative versions.\\r\\n\\r\\n> [!IMPORTANT]\\r\\n> This project is a personal learning exercise in understanding and implementing different GAN techniques.\\r\\n\\r\\n## References\\r\\n\\r\\n- The YouTube video series \\"Generative Adversarial Networks (GANs)\\", Aladdin Persson, at https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va\\r\\n\\r\\n## Setups\\r\\n**Environment**:\\r\\n- Python version: 3.x\\r\\n- Framework: PyTorch\\r\\n- Dataset: MNIST\\r\\n- Runned on Google Colab\\r\\n\\r\\n## Experiments\\r\\n### MNIST Digit Generation\\r\\nExplored various GAN architectures:\\r\\n- Standard GAN\\r\\n- Wasserstein GAN (WGAN)\\r\\n- Conditional WGAN\\r\\n\\r\\n### Key Experiments\\r\\n- Experimented with different learning rates\\r\\n- Observed the phoenomenon of mode collapse and the sensitivity of the GAN architecture to the learning rate\\r\\n- Understanding the architecture of GAN, improvements made by WGAN, and also the principles of providing class conditions to GANs\\r\\n\\r\\n### Experiment Results\\r\\n1. Vanilla GAN \\r\\n   [![Version v2](https://img.shields.io/badge/Version-v2-blue.svg)](https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208011748_GAN_mnist_v2%20good/202208011748_GAN_mnist_v2_final.ipynb)\\r\\n   [![Version v3](https://img.shields.io/badge/Version-v3-blue.svg)](https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208021155_GAN_mnist_v3%20good%2Cinterupted/202208021155_GAN_mnist_v3.ipynb)\\r\\n   [![Version v4](https://img.shields.io/badge/Version-v4-blue.svg)](https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208041411_GAN_mnist_v4%20faster%20GAN/202208031401_GAN_mnist_v4_epoch100_ed.ipynb)\\r\\n   <table>\\r\\n      <tr>\\r\\n         <th>v2 (100 epochs)</th>\\r\\n         <th>v3 (35 epochs *early stopped)</th>\\r\\n         <th>v4 (100 epochs)</th>\\r\\n      </tr>\\r\\n      <tr>\\r\\n         <td><img src=\\"/img/docs/gan/v2.png\\" width=\\"200\\" height=\\"200\\" /></td>\\r\\n         <td><img src=\\"/img/docs/gan/v3.png\\" width=\\"200\\" height=\\"200\\" /></td>\\r\\n         <td><img src=\\"/img/docs/gan/v4.gif\\" width=\\"200\\" height=\\"200\\" /></td>\\r\\n      </tr>\\r\\n   </table>\\r\\n\\r\\n2. Wasserstein GAN (WGAN) \\r\\n   [![Version v5](https://img.shields.io/badge/Version-v5-blue.svg)](https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208051901_GAN_mnist_v5_WGAN/202208051901_GAN_mnist_v5_WGAN%20epoch100.ipynb)\\r\\n   <table>\\r\\n      <tr>\\r\\n         <th>v5 (100 epochs)</th>\\r\\n      </tr>\\r\\n      <tr>\\r\\n         <td><img src=\\"/img/docs/gan/v5.gif\\" width=\\"200\\" height=\\"200\\" /></td>\\r\\n      </tr>\\r\\n   </table>\\r\\n\\r\\n3. Conditional Wasserstein GAN (incomplete)\\r\\n   [![Version v6](https://img.shields.io/badge/Version-v6-blue.svg)](https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208061306_GAN_mnist_v6_conditional%20WGAN/202208061306_GAN_mnist_v6_Conditional_WGAN.ipynb)"},{"id":"deep-q-learning-agent","metadata":{"permalink":"/personalblog/blog/deep-q-learning-agent","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-12-01-deep-q-learning-agent.md","source":"@site/blog/2022-12-01-deep-q-learning-agent.md","title":"Project: Deep Q-Learning Agent","description":"Reinforcement learning agent achieving 30\xd7 higher performance in custom Gym environment","date":"2022-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"Python","permalink":"/personalblog/blog/tags/python"},{"inline":true,"label":"Gymnasium","permalink":"/personalblog/blog/tags/gymnasium"},{"inline":true,"label":"Reinforcement Learning","permalink":"/personalblog/blog/tags/reinforcement-learning"},{"inline":true,"label":"Deep Learning","permalink":"/personalblog/blog/tags/deep-learning"}],"readingTime":1.895,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: Deep Q-Learning Agent","description":"Reinforcement learning agent achieving 30\xd7 higher performance in custom Gym environment","slug":"deep-q-learning-agent","tags":["Project","Python","Gymnasium","Reinforcement Learning","Deep Learning"],"image":"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715"},"unlisted":false,"prevItem":{"title":"Project: GAN Generation","permalink":"/personalblog/blog/gan-generation"},"nextItem":{"title":"Project: U-Net Segmentation","permalink":"/personalblog/blog/unet-segmentation"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-Deep%20Q--Learning%20Agent-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/SnowFight)\\r\\n\\r\\n\\r\\n# Deep Q-Learning Shooter Game Project \\"SnowFight\\" \\r\\n[![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)\\r\\n[![Gymnasium](https://img.shields.io/badge/Gymnasium-8B9467?style=flat&logo=openai)](https://gymnasium.farama.org/index.html)\\r\\n![Reinforcement Learning](https://img.shields.io/badge/Reinforcement_Learning-00BFFF?style=flat)\\r\\n![Group Project](https://img.shields.io/badge/Group_Project-FF9900?style=flat)\\r\\n![Last Updated](https://img.shields.io/badge/Last_Updated-December_2022-green?style=flat)\\r\\n\\r\\n## Overview\\r\\n- Created a Gym environment of a simple third-person shooter game in Python\\r\\n- Implemented a simple Deep-Q Network with PyTorch to train agents to master at the game (left image)\\r\\n- Fine-tuned the hyperparameters of the agent, achieving average kill streak of 7 (right image, top) and lengthend the survival duration by 4 times (right image, bottom), which significantly better than the random baseline of 0.22 kills on average.\\r\\n- Explored how deep-Q learning models handle a variable quantity of moving objects, i.e. the bullets and enemies, and relevant adjustments to the reward functions and representations of the observation space needed.\\r\\n\\r\\n**Report:**\xa0[![Report](https://img.shields.io/badge/Report-4285F4?style=flat&logo=github&logoColor=white&link=https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf)](https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf)\\r\\n\\r\\n<img src=\\"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715\\" width=\\"300\\" height=\\"300\\"/>\\r\\n<img src=\\"https://github.com/ash3327/ash3327/assets/86100752/9ac9a3e3-8e36-436c-bbd9-48b80c06e2d6\\" width=\\"400\\"/>\\r\\n\\r\\n## Game Design\\r\\n- Survive a zombie apocalypse by controlling a snowball-throwing character.\\r\\n- Goal: Survive as long as possible while killing zombies.\\r\\n- Player has limited vision range.\\r\\n- Game ends when a zombie touches the player.\\r\\n- Image: Human Gameplay; Art: Myself; AI training: Myself; Observation and Reward Design: Myself & Jerry; Game Code: Jerry & Myself\\r\\n<img src=\\"/img/docs/snowfight/gameplay-human-1.gif\\" width=\\"300\\" height=\\"300\\" />\\r\\n\\r\\n## Model\\r\\n- Deep Q-Network (DQN) model for agent training.\\r\\n- 3-layer feedforward neural network in TensorFlow.\\r\\n- Uses experience replay with batch training.\\r\\n- Epsilon-decay strategy for interaction.\\r\\n\\r\\n## Results\\r\\n- Tested multiple decay factors (gamma).\\r\\n- Adjusted rewards to improve agent\'s learning on long-term dependencies.\\r\\n- Achieving an average kill streak of 7 and quadrupling survival time, far surpassing the random baseline of 0.22 kills on average.\\r\\n<img src=\\"/img/docs/snowfight/results-1.png\\" width=\\"400\\" height=\\"400\\" />\\r\\n\\r\\n## Notable Behaviors (1)\\r\\n- AI learns to control its orientation for precise shooting.\\r\\n- AI refines orientation control to improve shooting accuracy.\\r\\n<img src=\\"https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715\\" width=\\"300\\" height=\\"300\\" />\\r\\n\\r\\n## Notable Behaviors (2)\\r\\n- AI learns to evade zombies by retreating to the map corner.\\r\\n- AI retreats to a corner for better firing coverage.\\r\\n<img src=\\"/img/docs/snowfight/results-2.gif\\" width=\\"300\\" height=\\"300\\" />\\r\\n\\r\\n## Notable Behaviors (3)\\r\\n- AI adopts a spinning and frequent shooting strategy.\\r\\n- AI spins and shoots frequently to maximize hits.\\r\\n<img src=\\"/img/docs/snowfight/results-3.gif\\" width=\\"300\\" height=\\"300\\" />\\r\\n\\r\\n## Future Directions\\r\\n- Further training needed due to time constraints of this project.\\r\\n- Interest in refining rewards and exploring new mechanics in near future."},{"id":"unet-segmentation","metadata":{"permalink":"/personalblog/blog/unet-segmentation","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-08-04-unet-segmentation.md","source":"@site/blog/2022-08-04-unet-segmentation.md","title":"Project: U-Net Segmentation","description":"99.55% pixel accuracy on Carvana dataset","date":"2022-08-04T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"PyTorch","permalink":"/personalblog/blog/tags/py-torch"},{"inline":true,"label":"Segmentation","permalink":"/personalblog/blog/tags/segmentation"},{"inline":true,"label":"Computer Vision","permalink":"/personalblog/blog/tags/computer-vision"},{"inline":true,"label":"Deep Learning","permalink":"/personalblog/blog/tags/deep-learning"}],"readingTime":3.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: U-Net Segmentation","description":"99.55% pixel accuracy on Carvana dataset","slug":"unet-segmentation","tags":["Project","PyTorch","Segmentation","Computer Vision","Deep Learning"],"image":"/img//img/docs/unet/unet/unet_1.png"},"unlisted":false,"prevItem":{"title":"Project: Deep Q-Learning Agent","permalink":"/personalblog/blog/deep-q-learning-agent"}},"content":"[![View Project](https://img.shields.io/badge/View_Project-U--Net%20Segmentation-4285F4?style=flat&logo=github&logoColor=white)](https://github.com/ash3327/ImageSegmentation-UNet)\\r\\n\\r\\n\\r\\n# Image Segmentation using U-Net\\r\\n[![Python](https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/) \\r\\n[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white)](https://pytorch.org/) \\r\\n![Artificial Intelligence (AI)](https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&logoColor=white)\\r\\n![Image Segmentation](https://img.shields.io/badge/Image%20Segmentation-red.svg?logo=segmentation&logoColor=white)\\r\\n[![Carvana Image Masking Challenge](https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/carvana-image-masking-challenge)\\r\\n[![Cityscapes Dataset](https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&logoColor=white)](https://www.cityscapes-dataset.com/)\\r\\n![Last Updated: December 2023](https://img.shields.io/badge/Last%20Updated-December%202023-green.svg)\\r\\n\\r\\n## Backup of Old Project (December 2023)\\r\\n\\r\\nThis is a backup of an old project focused on training a U-Net model from scratch for semantic segmentation from scratch on the Cityscapes dataset and Carvana dataset. The images are DOWNSCALED to speed up the training process for learning purposes. The model has been trained and tested with the following results:\\r\\n\\r\\n> [!WARNING]\\r\\n> If you are looking for a high-quality model, this is NOT the place. This is only a practice exercise when I was in year 2.\\r\\n\\r\\n> [!NOTE] \\r\\n> Training is done long ago and some parameters recorded, such as the number of epochs, may not be accurate\\r\\n\\r\\n> [!IMPORTANT]\\r\\n> This project is done few years ago from the point of writing this documentation. Back then, the training details are not documented properly and the jupyter notebook results are not saved for each experiment. The results are not excellent either. Sorry for the inconvenience.\\r\\n\\r\\n## Setups\\r\\n**Note**: \\r\\n- **Environment**: Python 3.10.8, PyTorch 2.1.2, CUDA 12.1\\r\\n- **Libraries**: See `requirements-lock.txt`\\r\\n- **Folder Layout**: \\r\\n  ```\\r\\n  data/\\r\\n    carvana/\\r\\n      test/\\r\\n      train/\\r\\n      train_masks/\\r\\n      val/\\r\\n      val_masks/\\r\\n    cityscapes/\\r\\n      gtFine/\\r\\n      leftImg8Bit/\\r\\n  ```\\r\\n  - Data Sources: \\r\\n    - Carvana: [![Carvana Image Masking Challenge](https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/carvana-image-masking-challenge)\\r\\n      - Download from the above link\\r\\n      - Unzip all zips and organize as described above\\r\\n    - Cityscapes: [![Cityscapes Dataset](https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&logoColor=white)](https://www.cityscapes-dataset.com/)\\r\\n      - Download the coarse dataset (images and the masks)\\r\\n      - Unzip all zips and organize as described above\\r\\n- **Models**: \\r\\n  ```bash\\r\\n  models/\\r\\n    sem_segmentation/\\r\\n      1703922437_cityscapes/model_1703922437.h5 # cityscapes\\r\\n      1703926149/model_1703926149.h5 # carvana\\r\\n  ```\\r\\n  Download from: [![Google Drive](https://img.shields.io/badge/Google%20Drive-Models-orange.svg?logo=googledrive&logoColor=white)](https://drive.google.com/drive/folders/1Mgb_YWV__zsQGNryXGvlOa2EaH49WERe)\\r\\n- **Training scripts**: Run `main_semantic_segmentation_carvana.py` and `main_semantic_segmentation_cityscape.py` as juypter notebooks.\\r\\n- **Testing scripts**: Run the `test_semantic_segmentation_carvana.py` script as jupyter notebook. Specify the paths of the h5 files in cell [8]. \\r\\n\\r\\n## Experiments\\r\\n### Carvana Dataset (Carvana Image Masking Challenge Dataset) [![Carvana Image Masking Challenge](https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/carvana-image-masking-challenge)\\r\\n\\r\\nDataset information\\r\\n* Original image dimension: 1918 x 1280\\r\\n* Training image dimension: 160 x 240 and 320 x 480\\r\\n\\r\\nValidation accuracies (highest)\\r\\n* Validation Pixel Accuracy: **0.9955**\\r\\n* Validation Dice Score: **0.9911**\\r\\n\\r\\n<h3 align=\\"center\\"> Results </h3>\\r\\n\\r\\nResults on test set: (Top: Prediction, Bottom: Reference)\\r\\n\\r\\n![alt text](/img/docs/unet/image.png)\\r\\n\\r\\nResults on train set: (Top: Prediction, Bottom: Reference)\\r\\n\\r\\n![alt text](/img/docs/unet/image2.png)\\r\\n\\r\\nInspection of intermediate layers: (In the order: Prediction, Output of Downsampling Block 1, Output of Bottleneck Block, Output of Upsampling Block 1, Reference)\\r\\n\\r\\n![alt text](/img/docs/unet/image4.png)\\r\\n\\r\\n<details>\\r\\n<summary>Details of the experiments</summary>\\r\\n\\r\\n```\\r\\nNormalization: mean 0, std 1\\r\\nAugments:\\r\\n        transforms.RandomHorizontalFlip(p=0.5),\\r\\n        transforms.RandomVerticalFlip(p=0.1),\\r\\n        transforms.RandomRotation(degrees=35),\\r\\n\\r\\n**29/12/2023 17:15: result12_**\\r\\nDownscaled image dimension: 160 x 240\\r\\nBatch size: 32\\r\\nLearning rate: 5e-7\\r\\nDecay: StepLR: step_size=5, gamma=0.85\\r\\nLoss: BCEWithLogitsLoss\\r\\nEpochs: 42\\r\\nFinal: Test accuracy: 90.9%, Avg loss: 0.403125, Test recall: 0.5831877589225769, precision: 0.980972170829773\\r\\n\\r\\n**30/12/2023 08:40: result13_**\\r\\nDownscaled image dimension: 160 x 240\\r\\nBatch size: 32\\r\\nLearning rate: 1e-4\\r\\nDecay: ReduceLROnPlateau: patience=5\\r\\nLoss: BCEWithLogitsLoss\\r\\nEpochs: 50\\r\\nFinal: Test accuracy: 81.0%, Avg loss: 0.775568, Test recall: 0.1010913997888565, precision: 0.9727051258087158\\r\\n\\r\\n**30/12/2023 11:33: result14_**\\r\\nDownscaled image dimension: 160 x 240\\r\\nBatch size: 32\\r\\nLearning rate: 1e-4\\r\\nDecay: ReduceLROnPlateau: patience=5\\r\\nLoss: BCEWithLogitsLoss\\r\\nEpochs: 100\\r\\nFinal:\\r\\nTest accuracy: 96.6%, Avg loss: 0.210099, Test recall: 0.9129086136817932, precision: 0.927651584148407\\r\\n\\r\\n**30/12/2023 15:47: 1703922437**\\r\\nDownscaled image dimension: 160 x 240\\r\\nBatch size: 16\\r\\nLearning rate: 1e-4\\r\\nDecay: None\\r\\nLoss: BCEWithLogitsLoss\\r\\nTest accuracy: 99.5%, Avg loss: 0.013175, \\r\\nTest recall: 0.9918522238731384, precision: 0.9872172474861145, dice_score: 0.9895293116569519\\r\\n\\r\\n**30/12/2023 16:49: 1703926149**\\r\\nDownscaled image dimension: 320 x 480\\r\\nBatch size: 8\\r\\nLearning rate: 1e-4\\r\\nDecay: None\\r\\nLoss: BCEWithLogitsLoss\\r\\nTest accuracy: 99.6%, Avg loss: 0.009994, \\r\\nTest recall: 0.9942919611930847, precision: 0.9879629611968994, dice_score: 0.9911173582077026\\r\\n```\\r\\n</details>\\r\\n\\r\\n-----\\r\\n\\r\\n### Cityscapes Dataset (Cityscapes Dataset) [![Cityscapes Dataset](https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&logoColor=white)](https://www.cityscapes-dataset.com/)\\r\\n\\r\\n* Test Pixel Accuracy: **0.8669**\\r\\n* The code for evaluating the mAP for the cityscape dataset has been lost. The code in this repository does not reflect the true results.\\r\\n\\r\\n```\\r\\n1704279189_cityscapes/model_1704284109.h5: \\r\\n\\r\\n**Documented:** \\r\\nTest acc: 0.8402184247970581, Test loss: 0.6274673556908965\\r\\n```"}]}}')}}]);