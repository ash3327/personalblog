"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1174],{2395:(e,s,i)=>{i.d(s,{A:()=>t});const t=i.p+"assets/images/image3-98190f450375afbd637ff97c3ff80d39.png"},4676:e=>{e.exports=JSON.parse('{"permalink":"/personalblog/blog/vision-transformer-analysis","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-01-vision-transformer-analysis.md","source":"@site/blog/2024-04-01-vision-transformer-analysis.md","title":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","date":"2024-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":true,"label":"PyTorch","permalink":"/personalblog/blog/tags/py-torch"},{"inline":true,"label":"Computer Vision","permalink":"/personalblog/blog/tags/computer-vision"},{"inline":true,"label":"Machine Learning","permalink":"/personalblog/blog/tags/machine-learning"},{"inline":true,"label":"Vision Transformers","permalink":"/personalblog/blog/tags/vision-transformers"}],"readingTime":3.605,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","slug":"vision-transformer-analysis","tags":["Project","PyTorch","Computer Vision","Machine Learning","Vision Transformers"],"image":"/img//img/docs/vit/vit/image4.png"},"unlisted":false,"prevItem":{"title":"Project: P2P Communication App","permalink":"/personalblog/blog/p2p-communication-app"},"nextItem":{"title":"Project: ARG Prediction with Transformers","permalink":"/personalblog/blog/arg-prediction-transformers"}}')},6015:(e,s,i)=>{i.d(s,{A:()=>t});const t=i.p+"assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png"},7042:(e,s,i)=>{i.d(s,{A:()=>t});const t=i.p+"assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png"},7517:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});var t=i(4676),n=i(4848),r=i(8453);const a={title:"Project: Vision Transformer Analysis",description:"Comparative study of Vision Transformers vs CNNs on small datasets",slug:"vision-transformer-analysis",tags:["Project","PyTorch","Computer Vision","Machine Learning","Vision Transformers"],image:"/img//img/docs/vit/vit/image4.png"},o="Project Vision Transformer",l={authorsImageUrls:[]},c=[{value:"Overview",id:"overview",level:2},{value:"Project Goals",id:"project-goals",level:3},{value:"Key Contributions",id:"key-contributions",level:3},{value:"Installation",id:"installation",level:2},{value:"Data Loading",id:"data-loading",level:2},{value:"Models Used",id:"models-used",level:2},{value:"Experimental Results",id:"experimental-results",level:2},{value:"Scalability Performance",id:"scalability-performance",level:3},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Image Classification Task",id:"image-classification-task",level:2},{value:"File Structure",id:"file-structure",level:3},{value:"Training",id:"training",level:3},{value:"Image Segmentation Task",id:"image-segmentation-task",level:2}];function d(e){const s={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.p,{children:(0,n.jsx)(s.a,{href:"https://github.com/ash3327/proj-vision-transformer",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&logo=github&logoColor=white",alt:"View Project"})})}),"\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.a,{href:"https://www.python.org/",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white",alt:"Python"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://pytorch.org/",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white",alt:"PyTorch"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://arxiv.org/abs/1505.04597",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&color=green",alt:"UNet"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://arxiv.org/abs/1512.03385",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&color=green",alt:"ResNet"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://arxiv.org/abs/2010.11929",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&color=green",alt:"ViT"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://github.com/facebookresearch/deit",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Model-DeiT-orange?logo=github&color=orange",alt:"DeiT"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://github.com/yitu-opensource/T2T-ViT",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Model-T2T-orange?logo=github&color=orange",alt:"T2T"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://www.cs.toronto.edu/~kriz/cifar.html",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Dataset-CIFAR10-blue.svg",alt:"Dataset | CIFAR10"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://cs.stanford.edu/~acoates/stl10/",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Dataset-STL10-blue.svg",alt:"Dataset | STL10"})}),"\r\n",(0,n.jsx)(s.a,{href:"https://www.cityscapes-dataset.com/",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Dataset-Cityscapes-blue.svg",alt:"Dataset | Cityscapes"})}),"\r\n",(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Last%20Update-April%202024-green.svg",alt:"Last Update | April 2024"})]}),"\n",(0,n.jsxs)(s.p,{children:["This is the final project for the course ",(0,n.jsx)(s.strong,{children:"AIST4010"}),". More details on the project can be found in the report. This project is done in April 2024."]}),"\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.strong,{children:"Report"}),": ",(0,n.jsx)(s.a,{href:"https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf",children:(0,n.jsx)(s.img,{src:"https://img.shields.io/badge/Final%20Report-blue.svg",alt:"Report"})})]}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h2,{id:"overview",children:"Overview"}),"\n",(0,n.jsx)(s.h3,{id:"project-goals",children:"Project Goals"}),"\n",(0,n.jsxs)(s.p,{children:["The project investigates the ",(0,n.jsx)(s.strong,{children:"generalizability of Vision Transformers (ViTs)"})," compared to Convolutional Neural Networks (CNNs) for ",(0,n.jsx)(s.strong,{children:"small-scale computer vision tasks"}),". While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets."]}),"\n",(0,n.jsx)(s.h3,{id:"key-contributions",children:"Key Contributions"}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"Scalability Analysis"}),": Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"Computational Efficiency"}),": Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"Comparison of Architectures"}),": Implemented and trained models with similar parameter counts for fair performance evaluations."]}),"\n"]}),"\n",(0,n.jsx)(s.p,{children:"Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report."}),"\n",(0,n.jsx)(s.p,{children:"Sections below are not generated by ChatGPT."}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h2,{id:"installation",children:"Installation"}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["Run all commands in ",(0,n.jsx)(s.code,{children:"commands.txt"}),". Ensure that the CUDA version is ",(0,n.jsx)(s.strong,{children:"<=11.x"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(s.blockquote,{children:["\n",(0,n.jsxs)(s.p,{children:["[!NOTE]\r\nExecute Jupyter notebooks ",(0,n.jsx)(s.strong,{children:"from the root folder"})," of the project to avoid import issues."]}),"\n"]}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h2,{id:"data-loading",children:"Data Loading"}),"\n",(0,n.jsxs)(s.p,{children:["Download datasets and place them in the ",(0,n.jsx)(s.code,{children:"data/"})," folder. The structure should match the following diagram:"]}),"\n",(0,n.jsx)("img",{src:"/img/docs/vit/image.png",alt:"drawing",height:"400"}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h2,{id:"models-used",children:"Models Used"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"alt text",src:i(2395).A+"",width:"602",height:"155"})}),"\n",(0,n.jsx)(s.p,{children:"The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme."}),"\n",(0,n.jsx)(s.h2,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,n.jsx)(s.h3,{id:"scalability-performance",children:"Scalability Performance"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"alt text",src:i(7042).A+"",width:"640",height:"480"})}),"\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.strong,{children:"Findings:"})," Transformer-based models perform poorly on small datasets."]}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"For models with the same input size, transformer-based models achieve significantly lower accuracy."}),"\n",(0,n.jsx)(s.li,{children:"The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green)."}),"\n"]}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,n.jsxs)(s.table,{children:[(0,n.jsx)(s.thead,{children:(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.th,{children:"Against #iterations"}),(0,n.jsx)(s.th,{children:"Against time in second-P100"})]})}),(0,n.jsx)(s.tbody,{children:(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:(0,n.jsx)("img",{src:"/img/docs/vit/image4.png",width:"300"})}),(0,n.jsx)(s.td,{children:(0,n.jsx)("img",{src:"/img/docs/vit/image5.png",width:"300"})})]})})]}),"\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.strong,{children:"Findings:"})," Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets."]}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Note that it is an unfair comparison if we compare all models directly since they don't have the same accuracy."}),"\n",(0,n.jsx)(s.li,{children:"We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge."}),"\n"]}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h2,{id:"image-classification-task",children:"Image Classification Task"}),"\n",(0,n.jsx)(s.h3,{id:"file-structure",children:"File Structure"}),"\n",(0,n.jsxs)(s.p,{children:["Relevant code and logs are located in the ",(0,n.jsx)(s.code,{children:"support/"})," folder."]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:"support/\r\n\u251c\u2500 commands.txt          # Commands for running the project.\r\n\u251c\u2500 main_code.ipynb       # Main training code.\r\n\u251c\u2500 models/\r\n\u2502   \u251c\u2500 <id>_<dataset>_<model>_<input_size>/\r\n\u2502       \u251c\u2500 <epoch>/\r\n\u2502           \u251c\u2500 model_<timestamp>.h5          # Trained model.\r\n\u2502           \u251c\u2500 model_<timestamp>.h5_accs.png # Accuracy history.\r\n\u2502           \u251c\u2500 model_<timestamp>.h5_lrs.png  # Learning rate history.\r\n\u2502           \u251c\u2500 model_<timestamp>.h5_details.txt # Model details.\r\n\u2514\u2500 requirements.txt       # Python dependencies.\n"})}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h3,{id:"training",children:"Training"}),"\n",(0,n.jsxs)(s.p,{children:["In ",(0,n.jsx)(s.strong,{children:"main_code.ipynb"}),", users can modify the following cells, following that order:"]}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.strong,{children:"Cells 1, 3, and 4"})," immediately below Main Function:"]}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Change batch size, image size, patch size (not changed throughout the experiment)."}),"\n",(0,n.jsx)(s.li,{children:"Change data augmentation (train_transform)."}),"\n",(0,n.jsx)(s.li,{children:"Change dataset and \u201cfraction\u201d (proportion of subset)."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{children:["\n",(0,n.jsxs)(s.p,{children:["The cell that imports ",(0,n.jsx)(s.code,{children:"torchsummary"}),' and those below it, before "Some Other Utility Function":']}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"Change the model used and output directory."}),"\n",(0,n.jsxs)(s.li,{children:["Change ",(0,n.jsx)(s.code,{children:"INITIAL_LR"}),", ",(0,n.jsx)(s.code,{children:"DECAY"}),", ",(0,n.jsx)(s.code,{children:"GAMMA"}),", ",(0,n.jsx)(s.code,{children:"kwargs"})," (arguments for the scheduler)."]}),"\n",(0,n.jsxs)(s.li,{children:["Change ",(0,n.jsx)(s.code,{children:"LOAD_PATH"})," (if not None, then the weights ",(0,n.jsx)(s.code,{children:"<id>.h5"})," will be loaded if the model matches the description)."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{children:["\n",(0,n.jsx)(s.p,{children:'The cell immediately after "The Training":'}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:["Change ",(0,n.jsx)(s.code,{children:"NUM_EPOCHS"})," and ",(0,n.jsx)(s.code,{children:"NUM_EPOCHS_TO_SAVE"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(s.p,{children:["Then, you can watch the results in the cell that follows the cell in (3). The outputs by default are in the path ",(0,n.jsx)(s.code,{children:"models/"}),"."]}),"\n",(0,n.jsx)(s.hr,{}),"\n",(0,n.jsx)(s.h2,{id:"image-segmentation-task",children:"Image Segmentation Task"}),"\n",(0,n.jsxs)(s.p,{children:["This segment explores ",(0,n.jsx)(s.strong,{children:"UNet-based architectures"})," for image segmentation tasks. Related code is in the ",(0,n.jsx)(s.code,{children:"models_archive/"})," folder. Not part of the final report."]}),"\n",(0,n.jsxs)("table",{children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("th",{children:"mAP"}),(0,n.jsx)("th",{children:"IoU"})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{children:(0,n.jsx)("img",{src:"/img/docs/vit/acc/model_1711376232.h5_accs.png",width:"300"})}),(0,n.jsx)("td",{children:(0,n.jsx)("img",{src:"/img/docs/vit/acc/model_1711376232.h5_ious.png",width:"300"})})]})]}),"\n",(0,n.jsx)(s.p,{children:"Resultant output:"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{src:i(6015).A+"",width:"630",height:"116"})})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,s,i)=>{i.d(s,{R:()=>a,x:()=>o});var t=i(6540);const n={},r=t.createContext(n);function a(e){const s=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);