"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1174],{2395:(e,s,i)=>{i.d(s,{A:()=>t});const t=i.p+"assets/images/image3-98190f450375afbd637ff97c3ff80d39.png"},4676:e=>{e.exports=JSON.parse('{"permalink":"/personalblog/blog/vision-transformer-analysis","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-01-vision-transformer-analysis.md","source":"@site/blog/2024-04-01-vision-transformer-analysis.md","title":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","date":"2024-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"Project","permalink":"/personalblog/blog/tags/project"},{"inline":false,"label":"PyTorch","permalink":"/personalblog/blog/tags/pytorch"},{"inline":true,"label":"Computer Vision","permalink":"/personalblog/blog/tags/computer-vision"},{"inline":true,"label":"Machine Learning","permalink":"/personalblog/blog/tags/machine-learning"},{"inline":true,"label":"Vision Transformers","permalink":"/personalblog/blog/tags/vision-transformers"}],"readingTime":3.655,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","slug":"vision-transformer-analysis","tags":["Project","PyTorch","Computer Vision","Machine Learning","Vision Transformers"],"image":"/img//img/docs/vit/vit/image4.png"},"unlisted":false,"prevItem":{"title":"Project: P2P Communication App","permalink":"/personalblog/blog/p2p-communication-app"},"nextItem":{"title":"Project: ARG Prediction with Transformers","permalink":"/personalblog/blog/arg-prediction-transformers"}}')},6015:(e,s,i)=>{i.d(s,{A:()=>t});const t=i.p+"assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png"},7042:(e,s,i)=>{i.d(s,{A:()=>t});const t=i.p+"assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png"},7517:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>c});var t=i(4676),r=i(4848),n=i(8453);const o={title:"Project: Vision Transformer Analysis",description:"Comparative study of Vision Transformers vs CNNs on small datasets",slug:"vision-transformer-analysis",tags:["Project","PyTorch","Computer Vision","Machine Learning","Vision Transformers"],image:"/img//img/docs/vit/vit/image4.png"},a="Project Vision Transformer",l={authorsImageUrls:[]},c=[{value:"Overview",id:"overview",level:2},{value:"Project Goals",id:"project-goals",level:3},{value:"Key Contributions",id:"key-contributions",level:3},{value:"Models Used",id:"models-used",level:2},{value:"Experimental Results",id:"experimental-results",level:2},{value:"Scalability Performance",id:"scalability-performance",level:3},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Image Segmentation Task",id:"image-segmentation-task",level:2}];function h(e){const s={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.a,{href:"https://github.com/ash3327/proj-vision-transformer",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=for-the-badge&logo=github&logoColor=white",alt:"View Project"})})," ",(0,r.jsx)(s.a,{href:"https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Final%20Report-blue.svg?style=for-the-badge",alt:"Report"})})]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.a,{href:"https://www.python.org/",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Python-3776AB.svg?logo=python&logoColor=white",alt:"Python"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://pytorch.org/",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&logoColor=white",alt:"PyTorch"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://arxiv.org/abs/1505.04597",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&color=green",alt:"UNet"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://arxiv.org/abs/1512.03385",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&color=green",alt:"ResNet"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://arxiv.org/abs/2010.11929",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&color=green",alt:"ViT"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://github.com/facebookresearch/deit",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Model-DeiT-orange?logo=github&color=orange",alt:"DeiT"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://github.com/yitu-opensource/T2T-ViT",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Model-T2T-orange?logo=github&color=orange",alt:"T2T"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://www.cs.toronto.edu/~kriz/cifar.html",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Dataset-CIFAR10-blue.svg",alt:"Dataset | CIFAR10"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://cs.stanford.edu/~acoates/stl10/",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Dataset-STL10-blue.svg",alt:"Dataset | STL10"})}),"\r\n",(0,r.jsx)(s.a,{href:"https://www.cityscapes-dataset.com/",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Dataset-Cityscapes-blue.svg",alt:"Dataset | Cityscapes"})}),"\r\n",(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Last%20Update-April%202024-green.svg",alt:"Last Update | April 2024"})]}),"\n",(0,r.jsxs)(s.p,{children:["This is the final project for the course ",(0,r.jsx)(s.strong,{children:"AIST4010"}),". More details on the project can be found in the report. This project is done in April 2024."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Report"}),": ",(0,r.jsx)(s.a,{href:"https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf",children:(0,r.jsx)(s.img,{src:"https://img.shields.io/badge/Final%20Report-blue.svg",alt:"Report"})})]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(s.h3,{id:"project-goals",children:"Project Goals"}),"\n",(0,r.jsxs)(s.p,{children:["The project investigates the ",(0,r.jsx)(s.strong,{children:"generalizability of Vision Transformers (ViTs)"})," compared to Convolutional Neural Networks (CNNs) for ",(0,r.jsx)(s.strong,{children:"small-scale computer vision tasks"}),". While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets."]}),"\n",(0,r.jsx)(s.h3,{id:"key-contributions",children:"Key Contributions"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scalability Analysis"}),": Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Computational Efficiency"}),": Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Comparison of Architectures"}),": Implemented and trained models with similar parameter counts for fair performance evaluations."]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report."}),"\n",(0,r.jsx)(s.p,{children:"Sections below are not generated by ChatGPT."}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"models-used",children:"Models Used"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"alt text",src:i(2395).A+"",width:"602",height:"155"})}),"\n",(0,r.jsx)(s.p,{children:"The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme."}),"\n",(0,r.jsx)(s.h2,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,r.jsx)(s.h3,{id:"scalability-performance",children:"Scalability Performance"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{alt:"alt text",src:i(7042).A+"",width:"640",height:"480"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Findings:"})," Transformer-based models perform poorly on small datasets."]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"For models with the same input size, transformer-based models achieve significantly lower accuracy."}),"\n",(0,r.jsx)(s.li,{children:"The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green)."}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:"Against #iterations"}),(0,r.jsx)(s.th,{children:"Against time in second-P100"})]})}),(0,r.jsx)(s.tbody,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)("img",{src:"/personalblog/img/docs/vit/image4.png",width:"300"})}),(0,r.jsx)(s.td,{children:(0,r.jsx)("img",{src:"/personalblog/img/docs/vit/image5.png",width:"300"})})]})})]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Findings:"})," Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets."]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Note that it is an unfair comparison if we compare all models directly since they don't have the same accuracy."}),"\n",(0,r.jsx)(s.li,{children:"We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge."}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"image-segmentation-task",children:"Image Segmentation Task"}),"\n",(0,r.jsxs)(s.p,{children:["This segment explores ",(0,r.jsx)(s.strong,{children:"UNet-based architectures"})," for image segmentation tasks. Related code is in the ",(0,r.jsx)(s.code,{children:"models_archive/"})," folder. Not part of the final report."]}),"\n",(0,r.jsxs)("table",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"mAP"}),(0,r.jsx)("th",{children:"IoU"})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)("img",{src:"/personalblog/img/docs/vit/acc/model_1711376232.h5_accs.png",width:"300"})}),(0,r.jsx)("td",{children:(0,r.jsx)("img",{src:"/personalblog/img/docs/vit/acc/model_1711376232.h5_ious.png",width:"300"})})]})]}),"\n",(0,r.jsx)(s.p,{children:"Resultant output:"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:i(6015).A+"",width:"630",height:"116"})})]})}function d(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,s,i)=>{i.d(s,{R:()=>o,x:()=>a});var t=i(6540);const r={},n=t.createContext(r);function o(e){const s=t.useContext(n);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(n.Provider,{value:s},e.children)}}}]);