<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">3 posts tagged with &quot;PyTorch&quot; | Sam&#x27;s Portfolio</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ash3327.github.io/personalblog/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ash3327.github.io/personalblog/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ash3327.github.io/personalblog/blog/tags/py-torch"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="3 posts tagged with &quot;PyTorch&quot; | Sam&#x27;s Portfolio"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/personalblog/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ash3327.github.io/personalblog/blog/tags/py-torch"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/tags/py-torch" hreflang="en"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/tags/py-torch" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/personalblog/blog/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog/atom.xml" title="Sam&#39;s Portfolio Atom Feed">




<link rel="alternate" type="application/rss+xml" href="/personalblog/blog_old/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog_old/atom.xml" title="Sam&#39;s Portfolio Atom Feed">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"><link rel="stylesheet" href="/personalblog/assets/css/styles.40526b60.css">
<script src="/personalblog/assets/js/runtime~main.a7eecdbf.js" defer="defer"></script>
<script src="/personalblog/assets/js/main.6c70eedc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/docs/vit/image.png"><link rel="preload" as="image" href="/img/docs/vit/image4.png"><link rel="preload" as="image" href="/img/docs/vit/image5.png"><link rel="preload" as="image" href="/img/docs/vit/acc/model_1711376232.h5_accs.png"><link rel="preload" as="image" href="/img/docs/vit/acc/model_1711376232.h5_ious.png"><link rel="preload" as="image" href="/img/docs/gan/v2.png"><link rel="preload" as="image" href="/img/docs/gan/v3.png"><link rel="preload" as="image" href="/img/docs/gan/v4.gif"><link rel="preload" as="image" href="/img/docs/gan/v5.gif"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/personalblog/"><b class="navbar__title text--truncate">SamKHT</b></a><a class="navbar__item navbar__link" href="/personalblog/life-in-weeks">Life in Weeks 生命倒計時</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/personalblog/blog">Blog 博客</a><a class="navbar__item navbar__link" href="/personalblog/docs/base/intro">Prog 編程</a><a class="navbar__item navbar__link" href="/personalblog/docs/algo/intro/">Algo 算法</a><a class="navbar__item navbar__link" href="/personalblog/docs/ai/intro">AI 人工智能</a><a class="navbar__item navbar__link" href="/personalblog/docs/interests/intro">Interests 興趣</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ash3327" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-github fa-brands fa-xl" aria-label="GitHub"></a><a href="https://linkedin.com/in/khtam-51a008256" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-linkedin fa-brands fa-xl" aria-label="LinkedIn"></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/2025/05/20/intro">5月規劃</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/oasis-event-planning-app">Project: Event-Planning App &quot;Oasis&quot;</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/p2p-communication-app">Project: P2P Communication App</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/vision-transformer-analysis">Project: Vision Transformer Analysis</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/arg-prediction-transformers">Project: ARG Prediction with Transformers</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/yolo-object-tracking">Project: YOLO Object Tracking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/gan-generation">Project: GAN Generation</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/deep-q-learning-agent">Project: Deep Q-Learning Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/unet-segmentation">Project: U-Net Segmentation</a></li></ul></div></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>3 posts tagged with &quot;PyTorch&quot;</h1><a href="/personalblog/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/vision-transformer-analysis">Project: Vision Transformer Analysis</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-04-01T00:00:00.000Z">April 1, 2024</time> · <!-- -->4 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/proj-vision-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&amp;color=green" alt="UNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&amp;color=green" alt="ResNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&amp;color=green" alt="ViT" class="img_ev3q"></a>
<a href="https://github.com/facebookresearch/deit" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-DeiT-orange?logo=github&amp;color=orange" alt="DeiT" class="img_ev3q"></a>
<a href="https://github.com/yitu-opensource/T2T-ViT" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-T2T-orange?logo=github&amp;color=orange" alt="T2T" class="img_ev3q"></a>
<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-CIFAR10-blue.svg" alt="Dataset | CIFAR10" class="img_ev3q"></a>
<a href="https://cs.stanford.edu/~acoates/stl10/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-STL10-blue.svg" alt="Dataset | STL10" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes-blue.svg" alt="Dataset | Cityscapes" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Update-April%202024-green.svg" alt="Last Update | April 2024" class="img_ev3q"></p>
<p>This is the final project for the course <strong>AIST4010</strong>. More details on the project can be found in the report. This project is done in April 2024.</p>
<p><strong>Report</strong>: <a href="https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Final%20Report-blue.svg" alt="Report" class="img_ev3q"></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-goals">Project Goals<a href="#project-goals" class="hash-link" aria-label="Direct link to Project Goals" title="Direct link to Project Goals">​</a></h3>
<p>The project investigates the <strong>generalizability of Vision Transformers (ViTs)</strong> compared to Convolutional Neural Networks (CNNs) for <strong>small-scale computer vision tasks</strong>. While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-contributions">Key Contributions<a href="#key-contributions" class="hash-link" aria-label="Direct link to Key Contributions" title="Direct link to Key Contributions">​</a></h3>
<ol>
<li><strong>Scalability Analysis</strong>: Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets.</li>
<li><strong>Computational Efficiency</strong>: Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets.</li>
<li><strong>Comparison of Architectures</strong>: Implemented and trained models with similar parameter counts for fair performance evaluations.</li>
</ol>
<p>Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report.</p>
<p>Sections below are not generated by ChatGPT.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="installation">Installation<a href="#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h2>
<ol>
<li>Run all commands in <code>commands.txt</code>. Ensure that the CUDA version is <strong>&lt;=11.x</strong>.</li>
</ol>
<blockquote>
<p>[!NOTE]
Execute Jupyter notebooks <strong>from the root folder</strong> of the project to avoid import issues.</p>
</blockquote>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-loading">Data Loading<a href="#data-loading" class="hash-link" aria-label="Direct link to Data Loading" title="Direct link to Data Loading">​</a></h2>
<p>Download datasets and place them in the <code>data/</code> folder. The structure should match the following diagram:</p>
<img src="/img/docs/vit/image.png" alt="drawing" height="400">
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="models-used">Models Used<a href="#models-used" class="hash-link" aria-label="Direct link to Models Used" title="Direct link to Models Used">​</a></h2>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image3-98190f450375afbd637ff97c3ff80d39.png" width="602" height="155" class="img_ev3q"></p>
<p>The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-results">Experimental Results<a href="#experimental-results" class="hash-link" aria-label="Direct link to Experimental Results" title="Direct link to Experimental Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalability-performance">Scalability Performance<a href="#scalability-performance" class="hash-link" aria-label="Direct link to Scalability Performance" title="Direct link to Scalability Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png" width="640" height="480" class="img_ev3q"></p>
<p><strong>Findings:</strong> Transformer-based models perform poorly on small datasets.</p>
<ul>
<li>For models with the same input size, transformer-based models achieve significantly lower accuracy.</li>
<li>The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green).</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="computational-efficiency">Computational Efficiency<a href="#computational-efficiency" class="hash-link" aria-label="Direct link to Computational Efficiency" title="Direct link to Computational Efficiency">​</a></h3>
<table><thead><tr><th>Against #iterations</th><th>Against time in second-P100</th></tr></thead><tbody><tr><td><img src="/img/docs/vit/image4.png" width="300"></td><td><img src="/img/docs/vit/image5.png" width="300"></td></tr></tbody></table>
<p><strong>Findings:</strong> Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets.</p>
<ul>
<li>Note that it is an unfair comparison if we compare all models directly since they don&#x27;t have the same accuracy.</li>
<li>We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-task">Image Classification Task<a href="#image-classification-task" class="hash-link" aria-label="Direct link to Image Classification Task" title="Direct link to Image Classification Task">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="file-structure">File Structure<a href="#file-structure" class="hash-link" aria-label="Direct link to File Structure" title="Direct link to File Structure">​</a></h3>
<p>Relevant code and logs are located in the <code>support/</code> folder.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">support</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ commands</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Commands for running the project.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ main_code</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ipynb       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Main training code.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ models</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├─ </span><span class="token operator">&lt;</span><span class="token builtin" style="color:rgb(189, 147, 249)">id</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">dataset</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">model</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">input_size</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├─ </span><span class="token operator">&lt;</span><span class="token plain">epoch</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Trained model.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_accs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png </span><span class="token comment" style="color:rgb(98, 114, 164)"># Accuracy history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_lrs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Learning rate history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_details</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt </span><span class="token comment" style="color:rgb(98, 114, 164)"># Model details.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└─ requirements</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Python dependencies.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h3>
<p>In <strong>main_code.ipynb</strong>, users can modify the following cells, following that order:</p>
<ol>
<li>
<p><strong>Cells 1, 3, and 4</strong> immediately below Main Function:</p>
<ul>
<li>Change batch size, image size, patch size (not changed throughout the experiment).</li>
<li>Change data augmentation (train_transform).</li>
<li>Change dataset and “fraction” (proportion of subset).</li>
</ul>
</li>
<li>
<p>The cell that imports <code>torchsummary</code> and those below it, before &quot;Some Other Utility Function&quot;:</p>
<ul>
<li>Change the model used and output directory.</li>
<li>Change <code>INITIAL_LR</code>, <code>DECAY</code>, <code>GAMMA</code>, <code>kwargs</code> (arguments for the scheduler).</li>
<li>Change <code>LOAD_PATH</code> (if not None, then the weights <code>&lt;id&gt;.h5</code> will be loaded if the model matches the description).</li>
</ul>
</li>
<li>
<p>The cell immediately after &quot;The Training&quot;:</p>
<ul>
<li>Change <code>NUM_EPOCHS</code> and <code>NUM_EPOCHS_TO_SAVE</code>.</li>
</ul>
</li>
</ol>
<p>Then, you can watch the results in the cell that follows the cell in (3). The outputs by default are in the path <code>models/</code>.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-task">Image Segmentation Task<a href="#image-segmentation-task" class="hash-link" aria-label="Direct link to Image Segmentation Task" title="Direct link to Image Segmentation Task">​</a></h2>
<p>This segment explores <strong>UNet-based architectures</strong> for image segmentation tasks. Related code is in the <code>models_archive/</code> folder. Not part of the final report.</p>
<table><tr><th>mAP</th><th>IoU</th></tr><tr><td><img src="/img/docs/vit/acc/model_1711376232.h5_accs.png" width="300"></td><td><img src="/img/docs/vit/acc/model_1711376232.h5_ious.png" width="300"></td></tr></table>
<p>Resultant output:</p>
<p><img decoding="async" loading="lazy" src="/personalblog/assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png" width="630" height="116" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/machine-learning">Machine Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/vision-transformers">Vision Transformers</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/gan-generation">Project: GAN Generation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-06-01T00:00:00.000Z">June 1, 2023</time> · <!-- -->2 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/GAN-self-learn-v1" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-GAN%20Generation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/GAN-Generative%20Adversarial%20Networks-blueviolet.svg" alt="Generative Adversarial Networks" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-MNIST-blue.svg" alt="MNIST Dataset" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-August%202022-green.svg" alt="Last Updated: August 2022" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-gan-learning-project-august-2022">Backup of GAN Learning Project (August 2022)<a href="#backup-of-gan-learning-project-august-2022" class="hash-link" aria-label="Direct link to Backup of GAN Learning Project (August 2022)" title="Direct link to Backup of GAN Learning Project (August 2022)">​</a></h2>
<blockquote>
<p>[!NOTE]
The project explores various GAN architectures and improvements through iterative versions.</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is a personal learning exercise in understanding and implementing different GAN techniques.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2>
<ul>
<li>The YouTube video series &quot;Generative Adversarial Networks (GANs)&quot;, Aladdin Persson, at <a href="https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va" target="_blank" rel="noopener noreferrer">https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setups">Setups<a href="#setups" class="hash-link" aria-label="Direct link to Setups" title="Direct link to Setups">​</a></h2>
<p><strong>Environment</strong>:</p>
<ul>
<li>Python version: 3.x</li>
<li>Framework: PyTorch</li>
<li>Dataset: MNIST</li>
<li>Runned on Google Colab</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mnist-digit-generation">MNIST Digit Generation<a href="#mnist-digit-generation" class="hash-link" aria-label="Direct link to MNIST Digit Generation" title="Direct link to MNIST Digit Generation">​</a></h3>
<p>Explored various GAN architectures:</p>
<ul>
<li>Standard GAN</li>
<li>Wasserstein GAN (WGAN)</li>
<li>Conditional WGAN</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-experiments">Key Experiments<a href="#key-experiments" class="hash-link" aria-label="Direct link to Key Experiments" title="Direct link to Key Experiments">​</a></h3>
<ul>
<li>Experimented with different learning rates</li>
<li>Observed the phoenomenon of mode collapse and the sensitivity of the GAN architecture to the learning rate</li>
<li>Understanding the architecture of GAN, improvements made by WGAN, and also the principles of providing class conditions to GANs</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-results">Experiment Results<a href="#experiment-results" class="hash-link" aria-label="Direct link to Experiment Results" title="Direct link to Experiment Results">​</a></h3>
<ol>
<li>
<p>Vanilla GAN
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208011748_GAN_mnist_v2%20good/202208011748_GAN_mnist_v2_final.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v2-blue.svg" alt="Version v2" class="img_ev3q"></a>
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208021155_GAN_mnist_v3%20good%2Cinterupted/202208021155_GAN_mnist_v3.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v3-blue.svg" alt="Version v3" class="img_ev3q"></a>
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208041411_GAN_mnist_v4%20faster%20GAN/202208031401_GAN_mnist_v4_epoch100_ed.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v4-blue.svg" alt="Version v4" class="img_ev3q"></a></p>
<table><tr><th>v2 (100 epochs)</th><th>v3 (35 epochs *early stopped)</th><th>v4 (100 epochs)</th></tr><tr><td><img src="/img/docs/gan/v2.png" width="200" height="200"></td><td><img src="/img/docs/gan/v3.png" width="200" height="200"></td><td><img src="/img/docs/gan/v4.gif" width="200" height="200"></td></tr></table>
</li>
<li>
<p>Wasserstein GAN (WGAN)
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208051901_GAN_mnist_v5_WGAN/202208051901_GAN_mnist_v5_WGAN%20epoch100.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v5-blue.svg" alt="Version v5" class="img_ev3q"></a></p>
<table><tr><th>v5 (100 epochs)</th></tr><tr><td><img src="/img/docs/gan/v5.gif" width="200" height="200"></td></tr></table>
</li>
<li>
<p>Conditional Wasserstein GAN (incomplete)
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208061306_GAN_mnist_v6_conditional%20WGAN/202208061306_GAN_mnist_v6_Conditional_WGAN.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v6-blue.svg" alt="Version v6" class="img_ev3q"></a></p>
</li>
</ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/gan">GAN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/deep-learning">Deep Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/generative-models">Generative Models</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/unet-segmentation">Project: U-Net Segmentation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-08-04T00:00:00.000Z">August 4, 2022</time> · <!-- -->4 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/ImageSegmentation-UNet" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-U--Net%20Segmentation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Image%20Segmentation-red.svg?logo=segmentation&amp;logoColor=white" alt="Image Segmentation" class="img_ev3q">
<a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-December%202023-green.svg" alt="Last Updated: December 2023" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-december-2023">Backup of Old Project (December 2023)<a href="#backup-of-old-project-december-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (December 2023)" title="Direct link to Backup of Old Project (December 2023)">​</a></h2>
<p>This is a backup of an old project focused on training a U-Net model from scratch for semantic segmentation from scratch on the Cityscapes dataset and Carvana dataset. The images are DOWNSCALED to speed up the training process for learning purposes. The model has been trained and tested with the following results:</p>
<blockquote>
<p>[!WARNING]
If you are looking for a high-quality model, this is NOT the place. This is only a practice exercise when I was in year 2.</p>
</blockquote>
<blockquote>
<p>[!NOTE]
Training is done long ago and some parameters recorded, such as the number of epochs, may not be accurate</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is done few years ago from the point of writing this documentation. Back then, the training details are not documented properly and the jupyter notebook results are not saved for each experiment. The results are not excellent either. Sorry for the inconvenience.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setups">Setups<a href="#setups" class="hash-link" aria-label="Direct link to Setups" title="Direct link to Setups">​</a></h2>
<p><strong>Note</strong>:</p>
<ul>
<li><strong>Environment</strong>: Python 3.10.8, PyTorch 2.1.2, CUDA 12.1</li>
<li><strong>Libraries</strong>: See <code>requirements-lock.txt</code></li>
<li><strong>Folder Layout</strong>:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">data/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  carvana/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    test/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train_masks/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val_masks/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  cityscapes/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gtFine/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    leftImg8Bit/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Data Sources:<!-- -->
<ul>
<li>Carvana: <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<ul>
<li>Download from the above link</li>
<li>Unzip all zips and organize as described above</li>
</ul>
</li>
<li>Cityscapes: <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<ul>
<li>Download the coarse dataset (images and the masks)</li>
<li>Unzip all zips and organize as described above</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Models</strong>:<!-- -->
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">models/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  sem_segmentation/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    1703922437_cityscapes/model_1703922437.h5 # cityscapes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    1703926149/model_1703926149.h5 # carvana</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Download from: <a href="https://drive.google.com/drive/folders/1Mgb_YWV__zsQGNryXGvlOa2EaH49WERe" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Google%20Drive-Models-orange.svg?logo=googledrive&amp;logoColor=white" alt="Google Drive" class="img_ev3q"></a></li>
<li><strong>Training scripts</strong>: Run <code>main_semantic_segmentation_carvana.py</code> and <code>main_semantic_segmentation_cityscape.py</code> as juypter notebooks.</li>
<li><strong>Testing scripts</strong>: Run the <code>test_semantic_segmentation_carvana.py</code> script as jupyter notebook. Specify the paths of the h5 files in cell [8].</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">Carvana Dataset (Carvana Image Masking Challenge Dataset) <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a><a href="#carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" class="hash-link" aria-label="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" title="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">​</a></h3>
<p>Dataset information</p>
<ul>
<li>Original image dimension: 1918 x 1280</li>
<li>Training image dimension: 160 x 240 and 320 x 480</li>
</ul>
<p>Validation accuracies (highest)</p>
<ul>
<li>Validation Pixel Accuracy: <strong>0.9955</strong></li>
<li>Validation Dice Score: <strong>0.9911</strong></li>
</ul>
<h3 align="center"> Results </h3>
<p>Results on test set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image-041267cdfd77e346a9d0e0d7d422bb48.png" width="520" height="341" class="img_ev3q"></p>
<p>Results on train set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image2-78057b4210e74b5ab7ccbaae64eb9529.png" width="518" height="341" class="img_ev3q"></p>
<p>Inspection of intermediate layers: (In the order: Prediction, Output of Downsampling Block 1, Output of Bottleneck Block, Output of Upsampling Block 1, Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image4-341eb0e84099144c4c517f3738e0f883.png" width="1008" height="501" class="img_ev3q"></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Details of the experiments</summary><div><div class="collapsibleContent_i85q"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Normalization: mean 0, std 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Augments:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomHorizontalFlip(p=0.5),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomVerticalFlip(p=0.1),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomRotation(degrees=35),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**29/12/2023 17:15: result12_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 5e-7</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: StepLR: step_size=5, gamma=0.85</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 42</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 90.9%, Avg loss: 0.403125, Test recall: 0.5831877589225769, precision: 0.980972170829773</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 08:40: result13_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 50</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 81.0%, Avg loss: 0.775568, Test recall: 0.1010913997888565, precision: 0.9727051258087158</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 11:33: result14_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 96.6%, Avg loss: 0.210099, Test recall: 0.9129086136817932, precision: 0.927651584148407</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 15:47: 1703922437**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 16</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.5%, Avg loss: 0.013175, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9918522238731384, precision: 0.9872172474861145, dice_score: 0.9895293116569519</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 16:49: 1703926149**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 320 x 480</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.6%, Avg loss: 0.009994, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9942919611930847, precision: 0.9879629611968994, dice_score: 0.9911173582077026</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">Cityscapes Dataset (Cityscapes Dataset) <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a><a href="#cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" class="hash-link" aria-label="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" title="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">​</a></h3>
<ul>
<li>Test Pixel Accuracy: <strong>0.8669</strong></li>
<li>The code for evaluating the mAP for the cityscape dataset has been lost. The code in this repository does not reflect the true results.</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">1704279189_cityscapes/model_1704284109.h5: </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**Documented:** </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test acc: 0.8402184247970581, Test loss: 0.6274673556908965</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/segmentation">Segmentation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/deep-learning">Deep Learning</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Sam K. H. Tam. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>