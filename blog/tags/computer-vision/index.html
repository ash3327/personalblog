<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">4 posts tagged with &quot;Computer Vision&quot; | Sam&#x27;s Portfolio</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ash3327.github.io/personalblog/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ash3327.github.io/personalblog/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ash3327.github.io/personalblog/blog/tags/computer-vision"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="4 posts tagged with &quot;Computer Vision&quot; | Sam&#x27;s Portfolio"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/personalblog/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ash3327.github.io/personalblog/blog/tags/computer-vision"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/tags/computer-vision" hreflang="en"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/tags/computer-vision" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/personalblog/blog/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog/atom.xml" title="Sam&#39;s Portfolio Atom Feed">




<link rel="alternate" type="application/rss+xml" href="/personalblog/blog_old/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog_old/atom.xml" title="Sam&#39;s Portfolio Atom Feed">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"><link rel="stylesheet" href="/personalblog/assets/css/styles.cbb2b67f.css">
<script src="/personalblog/assets/js/runtime~main.e9ffb867.js" defer="defer"></script>
<script src="/personalblog/assets/js/main.619feed8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/personalblog/img/docs/vit/image.png"><link rel="preload" as="image" href="/personalblog/img/docs/vit/image4.png"><link rel="preload" as="image" href="/personalblog/img/docs/vit/image5.png"><link rel="preload" as="image" href="/personalblog/img/docs/vit/acc/model_1711376232.h5_accs.png"><link rel="preload" as="image" href="/personalblog/img/docs/vit/acc/model_1711376232.h5_ious.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/personalblog/"><b class="navbar__title text--truncate">SamKHT</b></a><a class="navbar__item navbar__link" href="/personalblog/life-in-weeks">Life in Weeks 生命倒計時</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/personalblog/blog">Blog 博客</a><a class="navbar__item navbar__link" href="/personalblog/docs/base/intro">Prog 編程</a><a class="navbar__item navbar__link" href="/personalblog/docs/algo/intro/">Algo 算法</a><a class="navbar__item navbar__link" href="/personalblog/docs/ai/intro">AI 人工智能</a><a class="navbar__item navbar__link" href="/personalblog/docs/interests/intro">Interests 興趣</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ash3327" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-github fa-brands fa-xl" aria-label="GitHub"></a><a href="https://linkedin.com/in/khtam-51a008256" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-linkedin fa-brands fa-xl" aria-label="LinkedIn"></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/2025/05/20/intro">5月規劃</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/hand-gesture-recognition">Project: General Hand Gesture Recognition</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/oasis-event-planning-app">Project: Event-Planning App &quot;Oasis&quot;</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/p2p-communication-app">Project: P2P Communication App</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/vision-transformer-analysis">Project: Vision Transformer Analysis</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/arg-prediction-transformers">Project: ARG Prediction with Transformers</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/yolo-object-tracking">Project: YOLO Object Tracking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/gan-generation">Project: GAN Generation</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/deep-q-learning-agent">Project: Deep Q-Learning Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/unet-segmentation">Project: U-Net Segmentation</a></li></ul></div></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>4 posts tagged with &quot;Computer Vision&quot;</h1><a href="/personalblog/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/hand-gesture-recognition">Project: General Hand Gesture Recognition</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-04-15T00:00:00.000Z">April 15, 2025</time> · <!-- -->2 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/major-fyp-2024" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Hand%20Gesture%20Recognition-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://github.com/ash3327/major-fyp-2024/blob/rework-1/docs/KTL2401_1155175983_1155174636_final_report_term2.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Report-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="Report" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&amp;logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Computer_Vision-00BFFF?style=flat" alt="Computer Vision" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Contrastive_Learning-FF69B4?style=flat" alt="Contrastive Learning" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Sep%202024-Apr%202025-4285F4?style=flat" alt="Duration" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>This project aims to create a unified, semi-supervised contrastive-learning framework for hand gesture recognition. The framework is designed to adapt efficiently to various downstream tasks, such as human-computer interaction and sign language recognition, with minimal retraining or fine-tuning.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/contrastive-learning">Contrastive Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/deep-learning">Deep Learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Project: General Hand Gesture Recognition" href="/personalblog/blog/hand-gesture-recognition"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/vision-transformer-analysis">Project: Vision Transformer Analysis</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-04-01T00:00:00.000Z">April 1, 2024</time> · <!-- -->4 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/proj-vision-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&amp;color=green" alt="UNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&amp;color=green" alt="ResNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&amp;color=green" alt="ViT" class="img_ev3q"></a>
<a href="https://github.com/facebookresearch/deit" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-DeiT-orange?logo=github&amp;color=orange" alt="DeiT" class="img_ev3q"></a>
<a href="https://github.com/yitu-opensource/T2T-ViT" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-T2T-orange?logo=github&amp;color=orange" alt="T2T" class="img_ev3q"></a>
<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-CIFAR10-blue.svg" alt="Dataset | CIFAR10" class="img_ev3q"></a>
<a href="https://cs.stanford.edu/~acoates/stl10/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-STL10-blue.svg" alt="Dataset | STL10" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes-blue.svg" alt="Dataset | Cityscapes" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Update-April%202024-green.svg" alt="Last Update | April 2024" class="img_ev3q"></p>
<p>This is the final project for the course <strong>AIST4010</strong>. More details on the project can be found in the report. This project is done in April 2024.</p>
<p><strong>Report</strong>: <a href="https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Final%20Report-blue.svg" alt="Report" class="img_ev3q"></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-goals">Project Goals<a href="#project-goals" class="hash-link" aria-label="Direct link to Project Goals" title="Direct link to Project Goals">​</a></h3>
<p>The project investigates the <strong>generalizability of Vision Transformers (ViTs)</strong> compared to Convolutional Neural Networks (CNNs) for <strong>small-scale computer vision tasks</strong>. While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-contributions">Key Contributions<a href="#key-contributions" class="hash-link" aria-label="Direct link to Key Contributions" title="Direct link to Key Contributions">​</a></h3>
<ol>
<li><strong>Scalability Analysis</strong>: Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets.</li>
<li><strong>Computational Efficiency</strong>: Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets.</li>
<li><strong>Comparison of Architectures</strong>: Implemented and trained models with similar parameter counts for fair performance evaluations.</li>
</ol>
<p>Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report.</p>
<p>Sections below are not generated by ChatGPT.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="installation">Installation<a href="#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h2>
<ol>
<li>Run all commands in <code>commands.txt</code>. Ensure that the CUDA version is <strong>&lt;=11.x</strong>.</li>
</ol>
<blockquote>
<p>[!NOTE]
Execute Jupyter notebooks <strong>from the root folder</strong> of the project to avoid import issues.</p>
</blockquote>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-loading">Data Loading<a href="#data-loading" class="hash-link" aria-label="Direct link to Data Loading" title="Direct link to Data Loading">​</a></h2>
<p>Download datasets and place them in the <code>data/</code> folder. The structure should match the following diagram:</p>
<img src="/personalblog/img/docs/vit/image.png" alt="drawing" height="400">
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="models-used">Models Used<a href="#models-used" class="hash-link" aria-label="Direct link to Models Used" title="Direct link to Models Used">​</a></h2>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image3-98190f450375afbd637ff97c3ff80d39.png" width="602" height="155" class="img_ev3q"></p>
<p>The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-results">Experimental Results<a href="#experimental-results" class="hash-link" aria-label="Direct link to Experimental Results" title="Direct link to Experimental Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalability-performance">Scalability Performance<a href="#scalability-performance" class="hash-link" aria-label="Direct link to Scalability Performance" title="Direct link to Scalability Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png" width="640" height="480" class="img_ev3q"></p>
<p><strong>Findings:</strong> Transformer-based models perform poorly on small datasets.</p>
<ul>
<li>For models with the same input size, transformer-based models achieve significantly lower accuracy.</li>
<li>The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green).</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="computational-efficiency">Computational Efficiency<a href="#computational-efficiency" class="hash-link" aria-label="Direct link to Computational Efficiency" title="Direct link to Computational Efficiency">​</a></h3>
<table><thead><tr><th>Against #iterations</th><th>Against time in second-P100</th></tr></thead><tbody><tr><td><img src="/personalblog/img/docs/vit/image4.png" width="300"></td><td><img src="/personalblog/img/docs/vit/image5.png" width="300"></td></tr></tbody></table>
<p><strong>Findings:</strong> Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets.</p>
<ul>
<li>Note that it is an unfair comparison if we compare all models directly since they don&#x27;t have the same accuracy.</li>
<li>We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-task">Image Classification Task<a href="#image-classification-task" class="hash-link" aria-label="Direct link to Image Classification Task" title="Direct link to Image Classification Task">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="file-structure">File Structure<a href="#file-structure" class="hash-link" aria-label="Direct link to File Structure" title="Direct link to File Structure">​</a></h3>
<p>Relevant code and logs are located in the <code>support/</code> folder.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">support</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ commands</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Commands for running the project.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ main_code</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ipynb       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Main training code.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ models</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├─ </span><span class="token operator">&lt;</span><span class="token builtin" style="color:rgb(189, 147, 249)">id</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">dataset</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">model</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">input_size</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├─ </span><span class="token operator">&lt;</span><span class="token plain">epoch</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Trained model.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_accs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png </span><span class="token comment" style="color:rgb(98, 114, 164)"># Accuracy history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_lrs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Learning rate history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_details</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt </span><span class="token comment" style="color:rgb(98, 114, 164)"># Model details.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└─ requirements</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Python dependencies.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h3>
<p>In <strong>main_code.ipynb</strong>, users can modify the following cells, following that order:</p>
<ol>
<li>
<p><strong>Cells 1, 3, and 4</strong> immediately below Main Function:</p>
<ul>
<li>Change batch size, image size, patch size (not changed throughout the experiment).</li>
<li>Change data augmentation (train_transform).</li>
<li>Change dataset and “fraction” (proportion of subset).</li>
</ul>
</li>
<li>
<p>The cell that imports <code>torchsummary</code> and those below it, before &quot;Some Other Utility Function&quot;:</p>
<ul>
<li>Change the model used and output directory.</li>
<li>Change <code>INITIAL_LR</code>, <code>DECAY</code>, <code>GAMMA</code>, <code>kwargs</code> (arguments for the scheduler).</li>
<li>Change <code>LOAD_PATH</code> (if not None, then the weights <code>&lt;id&gt;.h5</code> will be loaded if the model matches the description).</li>
</ul>
</li>
<li>
<p>The cell immediately after &quot;The Training&quot;:</p>
<ul>
<li>Change <code>NUM_EPOCHS</code> and <code>NUM_EPOCHS_TO_SAVE</code>.</li>
</ul>
</li>
</ol>
<p>Then, you can watch the results in the cell that follows the cell in (3). The outputs by default are in the path <code>models/</code>.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-task">Image Segmentation Task<a href="#image-segmentation-task" class="hash-link" aria-label="Direct link to Image Segmentation Task" title="Direct link to Image Segmentation Task">​</a></h2>
<p>This segment explores <strong>UNet-based architectures</strong> for image segmentation tasks. Related code is in the <code>models_archive/</code> folder. Not part of the final report.</p>
<table><tr><th>mAP</th><th>IoU</th></tr><tr><td><img src="/personalblog/img/docs/vit/acc/model_1711376232.h5_accs.png" width="300"></td><td><img src="/personalblog/img/docs/vit/acc/model_1711376232.h5_ious.png" width="300"></td></tr></table>
<p>Resultant output:</p>
<p><img decoding="async" loading="lazy" src="/personalblog/assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png" width="630" height="116" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/machine-learning">Machine Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/vision-transformers">Vision Transformers</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/yolo-object-tracking">Project: YOLO Object Tracking</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-06-05T00:00:00.000Z">June 5, 2023</time> · <!-- -->3 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/ObjectDetection-v1" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-YOLO%20Object%20Tracking-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://github.com/ultralytics" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Ultralytics-00875A.svg?logo=ultralytics&amp;logoColor=white" alt="Ultralytics" class="img_ev3q"></a>
<a href="https://github.com/ultralytics/yolov5" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/YOLO-FF69B4.svg?logo=yolo&amp;logoColor=white" alt="YOLO" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Object%20Detection-EE4C2C.svg?logo=object-detection&amp;logoColor=white" alt="Object Detection" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-June%202023-green.svg" alt="Last Updated" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-june-2023">Backup of Old Project (June 2023)<a href="#backup-of-old-project-june-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (June 2023)" title="Direct link to Backup of Old Project (June 2023)">​</a></h2>
<p>This is a backup of an old project that focused on object detection and tracking over videos using <strong>YOLOv8</strong>. The project was based on the following tutorials:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=WgPbbWmnXJ8&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI" target="_blank" rel="noopener noreferrer">Murtaza&#x27;s Workshop - Robotics and AI&#x27;s Object Detection Tutorial</a></li>
<li><a href="https://www.computervision.zone/courses/object-detection-course/" target="_blank" rel="noopener noreferrer">Computer Vision Zone&#x27;s Object Detection Course</a></li>
</ul>
<p>The project was written in Python and uses the YOLOv8 object detection model.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="execution">Execution<a href="#execution" class="hash-link" aria-label="Direct link to Execution" title="Direct link to Execution">​</a></h2>
<p>Start a new virtual environment:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 -m venv venv</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">source venv/bin/activate # linux or windows WSL</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.\venv\Scripts\activate # windows cmd</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Then install the requirements:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install -r requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Finally, run the jupyter notebook <code>Object Detection.ipynb</code> within the virtual environment.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-project--key-features">About the Project / Key Features<a href="#about-the-project--key-features" class="hash-link" aria-label="Direct link to About the Project / Key Features" title="Direct link to About the Project / Key Features">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="static-image-detection-section-32"><strong>Static Image Detection</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.2-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.2" class="img_ev3q"><a href="#static-image-detection-section-32" class="hash-link" aria-label="Direct link to static-image-detection-section-32" title="Direct link to static-image-detection-section-32">​</a></h3>
<ul>
<li>Applying YOLOv8 on static image given by path specified in the &quot;Parameters&quot; section.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image-b1c62bc4e9d1bea54cd9a83744b9b221.png" width="1200" height="703" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="video-detection-section-34"><strong>Video Detection</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.4-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.4" class="img_ev3q"><a href="#video-detection-section-34" class="hash-link" aria-label="Direct link to video-detection-section-34" title="Direct link to video-detection-section-34">​</a></h3>
<ul>
<li>Applying YOLOv8 on video given by path specified in the &quot;Parameters&quot; section.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/vid1-24f111c7aa665024b482883cfb9f0491.gif" width="800" height="450" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort"><strong>Instance Tracking with Abrewlay Sort</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.5.1-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.5.1" class="img_ev3q"> <a href="https://github.com/abewley/sort" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Abrewlay%20Sort-007EC6.svg?logo=github&amp;logoColor=white" alt="Abrewlay Sort" class="img_ev3q"></a><a href="#instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort" class="hash-link" aria-label="Direct link to instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort" title="Direct link to instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort">​</a></h3>
<ul>
<li>Instance identification with Abrewlay Sort library for tracking objects.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/vid2-1c9c4db30069b35d42478de78c21cf9c.gif" width="800" height="450" class="img_ev3q"></p>
<p><strong>Notes</strong></p>
<ul>
<li>The color of the box indicates the INSTANCE ID</li>
</ul>
<p><strong>Problems</strong></p>
<ol>
<li><strong>Inconsistent IDs</strong>: Occurs under occlusion or label changes<!-- -->
<ul>
<li>Observe that the id of the truck 457 on the rightmost lane (cyan, with label &quot;car&quot;) changed to id 473 (purple, with label &quot;truck&quot;)</li>
<li>Indicates that the library cannot provide a consistent ID for the same object across frames</li>
</ul>
</li>
<li><strong>Multiple bounding boxes for the same object</strong>
<ul>
<li>As artifacts of the original YOLOv8 detection (due to using the nano model)</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="custom-tracking-algorithm-section-352"><strong>Custom Tracking Algorithm</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.5.2-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.5.2" class="img_ev3q"><a href="#custom-tracking-algorithm-section-352" class="hash-link" aria-label="Direct link to custom-tracking-algorithm-section-352" title="Direct link to custom-tracking-algorithm-section-352">​</a></h3>
<ul>
<li>Instance identification with custom-implemented algorithm for tracking objects.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/vid3-95dcb36459fc32eac62ef0ced26f3d9d.gif" width="800" height="450" class="img_ev3q"></p>
<p><strong>Goals</strong></p>
<ul>
<li>Implementing a version of the object tracking algorithm that is more resistent to lost frames and flickering compared to the Abrewlay Sort library.</li>
</ul>
<p><strong>Features</strong></p>
<ol>
<li><strong>Label Accumulation</strong>
<ul>
<li>Can accumulate labels from previous frames.</li>
<li>The ids (colors of the frames) of the objects are more consistent, which can be checked visually</li>
<li>The same truck now got a consistent id (no change in color indicates that)</li>
</ul>
</li>
<li><strong>Bounding Box Merging</strong>
<ul>
<li>Finds the closest bounding box to the previous frames (stored in a dictionary)</li>
<li>Dictionary is cleared after a certain number of frames</li>
<li>Only bounding boxes within a certain change in size and aspect ratio are merged by overwritting the same instance id</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="car-counter-section-36"><strong>Car Counter</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.6-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.6" class="img_ev3q"><a href="#car-counter-section-36" class="hash-link" aria-label="Direct link to car-counter-section-36" title="Direct link to car-counter-section-36">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/vid4-d03ca387a586325bd24979201d986374.gif" width="800" height="450" class="img_ev3q"></p>
<ul>
<li>Simple algorithm for counting cars across the line by masking the image before detection.</li>
<li>Utilized the algorithm in sectino 3.5.2 for tracking objects to ensure that a car is not detected twice.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/yolo">YOLO</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/object-detection">Object Detection</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/tracking">Tracking</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/personalblog/blog/unet-segmentation">Project: U-Net Segmentation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-08-04T00:00:00.000Z">August 4, 2022</time> · <!-- -->4 min read</div></header><div class="markdown"><p><a href="https://github.com/ash3327/ImageSegmentation-UNet" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-U--Net%20Segmentation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Image%20Segmentation-red.svg?logo=segmentation&amp;logoColor=white" alt="Image Segmentation" class="img_ev3q">
<a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-December%202023-green.svg" alt="Last Updated: December 2023" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-december-2023">Backup of Old Project (December 2023)<a href="#backup-of-old-project-december-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (December 2023)" title="Direct link to Backup of Old Project (December 2023)">​</a></h2>
<p>This is a backup of an old project focused on training a U-Net model from scratch for semantic segmentation from scratch on the Cityscapes dataset and Carvana dataset. The images are DOWNSCALED to speed up the training process for learning purposes. The model has been trained and tested with the following results:</p>
<blockquote>
<p>[!WARNING]
If you are looking for a high-quality model, this is NOT the place. This is only a practice exercise when I was in year 2.</p>
</blockquote>
<blockquote>
<p>[!NOTE]
Training is done long ago and some parameters recorded, such as the number of epochs, may not be accurate</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is done few years ago from the point of writing this documentation. Back then, the training details are not documented properly and the jupyter notebook results are not saved for each experiment. The results are not excellent either. Sorry for the inconvenience.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setups">Setups<a href="#setups" class="hash-link" aria-label="Direct link to Setups" title="Direct link to Setups">​</a></h2>
<p><strong>Note</strong>:</p>
<ul>
<li><strong>Environment</strong>: Python 3.10.8, PyTorch 2.1.2, CUDA 12.1</li>
<li><strong>Libraries</strong>: See <code>requirements-lock.txt</code></li>
<li><strong>Folder Layout</strong>:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">data/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  carvana/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    test/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train_masks/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val_masks/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  cityscapes/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gtFine/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    leftImg8Bit/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Data Sources:<!-- -->
<ul>
<li>Carvana: <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<ul>
<li>Download from the above link</li>
<li>Unzip all zips and organize as described above</li>
</ul>
</li>
<li>Cityscapes: <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<ul>
<li>Download the coarse dataset (images and the masks)</li>
<li>Unzip all zips and organize as described above</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Models</strong>:<!-- -->
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">models/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  sem_segmentation/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    1703922437_cityscapes/model_1703922437.h5 # cityscapes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    1703926149/model_1703926149.h5 # carvana</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Download from: <a href="https://drive.google.com/drive/folders/1Mgb_YWV__zsQGNryXGvlOa2EaH49WERe" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Google%20Drive-Models-orange.svg?logo=googledrive&amp;logoColor=white" alt="Google Drive" class="img_ev3q"></a></li>
<li><strong>Training scripts</strong>: Run <code>main_semantic_segmentation_carvana.py</code> and <code>main_semantic_segmentation_cityscape.py</code> as juypter notebooks.</li>
<li><strong>Testing scripts</strong>: Run the <code>test_semantic_segmentation_carvana.py</code> script as jupyter notebook. Specify the paths of the h5 files in cell [8].</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">Carvana Dataset (Carvana Image Masking Challenge Dataset) <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a><a href="#carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" class="hash-link" aria-label="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" title="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">​</a></h3>
<p>Dataset information</p>
<ul>
<li>Original image dimension: 1918 x 1280</li>
<li>Training image dimension: 160 x 240 and 320 x 480</li>
</ul>
<p>Validation accuracies (highest)</p>
<ul>
<li>Validation Pixel Accuracy: <strong>0.9955</strong></li>
<li>Validation Dice Score: <strong>0.9911</strong></li>
</ul>
<h3 align="center"> Results </h3>
<p>Results on test set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image-041267cdfd77e346a9d0e0d7d422bb48.png" width="520" height="341" class="img_ev3q"></p>
<p>Results on train set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image2-78057b4210e74b5ab7ccbaae64eb9529.png" width="518" height="341" class="img_ev3q"></p>
<p>Inspection of intermediate layers: (In the order: Prediction, Output of Downsampling Block 1, Output of Bottleneck Block, Output of Upsampling Block 1, Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image4-341eb0e84099144c4c517f3738e0f883.png" width="1008" height="501" class="img_ev3q"></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Details of the experiments</summary><div><div class="collapsibleContent_i85q"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Normalization: mean 0, std 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Augments:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomHorizontalFlip(p=0.5),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomVerticalFlip(p=0.1),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomRotation(degrees=35),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**29/12/2023 17:15: result12_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 5e-7</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: StepLR: step_size=5, gamma=0.85</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 42</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 90.9%, Avg loss: 0.403125, Test recall: 0.5831877589225769, precision: 0.980972170829773</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 08:40: result13_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 50</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 81.0%, Avg loss: 0.775568, Test recall: 0.1010913997888565, precision: 0.9727051258087158</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 11:33: result14_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 96.6%, Avg loss: 0.210099, Test recall: 0.9129086136817932, precision: 0.927651584148407</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 15:47: 1703922437**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 16</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.5%, Avg loss: 0.013175, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9918522238731384, precision: 0.9872172474861145, dice_score: 0.9895293116569519</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 16:49: 1703926149**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 320 x 480</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.6%, Avg loss: 0.009994, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9942919611930847, precision: 0.9879629611968994, dice_score: 0.9911173582077026</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">Cityscapes Dataset (Cityscapes Dataset) <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a><a href="#cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" class="hash-link" aria-label="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" title="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">​</a></h3>
<ul>
<li>Test Pixel Accuracy: <strong>0.8669</strong></li>
<li>The code for evaluating the mAP for the cityscape dataset has been lost. The code in this repository does not reflect the true results.</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">1704279189_cityscapes/model_1704284109.h5: </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**Documented:** </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test acc: 0.8402184247970581, Test loss: 0.6274673556908965</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/segmentation">Segmentation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/deep-learning">Deep Learning</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Sam K. H. Tam. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>