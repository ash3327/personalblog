<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Project: Vision Transformer Analysis | Sam&#x27;s Portfolio</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Project: Vision Transformer Analysis | Sam&#x27;s Portfolio"><meta data-rh="true" name="description" content="Comparative study of Vision Transformers vs CNNs on small datasets"><meta data-rh="true" property="og:description" content="Comparative study of Vision Transformers vs CNNs on small datasets"><meta data-rh="true" property="og:image" content="https://ash3327.github.io/personalblog/img//img/docs/vit/vit/image4.png"><meta data-rh="true" name="twitter:image" content="https://ash3327.github.io/personalblog/img//img/docs/vit/vit/image4.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-04-01T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Project,PyTorch,Computer Vision,Machine Learning,Vision Transformers"><link data-rh="true" rel="icon" href="/personalblog/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis" hreflang="en"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://ash3327.github.io/personalblog/blog/vision-transformer-analysis","mainEntityOfPage":"https://ash3327.github.io/personalblog/blog/vision-transformer-analysis","url":"https://ash3327.github.io/personalblog/blog/vision-transformer-analysis","headline":"Project: Vision Transformer Analysis","name":"Project: Vision Transformer Analysis","description":"Comparative study of Vision Transformers vs CNNs on small datasets","datePublished":"2024-04-01T00:00:00.000Z","author":[],"image":{"@type":"ImageObject","@id":"https://ash3327.github.io/personalblog/img//img/docs/vit/vit/image4.png","url":"https://ash3327.github.io/personalblog/img//img/docs/vit/vit/image4.png","contentUrl":"https://ash3327.github.io/personalblog/img//img/docs/vit/vit/image4.png","caption":"title image for the blog post: Project: Vision Transformer Analysis"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://ash3327.github.io/personalblog/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/personalblog/blog/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog/atom.xml" title="Sam&#39;s Portfolio Atom Feed">




<link rel="alternate" type="application/rss+xml" href="/personalblog/blog_old/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog_old/atom.xml" title="Sam&#39;s Portfolio Atom Feed">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"><link rel="stylesheet" href="/personalblog/assets/css/styles.40526b60.css">
<script src="/personalblog/assets/js/runtime~main.b8a37c5f.js" defer="defer"></script>
<script src="/personalblog/assets/js/main.6c70eedc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/docs/vit/image.png"><link rel="preload" as="image" href="/img/docs/vit/image4.png"><link rel="preload" as="image" href="/img/docs/vit/image5.png"><link rel="preload" as="image" href="/img/docs/vit/acc/model_1711376232.h5_accs.png"><link rel="preload" as="image" href="/img/docs/vit/acc/model_1711376232.h5_ious.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/personalblog/"><b class="navbar__title text--truncate">SamKHT</b></a><a class="navbar__item navbar__link" href="/personalblog/life-in-weeks">Life in Weeks 生命倒計時</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/personalblog/blog">Blog 博客</a><a class="navbar__item navbar__link" href="/personalblog/docs/base/intro">Prog 編程</a><a class="navbar__item navbar__link" href="/personalblog/docs/algo/intro/">Algo 算法</a><a class="navbar__item navbar__link" href="/personalblog/docs/ai/intro">AI 人工智能</a><a class="navbar__item navbar__link" href="/personalblog/docs/interests/intro">Interests 興趣</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ash3327" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-github fa-brands fa-xl" aria-label="GitHub"></a><a href="https://linkedin.com/in/khtam-51a008256" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-linkedin fa-brands fa-xl" aria-label="LinkedIn"></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/2025/05/20/intro">5月規劃</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/oasis-event-planning-app">Project: Event-Planning App &quot;Oasis&quot;</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/p2p-communication-app">Project: P2P Communication App</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/personalblog/blog/vision-transformer-analysis">Project: Vision Transformer Analysis</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/arg-prediction-transformers">Project: ARG Prediction with Transformers</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/yolo-object-tracking">Project: YOLO Object Tracking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/gan-generation">Project: GAN Generation</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/deep-q-learning-agent">Project: Deep Q-Learning Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/unet-segmentation">Project: U-Net Segmentation</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Project: Vision Transformer Analysis</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-04-01T00:00:00.000Z">April 1, 2024</time> · <!-- -->4 min read</div></header><div id="__blog-post-container" class="markdown"><p><a href="https://github.com/ash3327/proj-vision-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&amp;color=green" alt="UNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&amp;color=green" alt="ResNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&amp;color=green" alt="ViT" class="img_ev3q"></a>
<a href="https://github.com/facebookresearch/deit" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-DeiT-orange?logo=github&amp;color=orange" alt="DeiT" class="img_ev3q"></a>
<a href="https://github.com/yitu-opensource/T2T-ViT" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-T2T-orange?logo=github&amp;color=orange" alt="T2T" class="img_ev3q"></a>
<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-CIFAR10-blue.svg" alt="Dataset | CIFAR10" class="img_ev3q"></a>
<a href="https://cs.stanford.edu/~acoates/stl10/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-STL10-blue.svg" alt="Dataset | STL10" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes-blue.svg" alt="Dataset | Cityscapes" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Update-April%202024-green.svg" alt="Last Update | April 2024" class="img_ev3q"></p>
<p>This is the final project for the course <strong>AIST4010</strong>. More details on the project can be found in the report. This project is done in April 2024.</p>
<p><strong>Report</strong>: <a href="https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Final%20Report-blue.svg" alt="Report" class="img_ev3q"></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-goals">Project Goals<a href="#project-goals" class="hash-link" aria-label="Direct link to Project Goals" title="Direct link to Project Goals">​</a></h3>
<p>The project investigates the <strong>generalizability of Vision Transformers (ViTs)</strong> compared to Convolutional Neural Networks (CNNs) for <strong>small-scale computer vision tasks</strong>. While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-contributions">Key Contributions<a href="#key-contributions" class="hash-link" aria-label="Direct link to Key Contributions" title="Direct link to Key Contributions">​</a></h3>
<ol>
<li><strong>Scalability Analysis</strong>: Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets.</li>
<li><strong>Computational Efficiency</strong>: Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets.</li>
<li><strong>Comparison of Architectures</strong>: Implemented and trained models with similar parameter counts for fair performance evaluations.</li>
</ol>
<p>Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report.</p>
<p>Sections below are not generated by ChatGPT.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="installation">Installation<a href="#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h2>
<ol>
<li>Run all commands in <code>commands.txt</code>. Ensure that the CUDA version is <strong>&lt;=11.x</strong>.</li>
</ol>
<blockquote>
<p>[!NOTE]
Execute Jupyter notebooks <strong>from the root folder</strong> of the project to avoid import issues.</p>
</blockquote>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-loading">Data Loading<a href="#data-loading" class="hash-link" aria-label="Direct link to Data Loading" title="Direct link to Data Loading">​</a></h2>
<p>Download datasets and place them in the <code>data/</code> folder. The structure should match the following diagram:</p>
<img src="/img/docs/vit/image.png" alt="drawing" height="400">
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="models-used">Models Used<a href="#models-used" class="hash-link" aria-label="Direct link to Models Used" title="Direct link to Models Used">​</a></h2>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image3-98190f450375afbd637ff97c3ff80d39.png" width="602" height="155" class="img_ev3q"></p>
<p>The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-results">Experimental Results<a href="#experimental-results" class="hash-link" aria-label="Direct link to Experimental Results" title="Direct link to Experimental Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalability-performance">Scalability Performance<a href="#scalability-performance" class="hash-link" aria-label="Direct link to Scalability Performance" title="Direct link to Scalability Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="/personalblog/assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png" width="640" height="480" class="img_ev3q"></p>
<p><strong>Findings:</strong> Transformer-based models perform poorly on small datasets.</p>
<ul>
<li>For models with the same input size, transformer-based models achieve significantly lower accuracy.</li>
<li>The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green).</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="computational-efficiency">Computational Efficiency<a href="#computational-efficiency" class="hash-link" aria-label="Direct link to Computational Efficiency" title="Direct link to Computational Efficiency">​</a></h3>
<table><thead><tr><th>Against #iterations</th><th>Against time in second-P100</th></tr></thead><tbody><tr><td><img src="/img/docs/vit/image4.png" width="300"></td><td><img src="/img/docs/vit/image5.png" width="300"></td></tr></tbody></table>
<p><strong>Findings:</strong> Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets.</p>
<ul>
<li>Note that it is an unfair comparison if we compare all models directly since they don&#x27;t have the same accuracy.</li>
<li>We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-task">Image Classification Task<a href="#image-classification-task" class="hash-link" aria-label="Direct link to Image Classification Task" title="Direct link to Image Classification Task">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="file-structure">File Structure<a href="#file-structure" class="hash-link" aria-label="Direct link to File Structure" title="Direct link to File Structure">​</a></h3>
<p>Relevant code and logs are located in the <code>support/</code> folder.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">support</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ commands</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Commands for running the project.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ main_code</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ipynb       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Main training code.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ models</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├─ </span><span class="token operator">&lt;</span><span class="token builtin" style="color:rgb(189, 147, 249)">id</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">dataset</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">model</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">input_size</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├─ </span><span class="token operator">&lt;</span><span class="token plain">epoch</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Trained model.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_accs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png </span><span class="token comment" style="color:rgb(98, 114, 164)"># Accuracy history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_lrs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Learning rate history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_details</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt </span><span class="token comment" style="color:rgb(98, 114, 164)"># Model details.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└─ requirements</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Python dependencies.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h3>
<p>In <strong>main_code.ipynb</strong>, users can modify the following cells, following that order:</p>
<ol>
<li>
<p><strong>Cells 1, 3, and 4</strong> immediately below Main Function:</p>
<ul>
<li>Change batch size, image size, patch size (not changed throughout the experiment).</li>
<li>Change data augmentation (train_transform).</li>
<li>Change dataset and “fraction” (proportion of subset).</li>
</ul>
</li>
<li>
<p>The cell that imports <code>torchsummary</code> and those below it, before &quot;Some Other Utility Function&quot;:</p>
<ul>
<li>Change the model used and output directory.</li>
<li>Change <code>INITIAL_LR</code>, <code>DECAY</code>, <code>GAMMA</code>, <code>kwargs</code> (arguments for the scheduler).</li>
<li>Change <code>LOAD_PATH</code> (if not None, then the weights <code>&lt;id&gt;.h5</code> will be loaded if the model matches the description).</li>
</ul>
</li>
<li>
<p>The cell immediately after &quot;The Training&quot;:</p>
<ul>
<li>Change <code>NUM_EPOCHS</code> and <code>NUM_EPOCHS_TO_SAVE</code>.</li>
</ul>
</li>
</ol>
<p>Then, you can watch the results in the cell that follows the cell in (3). The outputs by default are in the path <code>models/</code>.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-task">Image Segmentation Task<a href="#image-segmentation-task" class="hash-link" aria-label="Direct link to Image Segmentation Task" title="Direct link to Image Segmentation Task">​</a></h2>
<p>This segment explores <strong>UNet-based architectures</strong> for image segmentation tasks. Related code is in the <code>models_archive/</code> folder. Not part of the final report.</p>
<table><tr><th>mAP</th><th>IoU</th></tr><tr><td><img src="/img/docs/vit/acc/model_1711376232.h5_accs.png" width="300"></td><td><img src="/img/docs/vit/acc/model_1711376232.h5_ious.png" width="300"></td></tr></table>
<p>Resultant output:</p>
<p><img decoding="async" loading="lazy" src="/personalblog/assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png" width="630" height="116" class="img_ev3q"></p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/py-torch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/machine-learning">Machine Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/vision-transformers">Vision Transformers</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-01-vision-transformer-analysis.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/personalblog/blog/p2p-communication-app"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Project: P2P Communication App</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/personalblog/blog/arg-prediction-transformers"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Project: ARG Prediction with Transformers</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a><ul><li><a href="#project-goals" class="table-of-contents__link toc-highlight">Project Goals</a></li><li><a href="#key-contributions" class="table-of-contents__link toc-highlight">Key Contributions</a></li></ul></li><li><a href="#installation" class="table-of-contents__link toc-highlight">Installation</a></li><li><a href="#data-loading" class="table-of-contents__link toc-highlight">Data Loading</a></li><li><a href="#models-used" class="table-of-contents__link toc-highlight">Models Used</a></li><li><a href="#experimental-results" class="table-of-contents__link toc-highlight">Experimental Results</a><ul><li><a href="#scalability-performance" class="table-of-contents__link toc-highlight">Scalability Performance</a></li><li><a href="#computational-efficiency" class="table-of-contents__link toc-highlight">Computational Efficiency</a></li></ul></li><li><a href="#image-classification-task" class="table-of-contents__link toc-highlight">Image Classification Task</a><ul><li><a href="#file-structure" class="table-of-contents__link toc-highlight">File Structure</a></li><li><a href="#training" class="table-of-contents__link toc-highlight">Training</a></li></ul></li><li><a href="#image-segmentation-task" class="table-of-contents__link toc-highlight">Image Segmentation Task</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Sam K. H. Tam. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>