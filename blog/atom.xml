<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="atom.xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ash3327.github.io/blog/blog</id>
    <title>Sam's Portfolio Blog</title>
    <updated>2025-05-20T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ash3327.github.io/blog/blog"/>
    <subtitle>Sam's Portfolio Blog</subtitle>
    <icon>https://ash3327.github.io/blog/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[5月規劃]]></title>
        <id>https://ash3327.github.io/blog/blog/2025/05/20/intro</id>
        <link href="https://ash3327.github.io/blog/blog/2025/05/20/intro"/>
        <updated>2025-05-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[經過了二月到五月期間衆多的 Projects、Reports 的困擾之後，到了五月末似乎終於有些時間能夠讓自己沉澱一下、重整一下自己對未來的規劃。]]></summary>
        <content type="html"><![CDATA[<p>經過了二月到五月期間衆多的 Projects、Reports 的困擾之後，到了五月末似乎終於有些時間能夠讓自己沉澱一下、重整一下自己對未來的規劃。</p>
<p>本身自己對 Reinforcement Learning 的興趣就比較大，但前陣子有些陷入了與他人的比較之中，反而失去了本來對知識的熱情。</p>
<p>這周打算重溫曾經大二時上過的 RL，並重新翻新 SnowFight 這一個 Project。</p>
<p>希望能夠避免自己進入拖延症的狀態中。</p>
<hr>
<p>今天暫時復習了 RL 的基礎
關於 Stanford, Sutton Bartol 的 Reinforcement Learning 的 Ex 1</p>]]></content>
        <category label="Daily 日常" term="Daily 日常"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: Event-Planning App "Oasis"]]></title>
        <id>https://ash3327.github.io/blog/blog/oasis-event-planning-app</id>
        <link href="https://ash3327.github.io/blog/blog/oasis-event-planning-app"/>
        <updated>2024-06-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Android event planning app with robust notification system and SQL database]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/OasisPlanner/tree/development" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Event--Planning%20App%20'Oasis'-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><em>Calendar App Project</em> (Developing Phase)</p>
<blockquote>
<p>[!INFO]</p>
<ul>
<li>Still under development and the functionalities are not completed yet.</li>
<li>Distributable version not delivered yet.</li>
</ul>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="our-goals">Our Goals<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#our-goals" class="hash-link" aria-label="Direct link to Our Goals" title="Direct link to Our Goals">​</a></h2>
<p>Oasis: where you find calm <em>in the past, present, and future.</em></p>
<p>We aim to provide a convenient and simple interface for users to effortlessly jot down and analyze their everyday productive work, without putting too much stress on yourself.</p>
<p>We believe:</p>
<ul>
<li>Visualizing past work done can improve confidence</li>
<li>Striking a balance between work and relaxing is crucial to boosting long-term productivity</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="environment-setup">Environment Setup<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#environment-setup" class="hash-link" aria-label="Direct link to Environment Setup" title="Direct link to Environment Setup">​</a></h2>
<ul>
<li><strong>JDK</strong>: Version 1.8 (Java 8). Download from <a href="https://www.oracle.com/java/technologies/javase-jdk8-downloads.html" target="_blank" rel="noopener noreferrer">Oracle JDK 8</a> or use <a href="https://sdkman.io/" target="_blank" rel="noopener noreferrer">SDKMAN!</a>.</li>
<li><strong>Gradle</strong>: Use the included Gradle Wrapper (v7.3.3). Run <code>./gradlew</code> (Unix) or <code>gradlew.bat</code> (Windows).</li>
<li><strong>OS</strong>: Tested on Windows 11. Should work on macOS/Linux with JDK installed.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-maintenance">Code Maintenance<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#code-maintenance" class="hash-link" aria-label="Direct link to Code Maintenance" title="Direct link to Code Maintenance">​</a></h2>
<p><em>Most part of the features are still under development.</em></p>
<ul>
<li><a href="https://github.com/ash3327/OasisPlanner/tree/development/markdowns/code_structure.md" target="_blank" rel="noopener noreferrer">Design Reference on Code Structure</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="design">Design<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#design" class="hash-link" aria-label="Direct link to Design" title="Direct link to Design">​</a></h2>
<ul>
<li><a href="https://github.com/ash3327/OasisPlanner/tree/development/markdowns/first_draft_design_doc.png" target="_blank" rel="noopener noreferrer">Design Document</a></li>
<li>Note: In the deisgn document, some feature designs are copied from existing apps. Such designs will ONLY BE USED AS A BASIC REFERENCE, and the final product WILL NOT include the exact same design.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="our-plan">Our Plan<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#our-plan" class="hash-link" aria-label="Direct link to Our Plan" title="Direct link to Our Plan">​</a></h2>
<ul>
<li>Complete basic functionalities as described in our goals before September 2024.</li>
<li>We are currently working on:<!-- -->
<ul>
<li>Basic Functionalities</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="recent-updates">Recent Updates<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#recent-updates" class="hash-link" aria-label="Direct link to Recent Updates" title="Direct link to Recent Updates">​</a></h2>
<ul>
<li>Our recent update added support to devices of Android version 13+. [16 JUN 2024]</li>
<li>Added "Quick Add" and "Quick Edit" function - users can now add new events with just one click!</li>
<li>Added UI for Home and Projects fragments.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="functionalities">Functionalities<a href="https://ash3327.github.io/blog/blog/oasis-event-planning-app#functionalities" class="hash-link" aria-label="Direct link to Functionalities" title="Direct link to Functionalities">​</a></h2>
<ul>
<li>
<p>Preview of work:</p>
<img src="https://github.com/ash3327/ash3327/assets/86100752/3548ccde-c41b-440f-af3d-4f35303066e4" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/73996de9-525e-4c91-a27d-f76b8054de93" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/43f12dbf-4ab5-45ec-9f50-6086b3f7e601" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/82471662-dfac-44f0-bbc5-c06190d2a21e" width="200">
</li>
<li>
<p>Relevant documents will be uploaded later.</p>
</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="Java" term="Java"/>
        <category label="Android" term="Android"/>
        <category label="RoomDB" term="RoomDB"/>
        <category label="Mobile Development" term="Mobile Development"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: P2P Communication App]]></title>
        <id>https://ash3327.github.io/blog/blog/p2p-communication-app</id>
        <link href="https://ash3327.github.io/blog/blog/p2p-communication-app"/>
        <updated>2024-04-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Real-time audio/video streaming with optimized packet synchronization]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/Peer-to-Peer-Communication-App" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-P2P%20Communication%20App-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://github.com/ash3327/Peer-to-Peer-Communication-App" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/public%20repo-github-%2324292e.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/last%20commit-Apr%202024-%23FFA500.svg?style=for-the-badge&amp;logo=git&amp;logoColor=white" alt="Last Commit" class="img_ev3q"></p>
<p>A fork of the project in the semester 2023-24 Term 2, creating a peer-to-peer communication app supporting audio recording, waveform display and editing, and also screen share function.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-features">Key Features<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#key-features" class="hash-link" aria-label="Direct link to Key Features" title="Direct link to Key Features">​</a></h2>
<ul>
<li>Peer-to-peer communication within local area network</li>
<li>Functions:<!-- -->
<ul>
<li>Create, join and leave chat rooms</li>
<li>Audio recording</li>
<li>Screen sharing</li>
</ul>
</li>
<li>Synchronization and handling of audio and video streams from multiple users so that they do not hear their own voices</li>
</ul>
<table><tbody><tr><td>Chatroom Creation &amp; User Alias</td><td>Audio Recording</td></tr><tr><td><img src="https://ash3327.github.io/img/docs/p2p/image-1.png" alt="Chatroom Creation &amp; User Alias"></td><td><img src="https://ash3327.github.io/img/docs/p2p/image-2.png" alt="Audio Recording"></td></tr><tr><td colspan="2">Screen Sharing</td></tr><tr><td colspan="2"><img src="https://ash3327.github.io/img/docs/p2p/image-3.png" alt="Screen Sharing"></td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h2>
<ul>
<li>Consists of ONE server and multiple clients</li>
<li>The server and clients can be runned on the same machine, or on different machines within the same local area network</li>
</ul>
<p>The calls between server and clients, visualized:
<img decoding="async" loading="lazy" src="https://ash3327.github.io/blog/assets/images/gui-533c3f5b8d7f0b78f7a4ad1134bc3771.jpeg" width="800" height="504" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notes">Notes<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#notes" class="hash-link" aria-label="Direct link to Notes" title="Direct link to Notes">​</a></h2>
<ul>
<li>Functions that allows usage over the internet are not implemented yet due to problems in port forwarding that requires extra care</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="installation">Installation<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h2>
<p>To install this package, perform the following:</p>
<ol>
<li>Execute <code>pip3 install -r requirements.txt</code> in command prompt.</li>
</ol>
<blockquote>
<p>[!CAUTION]</p>
<ol start="2">
<li>
<p>To allow the usage of the audio-to-text function, you need to manually install the ffmpeg package. (Note that we only allow English transcription for now)</p>
<p>If you're using conda, then do <code>conda install ffmpeg</code> (advised).</p>
</li>
</ol>
</blockquote>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>More:</summary><div><div class="collapsibleContent_i85q"><p>If your OS is linux, then do <code>sudo apt install ffmpeg</code>.</p><p>If your OS is windows, you can download FFMPEG from the official download page: <a href="https://ffmpeg.org/download.html#build-windows" target="_blank" rel="noopener noreferrer">https://ffmpeg.org/download.html#build-windows</a>. (Personally, we used the version ffmpeg-master-latest-win64-gpl.zip provided in <a href="https://github.com/BtbN/FFmpeg-Builds/releases" target="_blank" rel="noopener noreferrer">https://github.com/BtbN/FFmpeg-Builds/releases</a>)</p><p>If your OS is MacOS, you can also download it from the official download page: <a href="https://ffmpeg.org/download.html#build-mac" target="_blank" rel="noopener noreferrer">https://ffmpeg.org/download.html#build-mac</a>.</p><p>Note that if you downloaded the executable from the website, you'll have to manually move it (<code>ffmpeg.exe</code> in the unzipped <code>bin</code> subfolder) to your script's root directory (in the SAME layer as other .py files) (or add it to PATH).</p></div></div></details>
<p>Note:</p>
<ol>
<li>The application currently only work with computers <em>in the same local area network</em> due to port forwarding.</li>
</ol>
<blockquote>
<p>[!CAUTION]</p>
<ol start="2">
<li>Please TURN OFF Windows Defender Firewall AND Windows Firewall (or any firewalls) in the private network before using.</li>
</ol>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="server-side">Server-Side<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#server-side" class="hash-link" aria-label="Direct link to Server-Side" title="Direct link to Server-Side">​</a></h2>
<p>To start the server, perform the following:</p>
<ol>
<li>
<p>Run <code>chat_server.py</code> by calling <code>python chat_server.py --port &lt;port&gt;</code>.</p>
<ul>
<li>Use the token <code>-h</code> to get hints on the arguments.</li>
<li>Use the token <code>-l</code> to show logs of all communication between server and client.</li>
<li>Use the token <code>-r</code> to change the sampling frequency of all audio messages sent to and received from this server.</li>
<li>Press <code>Ctrl+C</code> for the following:<!-- -->
<ul>
<li>check the ip and port of the server, or</li>
<li>terminate the server</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>Notes:</p>
<ol>
<li>
<p>You should be able to read the server IP and the port as follows:
<code>Initializing Chat Server at IP [10.13.252.5] and port [12345]</code></p>
</li>
<li>
<p>You can terminate the server end program and all its associated connections by pressing <code>Ctrl+C</code> in the command prompt.</p>
</li>
<li>
<p>Sampling Frequency can be set with <code>-r &lt;frequency&gt;</code> (Default = 5000). It is advised to lower your sampling frequency if your computer cannot handle the default sampling frequency.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="client-side">Client-Side<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#client-side" class="hash-link" aria-label="Direct link to Client-Side" title="Direct link to Client-Side">​</a></h2>
<p>To start the client-side software, perform the following:</p>
<ol>
<li>
<p>Run <code>chat_client.py</code> by calling <code>python chat_client.py --ip &lt;ip&gt; --port &lt;port&gt;</code>, where the IP and port can be read from the server side.</p>
<ul>
<li>Use the token <code>-h</code> to get hints on the arguments.</li>
<li>Use the token <code>-l</code> to show logs of all communication between server and client.</li>
</ul>
</li>
</ol>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Functionalities:</summary><div><div class="collapsibleContent_i85q"><ol>
<li>Users can click on 'new room' button to enter the name of the room now.
<img decoding="async" loading="lazy" src="https://ash3327.github.io/blog/assets/images/image-1-83829a6979e7ec7541ab69441b68444f.png" width="911" height="630" class="img_ev3q"></li>
<li>users can click on the room names to join the room directly now.
<img decoding="async" loading="lazy" src="https://ash3327.github.io/blog/assets/images/image-0bf0d28d58a47f3ddffab4878347d4b5.png" width="827" height="565" class="img_ev3q"></li>
<li>Users can mute/unmute his/her voices, and also quit room now.
<img decoding="async" loading="lazy" src="https://ash3327.github.io/blog/assets/images/image-2-0f02d0a61e389d69f55d686505359489.png" width="1200" height="944" class="img_ev3q"></li>
</ol></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logs">Logs<a href="https://ash3327.github.io/blog/blog/p2p-communication-app#logs" class="hash-link" aria-label="Direct link to Logs" title="Direct link to Logs">​</a></h2>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Example of server side log:</summary><div><div class="collapsibleContent_i85q"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Initializing Chat Server at IP [10.13.252.5] and port [12345]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Starting server...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Accepted request from: 10.13.252.5 port 1749</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list                  : 10.13.252.5 1749      : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list_rooms            : 10.13.252.5 1749      : {'rooms': []}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/create                : 10.13.252.5 1749      : {'action': 'create', 'room': 'hello'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/created_room          : 10.13.252.5 1749      : {'status': 'ok', 'room': 'hello'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list                  : 10.13.252.5 1749      : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list_rooms            : 10.13.252.5 1749      : {'rooms': ['hello']}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/create                : 10.13.252.5 1749      : {'action': 'create', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/created_room          : 10.13.252.5 1749      : {'status': 'ok', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list                  : 10.13.252.5 1749      : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list_rooms            : 10.13.252.5 1749      : {'rooms': ['hello', 'world']}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/join                  : 10.13.252.5 1749      : {'action': 'join', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/join_room             : 10.13.252.5 1749      : {'status': 'room already joined', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/exit                  : 10.13.252.5 1749      : {'action': 'exit'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Ended request from: 10.13.252.5 port 1749</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Accepted request from: 10.13.252.5 port 1819</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list                  : 10.13.252.5 1819      : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list_rooms            : 10.13.252.5 1819      : {'rooms': ['hello', 'world']}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Accepted request from: 10.13.252.5 port 1829</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list                  : 10.13.252.5 1829      : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list_rooms            : 10.13.252.5 1829      : {'rooms': ['hello', 'world']}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/exit                  : 10.13.252.5 1829      : {'action': 'exit'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Ended request from: 10.13.252.5 port 1829</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/exit                  : 10.13.252.5 1819      : {'action': 'exit'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Ended request from: 10.13.252.5 port 1819</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Example of client side log:</summary><div><div class="collapsibleContent_i85q"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list                  : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list_rooms            : {'rooms': ['hello', 'world']}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/create                : {'action': 'create', 'room': 'room 4'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list                  : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/created_room          : {'status': 'ok', 'room': 'room 4'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Room 'room 4' created successfully.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list_rooms            : {'rooms': ['hello', 'world', 'room 4']}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/join                  : {'action': 'join', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/join_room             : {'status': 'ok', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Joined room 'world' successfully.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/join                  : {'action': 'join', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/join_room             : {'status': 'room already joined', 'room': 'world'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Room already joined.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/create                : {'action': 'create', 'room': 'room 4'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">O/list                  : {'action': 'list'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/created_room          : {'status': 'room already exists', 'room': 'room 4'}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Failed to create room.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">I/list_rooms            : {'rooms': ['hello', 'world', 'room 4']}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>]]></content>
        <category label="Project" term="Project"/>
        <category label="Python" term="Python"/>
        <category label="Networking" term="Networking"/>
        <category label="Real-time Communication" term="Real-time Communication"/>
        <category label="P2P" term="P2P"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: Vision Transformer Analysis]]></title>
        <id>https://ash3327.github.io/blog/blog/vision-transformer-analysis</id>
        <link href="https://ash3327.github.io/blog/blog/vision-transformer-analysis"/>
        <updated>2024-04-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Comparative study of Vision Transformers vs CNNs on small datasets]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/proj-vision-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&amp;color=green" alt="UNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&amp;color=green" alt="ResNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&amp;color=green" alt="ViT" class="img_ev3q"></a>
<a href="https://github.com/facebookresearch/deit" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-DeiT-orange?logo=github&amp;color=orange" alt="DeiT" class="img_ev3q"></a>
<a href="https://github.com/yitu-opensource/T2T-ViT" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-T2T-orange?logo=github&amp;color=orange" alt="T2T" class="img_ev3q"></a>
<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-CIFAR10-blue.svg" alt="Dataset | CIFAR10" class="img_ev3q"></a>
<a href="https://cs.stanford.edu/~acoates/stl10/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-STL10-blue.svg" alt="Dataset | STL10" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes-blue.svg" alt="Dataset | Cityscapes" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Update-April%202024-green.svg" alt="Last Update | April 2024" class="img_ev3q"></p>
<p>This is the final project for the course <strong>AIST4010</strong>. More details on the project can be found in the report. This project is done in April 2024.</p>
<p><strong>Report</strong>: <a href="https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Final%20Report-blue.svg" alt="Report" class="img_ev3q"></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-goals">Project Goals<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#project-goals" class="hash-link" aria-label="Direct link to Project Goals" title="Direct link to Project Goals">​</a></h3>
<p>The project investigates the <strong>generalizability of Vision Transformers (ViTs)</strong> compared to Convolutional Neural Networks (CNNs) for <strong>small-scale computer vision tasks</strong>. While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-contributions">Key Contributions<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#key-contributions" class="hash-link" aria-label="Direct link to Key Contributions" title="Direct link to Key Contributions">​</a></h3>
<ol>
<li><strong>Scalability Analysis</strong>: Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets.</li>
<li><strong>Computational Efficiency</strong>: Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets.</li>
<li><strong>Comparison of Architectures</strong>: Implemented and trained models with similar parameter counts for fair performance evaluations.</li>
</ol>
<p>Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report.</p>
<p>Sections below are not generated by ChatGPT.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="installation">Installation<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h2>
<ol>
<li>Run all commands in <code>commands.txt</code>. Ensure that the CUDA version is <strong>&lt;=11.x</strong>.</li>
</ol>
<blockquote>
<p>[!NOTE]
Execute Jupyter notebooks <strong>from the root folder</strong> of the project to avoid import issues.</p>
</blockquote>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-loading">Data Loading<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#data-loading" class="hash-link" aria-label="Direct link to Data Loading" title="Direct link to Data Loading">​</a></h2>
<p>Download datasets and place them in the <code>data/</code> folder. The structure should match the following diagram:</p>
<img src="https://ash3327.github.io/img/docs/vit/image.png" alt="drawing" height="400">
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="models-used">Models Used<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#models-used" class="hash-link" aria-label="Direct link to Models Used" title="Direct link to Models Used">​</a></h2>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/image3-98190f450375afbd637ff97c3ff80d39.png" width="602" height="155" class="img_ev3q"></p>
<p>The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-results">Experimental Results<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#experimental-results" class="hash-link" aria-label="Direct link to Experimental Results" title="Direct link to Experimental Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalability-performance">Scalability Performance<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#scalability-performance" class="hash-link" aria-label="Direct link to Scalability Performance" title="Direct link to Scalability Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png" width="640" height="480" class="img_ev3q"></p>
<p><strong>Findings:</strong> Transformer-based models perform poorly on small datasets.</p>
<ul>
<li>For models with the same input size, transformer-based models achieve significantly lower accuracy.</li>
<li>The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green).</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="computational-efficiency">Computational Efficiency<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#computational-efficiency" class="hash-link" aria-label="Direct link to Computational Efficiency" title="Direct link to Computational Efficiency">​</a></h3>
<table><thead><tr><th>Against #iterations</th><th>Against time in second-P100</th></tr></thead><tbody><tr><td><img src="https://ash3327.github.io/img/docs/vit/image4.png" width="300"></td><td><img src="https://ash3327.github.io/img/docs/vit/image5.png" width="300"></td></tr></tbody></table>
<p><strong>Findings:</strong> Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets.</p>
<ul>
<li>Note that it is an unfair comparison if we compare all models directly since they don't have the same accuracy.</li>
<li>We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-task">Image Classification Task<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#image-classification-task" class="hash-link" aria-label="Direct link to Image Classification Task" title="Direct link to Image Classification Task">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="file-structure">File Structure<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#file-structure" class="hash-link" aria-label="Direct link to File Structure" title="Direct link to File Structure">​</a></h3>
<p>Relevant code and logs are located in the <code>support/</code> folder.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">support</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ commands</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Commands for running the project.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ main_code</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ipynb       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Main training code.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├─ models</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├─ </span><span class="token operator">&lt;</span><span class="token builtin" style="color:rgb(189, 147, 249)">id</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">dataset</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">model</span><span class="token operator">&gt;</span><span class="token plain">_</span><span class="token operator">&lt;</span><span class="token plain">input_size</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├─ </span><span class="token operator">&lt;</span><span class="token plain">epoch</span><span class="token operator">&gt;</span><span class="token operator">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5          </span><span class="token comment" style="color:rgb(98, 114, 164)"># Trained model.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_accs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png </span><span class="token comment" style="color:rgb(98, 114, 164)"># Accuracy history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_lrs</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">png  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Learning rate history.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├─ model_</span><span class="token operator">&lt;</span><span class="token plain">timestamp</span><span class="token operator">&gt;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">h5_details</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt </span><span class="token comment" style="color:rgb(98, 114, 164)"># Model details.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└─ requirements</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">txt       </span><span class="token comment" style="color:rgb(98, 114, 164)"># Python dependencies.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h3>
<p>In <strong>main_code.ipynb</strong>, users can modify the following cells, following that order:</p>
<ol>
<li>
<p><strong>Cells 1, 3, and 4</strong> immediately below Main Function:</p>
<ul>
<li>Change batch size, image size, patch size (not changed throughout the experiment).</li>
<li>Change data augmentation (train_transform).</li>
<li>Change dataset and “fraction” (proportion of subset).</li>
</ul>
</li>
<li>
<p>The cell that imports <code>torchsummary</code> and those below it, before "Some Other Utility Function":</p>
<ul>
<li>Change the model used and output directory.</li>
<li>Change <code>INITIAL_LR</code>, <code>DECAY</code>, <code>GAMMA</code>, <code>kwargs</code> (arguments for the scheduler).</li>
<li>Change <code>LOAD_PATH</code> (if not None, then the weights <code>&lt;id&gt;.h5</code> will be loaded if the model matches the description).</li>
</ul>
</li>
<li>
<p>The cell immediately after "The Training":</p>
<ul>
<li>Change <code>NUM_EPOCHS</code> and <code>NUM_EPOCHS_TO_SAVE</code>.</li>
</ul>
</li>
</ol>
<p>Then, you can watch the results in the cell that follows the cell in (3). The outputs by default are in the path <code>models/</code>.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-task">Image Segmentation Task<a href="https://ash3327.github.io/blog/blog/vision-transformer-analysis#image-segmentation-task" class="hash-link" aria-label="Direct link to Image Segmentation Task" title="Direct link to Image Segmentation Task">​</a></h2>
<p>This segment explores <strong>UNet-based architectures</strong> for image segmentation tasks. Related code is in the <code>models_archive/</code> folder. Not part of the final report.</p>
<table><tbody><tr><th>mAP</th><th>IoU</th></tr><tr><td><img src="https://ash3327.github.io/img/docs/vit/acc/model_1711376232.h5_accs.png" width="300"></td><td><img src="https://ash3327.github.io/img/docs/vit/acc/model_1711376232.h5_ious.png" width="300"></td></tr></tbody></table>
<p>Resultant output:</p>
<p><img decoding="async" loading="lazy" src="https://ash3327.github.io/blog/assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png" width="630" height="116" class="img_ev3q"></p>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Vision Transformers" term="Vision Transformers"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: ARG Prediction with Transformers]]></title>
        <id>https://ash3327.github.io/blog/blog/arg-prediction-transformers</id>
        <link href="https://ash3327.github.io/blog/blog/arg-prediction-transformers"/>
        <updated>2024-03-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Fine-tuned ProtTrans model for antibiotic resistance gene classification with 0.94 F-score]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-ARG%20Prediction%20with%20Transformers-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p>Fine-tuned ProtTrans model for antibiotic resistance gene classification achieving 0.94 F-score.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/blog/blog/arg-prediction-transformers#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<ul>
<li>
<p>An assignment.</p>
</li>
<li>
<p>Competition Link and Resources: <a href="https://www.kaggle.com/competitions/aist4010-spring2024-a2/leaderboard?tab=public" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/competitions/aist4010-spring2024-a2/leaderboard?tab=public</a></p>
</li>
<li>
<p>This repository is posted just for reference of myself.</p>
</li>
<li>
<p>Code style may not be nice if you're trying to use this as your own reference for learning.</p>
</li>
<li>
<p>Here is the report: <a href="https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer/blob/main/report.pdf" target="_blank" rel="noopener noreferrer">Report</a></p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-achievements">Key Achievements<a href="https://ash3327.github.io/blog/blog/arg-prediction-transformers#key-achievements" class="hash-link" aria-label="Direct link to Key Achievements" title="Direct link to Key Achievements">​</a></h2>
<ul>
<li>0.94 F-score on ARG classification</li>
<li>Fine-tuned ProtTrans model</li>
<li>Robust bioinformatics pipeline</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="technologies-used">Technologies Used<a href="https://ash3327.github.io/blog/blog/arg-prediction-transformers#technologies-used" class="hash-link" aria-label="Direct link to Technologies Used" title="Direct link to Technologies Used">​</a></h2>
<ul>
<li>Transformers</li>
<li>ProtTrans</li>
<li>Bioinformatics tools</li>
<li>Python</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="procedures">Procedures<a href="https://ash3327.github.io/blog/blog/arg-prediction-transformers#procedures" class="hash-link" aria-label="Direct link to Procedures" title="Direct link to Procedures">​</a></h2>
<ol>
<li>
<p>Download the dataset either by direct downloading, or through kaggle by the following:</p>
<p>a. Generate the Kaggle api key from kaggle / accounts / generate api key.</p>
<p>b. Put the kaggle.json generated into the folder specified by the error message generated when you execute <code>kaggle competitions download -c aist4010-spring2024-a2</code>.</p>
<p>c. Execute <code>kaggle competitions download -c aist4010-spring2024-a2</code> in command prompt and unzip the file in any manner. Make sure you unzipped it with root folder containing the directory <code>aist4010-spring2024-a2/data</code>.</p>
<p>*Note: For replacement, you can also place the <code>data/</code> directory from the dataset under the directory <code>(root)/aist4010-spring2024-a2</code>. You may also change the <code>paths</code> variable under the section <code>Parameters and Settings</code>.</p>
</li>
<li>
<p>Execute <code>pip3 install -r requirements.txt</code> in command prompt, and also install PyTorch that matches your needs. You may want to install PyTorch versions compatible with the CUDA and GPU you're using.</p>
</li>
<li>
<p>Open the Jupyter notebook main.ipynb.</p>
<p>a. For the first time of training, you would have to prepare the embeddings by setting <code>LOAD = False</code>. This way, the embeddings generated are placed in the directory <code>(root)/cache</code> or <code>(root)/cache_2</code>. You can then load the embeddings generated by setting <code>LOAD = True</code>.</p>
<p>b. You can change any parameters under the sections with header <code>Parameters and Settings</code>, and you are NOT advised to change any code from other sections. The names of the variables in the section that users can modify should be self-explanatory.</p>
<p>c. Run the code.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="github-repository">GitHub Repository<a href="https://ash3327.github.io/blog/blog/arg-prediction-transformers#github-repository" class="hash-link" aria-label="Direct link to GitHub Repository" title="Direct link to GitHub Repository">​</a></h2>
<p>You can find the complete source code and implementation details on <a href="https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>]]></content>
        <category label="Project" term="Project"/>
        <category label="Transformers" term="Transformers"/>
        <category label="Bioinformatics" term="Bioinformatics"/>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Protein Analysis" term="Protein Analysis"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: YOLO Object Tracking]]></title>
        <id>https://ash3327.github.io/blog/blog/yolo-object-tracking</id>
        <link href="https://ash3327.github.io/blog/blog/yolo-object-tracking"/>
        <updated>2023-06-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Improved instance tracking with custom algorithm from outputs of YOLOv8]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/ObjectDetection-v1" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-YOLO%20Object%20Tracking-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://github.com/ultralytics" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Ultralytics-00875A.svg?logo=ultralytics&amp;logoColor=white" alt="Ultralytics" class="img_ev3q"></a>
<a href="https://github.com/ultralytics/yolov5" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/YOLO-FF69B4.svg?logo=yolo&amp;logoColor=white" alt="YOLO" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Object%20Detection-EE4C2C.svg?logo=object-detection&amp;logoColor=white" alt="Object Detection" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-June%202023-green.svg" alt="Last Updated" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-june-2023">Backup of Old Project (June 2023)<a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#backup-of-old-project-june-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (June 2023)" title="Direct link to Backup of Old Project (June 2023)">​</a></h2>
<p>This is a backup of an old project that focused on object detection and tracking over videos using <strong>YOLOv8</strong>. The project was based on the following tutorials:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=WgPbbWmnXJ8&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI" target="_blank" rel="noopener noreferrer">Murtaza's Workshop - Robotics and AI's Object Detection Tutorial</a></li>
<li><a href="https://www.computervision.zone/courses/object-detection-course/" target="_blank" rel="noopener noreferrer">Computer Vision Zone's Object Detection Course</a></li>
</ul>
<p>The project was written in Python and uses the YOLOv8 object detection model.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="execution">Execution<a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#execution" class="hash-link" aria-label="Direct link to Execution" title="Direct link to Execution">​</a></h2>
<p>Start a new virtual environment:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 -m venv venv</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">source venv/bin/activate # linux or windows WSL</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.\venv\Scripts\activate # windows cmd</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Then install the requirements:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install -r requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Finally, run the jupyter notebook <code>Object Detection.ipynb</code> within the virtual environment.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-project--key-features">About the Project / Key Features<a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#about-the-project--key-features" class="hash-link" aria-label="Direct link to About the Project / Key Features" title="Direct link to About the Project / Key Features">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="static-image-detection-section-32"><strong>Static Image Detection</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.2-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.2" class="img_ev3q"><a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#static-image-detection-section-32" class="hash-link" aria-label="Direct link to static-image-detection-section-32" title="Direct link to static-image-detection-section-32">​</a></h3>
<ul>
<li>Applying YOLOv8 on static image given by path specified in the "Parameters" section.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/image-b1c62bc4e9d1bea54cd9a83744b9b221.png" width="1200" height="703" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="video-detection-section-34"><strong>Video Detection</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.4-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.4" class="img_ev3q"><a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#video-detection-section-34" class="hash-link" aria-label="Direct link to video-detection-section-34" title="Direct link to video-detection-section-34">​</a></h3>
<ul>
<li>Applying YOLOv8 on video given by path specified in the "Parameters" section.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/vid1-24f111c7aa665024b482883cfb9f0491.gif" width="800" height="450" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort"><strong>Instance Tracking with Abrewlay Sort</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.5.1-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.5.1" class="img_ev3q"> <a href="https://github.com/abewley/sort" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Abrewlay%20Sort-007EC6.svg?logo=github&amp;logoColor=white" alt="Abrewlay Sort" class="img_ev3q"></a><a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort" class="hash-link" aria-label="Direct link to instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort" title="Direct link to instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort">​</a></h3>
<ul>
<li>Instance identification with Abrewlay Sort library for tracking objects.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/vid2-1c9c4db30069b35d42478de78c21cf9c.gif" width="800" height="450" class="img_ev3q"></p>
<p><strong>Notes</strong></p>
<ul>
<li>The color of the box indicates the INSTANCE ID</li>
</ul>
<p><strong>Problems</strong></p>
<ol>
<li><strong>Inconsistent IDs</strong>: Occurs under occlusion or label changes<!-- -->
<ul>
<li>Observe that the id of the truck 457 on the rightmost lane (cyan, with label "car") changed to id 473 (purple, with label "truck")</li>
<li>Indicates that the library cannot provide a consistent ID for the same object across frames</li>
</ul>
</li>
<li><strong>Multiple bounding boxes for the same object</strong>
<ul>
<li>As artifacts of the original YOLOv8 detection (due to using the nano model)</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="custom-tracking-algorithm-section-352"><strong>Custom Tracking Algorithm</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.5.2-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.5.2" class="img_ev3q"><a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#custom-tracking-algorithm-section-352" class="hash-link" aria-label="Direct link to custom-tracking-algorithm-section-352" title="Direct link to custom-tracking-algorithm-section-352">​</a></h3>
<ul>
<li>Instance identification with custom-implemented algorithm for tracking objects.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/vid3-95dcb36459fc32eac62ef0ced26f3d9d.gif" width="800" height="450" class="img_ev3q"></p>
<p><strong>Goals</strong></p>
<ul>
<li>Implementing a version of the object tracking algorithm that is more resistent to lost frames and flickering compared to the Abrewlay Sort library.</li>
</ul>
<p><strong>Features</strong></p>
<ol>
<li><strong>Label Accumulation</strong>
<ul>
<li>Can accumulate labels from previous frames.</li>
<li>The ids (colors of the frames) of the objects are more consistent, which can be checked visually</li>
<li>The same truck now got a consistent id (no change in color indicates that)</li>
</ul>
</li>
<li><strong>Bounding Box Merging</strong>
<ul>
<li>Finds the closest bounding box to the previous frames (stored in a dictionary)</li>
<li>Dictionary is cleared after a certain number of frames</li>
<li>Only bounding boxes within a certain change in size and aspect ratio are merged by overwritting the same instance id</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="car-counter-section-36"><strong>Car Counter</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.6-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.6" class="img_ev3q"><a href="https://ash3327.github.io/blog/blog/yolo-object-tracking#car-counter-section-36" class="hash-link" aria-label="Direct link to car-counter-section-36" title="Direct link to car-counter-section-36">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/vid4-d03ca387a586325bd24979201d986374.gif" width="800" height="450" class="img_ev3q"></p>
<ul>
<li>Simple algorithm for counting cars across the line by masking the image before detection.</li>
<li>Utilized the algorithm in sectino 3.5.2 for tracking objects to ensure that a car is not detected twice.</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="YOLO" term="YOLO"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Tracking" term="Tracking"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: GAN Generation]]></title>
        <id>https://ash3327.github.io/blog/blog/gan-generation</id>
        <link href="https://ash3327.github.io/blog/blog/gan-generation"/>
        <updated>2023-06-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[WGAN implementation on MNIST dataset]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/GAN-self-learn-v1" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-GAN%20Generation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/GAN-Generative%20Adversarial%20Networks-blueviolet.svg" alt="Generative Adversarial Networks" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-MNIST-blue.svg" alt="MNIST Dataset" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-August%202022-green.svg" alt="Last Updated: August 2022" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-gan-learning-project-august-2022">Backup of GAN Learning Project (August 2022)<a href="https://ash3327.github.io/blog/blog/gan-generation#backup-of-gan-learning-project-august-2022" class="hash-link" aria-label="Direct link to Backup of GAN Learning Project (August 2022)" title="Direct link to Backup of GAN Learning Project (August 2022)">​</a></h2>
<blockquote>
<p>[!NOTE]
The project explores various GAN architectures and improvements through iterative versions.</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is a personal learning exercise in understanding and implementing different GAN techniques.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="https://ash3327.github.io/blog/blog/gan-generation#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2>
<ul>
<li>The YouTube video series "Generative Adversarial Networks (GANs)", Aladdin Persson, at <a href="https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va" target="_blank" rel="noopener noreferrer">https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setups">Setups<a href="https://ash3327.github.io/blog/blog/gan-generation#setups" class="hash-link" aria-label="Direct link to Setups" title="Direct link to Setups">​</a></h2>
<p><strong>Environment</strong>:</p>
<ul>
<li>Python version: 3.x</li>
<li>Framework: PyTorch</li>
<li>Dataset: MNIST</li>
<li>Runned on Google Colab</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="https://ash3327.github.io/blog/blog/gan-generation#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mnist-digit-generation">MNIST Digit Generation<a href="https://ash3327.github.io/blog/blog/gan-generation#mnist-digit-generation" class="hash-link" aria-label="Direct link to MNIST Digit Generation" title="Direct link to MNIST Digit Generation">​</a></h3>
<p>Explored various GAN architectures:</p>
<ul>
<li>Standard GAN</li>
<li>Wasserstein GAN (WGAN)</li>
<li>Conditional WGAN</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-experiments">Key Experiments<a href="https://ash3327.github.io/blog/blog/gan-generation#key-experiments" class="hash-link" aria-label="Direct link to Key Experiments" title="Direct link to Key Experiments">​</a></h3>
<ul>
<li>Experimented with different learning rates</li>
<li>Observed the phoenomenon of mode collapse and the sensitivity of the GAN architecture to the learning rate</li>
<li>Understanding the architecture of GAN, improvements made by WGAN, and also the principles of providing class conditions to GANs</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-results">Experiment Results<a href="https://ash3327.github.io/blog/blog/gan-generation#experiment-results" class="hash-link" aria-label="Direct link to Experiment Results" title="Direct link to Experiment Results">​</a></h3>
<ol>
<li>
<p>Vanilla GAN
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208011748_GAN_mnist_v2%20good/202208011748_GAN_mnist_v2_final.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v2-blue.svg" alt="Version v2" class="img_ev3q"></a>
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208021155_GAN_mnist_v3%20good%2Cinterupted/202208021155_GAN_mnist_v3.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v3-blue.svg" alt="Version v3" class="img_ev3q"></a>
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208041411_GAN_mnist_v4%20faster%20GAN/202208031401_GAN_mnist_v4_epoch100_ed.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v4-blue.svg" alt="Version v4" class="img_ev3q"></a></p>
<table><tbody><tr><th>v2 (100 epochs)</th><th>v3 (35 epochs *early stopped)</th><th>v4 (100 epochs)</th></tr><tr><td><img src="https://ash3327.github.io/img/docs/gan/v2.png" width="200" height="200"></td><td><img src="https://ash3327.github.io/img/docs/gan/v3.png" width="200" height="200"></td><td><img src="https://ash3327.github.io/img/docs/gan/v4.gif" width="200" height="200"></td></tr></tbody></table>
</li>
<li>
<p>Wasserstein GAN (WGAN)
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208051901_GAN_mnist_v5_WGAN/202208051901_GAN_mnist_v5_WGAN%20epoch100.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v5-blue.svg" alt="Version v5" class="img_ev3q"></a></p>
<table><tbody><tr><th>v5 (100 epochs)</th></tr><tr><td><img src="https://ash3327.github.io/img/docs/gan/v5.gif" width="200" height="200"></td></tr></tbody></table>
</li>
<li>
<p>Conditional Wasserstein GAN (incomplete)
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208061306_GAN_mnist_v6_conditional%20WGAN/202208061306_GAN_mnist_v6_Conditional_WGAN.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v6-blue.svg" alt="Version v6" class="img_ev3q"></a></p>
</li>
</ol>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="GAN" term="GAN"/>
        <category label="Deep Learning" term="Deep Learning"/>
        <category label="Generative Models" term="Generative Models"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: Deep Q-Learning Agent]]></title>
        <id>https://ash3327.github.io/blog/blog/deep-q-learning-agent</id>
        <link href="https://ash3327.github.io/blog/blog/deep-q-learning-agent"/>
        <updated>2022-12-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning agent achieving 30× higher performance in custom Gym environment]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/SnowFight" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Deep%20Q--Learning%20Agent-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB?style=flat&amp;logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://gymnasium.farama.org/index.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Gymnasium-8B9467?style=flat&amp;logo=openai" alt="Gymnasium" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Reinforcement_Learning-00BFFF?style=flat" alt="Reinforcement Learning" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Group_Project-FF9900?style=flat" alt="Group Project" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last_Updated-December_2022-green?style=flat" alt="Last Updated" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<ul>
<li>Created a Gym environment of a simple third-person shooter game in Python</li>
<li>Implemented a simple Deep-Q Network with PyTorch to train agents to master at the game (left image)</li>
<li>Fine-tuned the hyperparameters of the agent, achieving average kill streak of 7 (right image, top) and lengthend the survival duration by 4 times (right image, bottom), which significantly better than the random baseline of 0.22 kills on average.</li>
<li>Explored how deep-Q learning models handle a variable quantity of moving objects, i.e. the bullets and enemies, and relevant adjustments to the reward functions and representations of the observation space needed.</li>
</ul>
<p><strong>Report:</strong>&nbsp;<a href="https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Report-4285F4?style=flat&amp;logo=github&amp;logoColor=white&amp;link=https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf" alt="Report" class="img_ev3q"></a></p>
<img src="https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715" width="300" height="300">
<img src="https://github.com/ash3327/ash3327/assets/86100752/9ac9a3e3-8e36-436c-bbd9-48b80c06e2d6" width="400">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="game-design">Game Design<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#game-design" class="hash-link" aria-label="Direct link to Game Design" title="Direct link to Game Design">​</a></h2>
<ul>
<li>Survive a zombie apocalypse by controlling a snowball-throwing character.</li>
<li>Goal: Survive as long as possible while killing zombies.</li>
<li>Player has limited vision range.</li>
<li>Game ends when a zombie touches the player.</li>
<li>Image: Human Gameplay; Art: Myself; AI training: Myself; Observation and Reward Design: Myself &amp; Jerry; Game Code: Jerry &amp; Myself</li>
</ul>
<img src="https://ash3327.github.io/img/docs/snowfight/gameplay-human-1.gif" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model">Model<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#model" class="hash-link" aria-label="Direct link to Model" title="Direct link to Model">​</a></h2>
<ul>
<li>Deep Q-Network (DQN) model for agent training.</li>
<li>3-layer feedforward neural network in TensorFlow.</li>
<li>Uses experience replay with batch training.</li>
<li>Epsilon-decay strategy for interaction.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2>
<ul>
<li>Tested multiple decay factors (gamma).</li>
<li>Adjusted rewards to improve agent's learning on long-term dependencies.</li>
<li>Achieving an average kill streak of 7 and quadrupling survival time, far surpassing the random baseline of 0.22 kills on average.</li>
</ul>
<img src="https://ash3327.github.io/img/docs/snowfight/results-1.png" width="400" height="400">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-behaviors-1">Notable Behaviors (1)<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#notable-behaviors-1" class="hash-link" aria-label="Direct link to Notable Behaviors (1)" title="Direct link to Notable Behaviors (1)">​</a></h2>
<ul>
<li>AI learns to control its orientation for precise shooting.</li>
<li>AI refines orientation control to improve shooting accuracy.</li>
</ul>
<img src="https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-behaviors-2">Notable Behaviors (2)<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#notable-behaviors-2" class="hash-link" aria-label="Direct link to Notable Behaviors (2)" title="Direct link to Notable Behaviors (2)">​</a></h2>
<ul>
<li>AI learns to evade zombies by retreating to the map corner.</li>
<li>AI retreats to a corner for better firing coverage.</li>
</ul>
<img src="https://ash3327.github.io/img/docs/snowfight/results-2.gif" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-behaviors-3">Notable Behaviors (3)<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#notable-behaviors-3" class="hash-link" aria-label="Direct link to Notable Behaviors (3)" title="Direct link to Notable Behaviors (3)">​</a></h2>
<ul>
<li>AI adopts a spinning and frequent shooting strategy.</li>
<li>AI spins and shoots frequently to maximize hits.</li>
</ul>
<img src="https://ash3327.github.io/img/docs/snowfight/results-3.gif" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-directions">Future Directions<a href="https://ash3327.github.io/blog/blog/deep-q-learning-agent#future-directions" class="hash-link" aria-label="Direct link to Future Directions" title="Direct link to Future Directions">​</a></h2>
<ul>
<li>Further training needed due to time constraints of this project.</li>
<li>Interest in refining rewards and exploring new mechanics in near future.</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="Python" term="Python"/>
        <category label="Gymnasium" term="Gymnasium"/>
        <category label="Reinforcement Learning" term="Reinforcement Learning"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: U-Net Segmentation]]></title>
        <id>https://ash3327.github.io/blog/blog/unet-segmentation</id>
        <link href="https://ash3327.github.io/blog/blog/unet-segmentation"/>
        <updated>2022-08-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[99.55% pixel accuracy on Carvana dataset]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/ImageSegmentation-UNet" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-U--Net%20Segmentation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Image%20Segmentation-red.svg?logo=segmentation&amp;logoColor=white" alt="Image Segmentation" class="img_ev3q">
<a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-December%202023-green.svg" alt="Last Updated: December 2023" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-december-2023">Backup of Old Project (December 2023)<a href="https://ash3327.github.io/blog/blog/unet-segmentation#backup-of-old-project-december-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (December 2023)" title="Direct link to Backup of Old Project (December 2023)">​</a></h2>
<p>This is a backup of an old project focused on training a U-Net model from scratch for semantic segmentation from scratch on the Cityscapes dataset and Carvana dataset. The images are DOWNSCALED to speed up the training process for learning purposes. The model has been trained and tested with the following results:</p>
<blockquote>
<p>[!WARNING]
If you are looking for a high-quality model, this is NOT the place. This is only a practice exercise when I was in year 2.</p>
</blockquote>
<blockquote>
<p>[!NOTE]
Training is done long ago and some parameters recorded, such as the number of epochs, may not be accurate</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is done few years ago from the point of writing this documentation. Back then, the training details are not documented properly and the jupyter notebook results are not saved for each experiment. The results are not excellent either. Sorry for the inconvenience.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setups">Setups<a href="https://ash3327.github.io/blog/blog/unet-segmentation#setups" class="hash-link" aria-label="Direct link to Setups" title="Direct link to Setups">​</a></h2>
<p><strong>Note</strong>:</p>
<ul>
<li><strong>Environment</strong>: Python 3.10.8, PyTorch 2.1.2, CUDA 12.1</li>
<li><strong>Libraries</strong>: See <code>requirements-lock.txt</code></li>
<li><strong>Folder Layout</strong>:<!-- -->
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">data/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  carvana/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    test/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    train_masks/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    val_masks/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  cityscapes/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    gtFine/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    leftImg8Bit/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Data Sources:<!-- -->
<ul>
<li>Carvana: <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<ul>
<li>Download from the above link</li>
<li>Unzip all zips and organize as described above</li>
</ul>
</li>
<li>Cityscapes: <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<ul>
<li>Download the coarse dataset (images and the masks)</li>
<li>Unzip all zips and organize as described above</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Models</strong>:<!-- -->
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">models/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  sem_segmentation/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    1703922437_cityscapes/model_1703922437.h5 # cityscapes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    1703926149/model_1703926149.h5 # carvana</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<!-- -->Download from: <a href="https://drive.google.com/drive/folders/1Mgb_YWV__zsQGNryXGvlOa2EaH49WERe" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Google%20Drive-Models-orange.svg?logo=googledrive&amp;logoColor=white" alt="Google Drive" class="img_ev3q"></a></li>
<li><strong>Training scripts</strong>: Run <code>main_semantic_segmentation_carvana.py</code> and <code>main_semantic_segmentation_cityscape.py</code> as juypter notebooks.</li>
<li><strong>Testing scripts</strong>: Run the <code>test_semantic_segmentation_carvana.py</code> script as jupyter notebook. Specify the paths of the h5 files in cell [8].</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="https://ash3327.github.io/blog/blog/unet-segmentation#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">Carvana Dataset (Carvana Image Masking Challenge Dataset) <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a><a href="https://ash3327.github.io/blog/blog/unet-segmentation#carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" class="hash-link" aria-label="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" title="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">​</a></h3>
<p>Dataset information</p>
<ul>
<li>Original image dimension: 1918 x 1280</li>
<li>Training image dimension: 160 x 240 and 320 x 480</li>
</ul>
<p>Validation accuracies (highest)</p>
<ul>
<li>Validation Pixel Accuracy: <strong>0.9955</strong></li>
<li>Validation Dice Score: <strong>0.9911</strong></li>
</ul>
<h3 align="center"> Results </h3>
<p>Results on test set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/image-041267cdfd77e346a9d0e0d7d422bb48.png" width="520" height="341" class="img_ev3q"></p>
<p>Results on train set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/image2-78057b4210e74b5ab7ccbaae64eb9529.png" width="518" height="341" class="img_ev3q"></p>
<p>Inspection of intermediate layers: (In the order: Prediction, Output of Downsampling Block 1, Output of Bottleneck Block, Output of Upsampling Block 1, Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/blog/assets/images/image4-341eb0e84099144c4c517f3738e0f883.png" width="1008" height="501" class="img_ev3q"></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Details of the experiments</summary><div><div class="collapsibleContent_i85q"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Normalization: mean 0, std 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Augments:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomHorizontalFlip(p=0.5),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomVerticalFlip(p=0.1),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomRotation(degrees=35),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**29/12/2023 17:15: result12_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 5e-7</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: StepLR: step_size=5, gamma=0.85</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 42</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 90.9%, Avg loss: 0.403125, Test recall: 0.5831877589225769, precision: 0.980972170829773</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 08:40: result13_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 50</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 81.0%, Avg loss: 0.775568, Test recall: 0.1010913997888565, precision: 0.9727051258087158</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 11:33: result14_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 96.6%, Avg loss: 0.210099, Test recall: 0.9129086136817932, precision: 0.927651584148407</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 15:47: 1703922437**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 16</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.5%, Avg loss: 0.013175, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9918522238731384, precision: 0.9872172474861145, dice_score: 0.9895293116569519</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 16:49: 1703926149**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 320 x 480</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.6%, Avg loss: 0.009994, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9942919611930847, precision: 0.9879629611968994, dice_score: 0.9911173582077026</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">Cityscapes Dataset (Cityscapes Dataset) <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a><a href="https://ash3327.github.io/blog/blog/unet-segmentation#cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" class="hash-link" aria-label="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" title="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">​</a></h3>
<ul>
<li>Test Pixel Accuracy: <strong>0.8669</strong></li>
<li>The code for evaluating the mAP for the cityscape dataset has been lost. The code in this repository does not reflect the true results.</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">1704279189_cityscapes/model_1704284109.h5: </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**Documented:** </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test acc: 0.8402184247970581, Test loss: 0.6274673556908965</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="Segmentation" term="Segmentation"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
</feed>