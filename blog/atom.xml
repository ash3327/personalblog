<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="atom.xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ash3327.github.io/personalblog/blog</id>
    <title>Sam's Portfolio Blog</title>
    <updated>2025-04-15T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ash3327.github.io/personalblog/blog"/>
    <subtitle>Sam's Portfolio Blog</subtitle>
    <icon>https://ash3327.github.io/personalblog/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Project: General Hand Gesture Recognition]]></title>
        <id>https://ash3327.github.io/personalblog/blog/hand-gesture-recognition</id>
        <link href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition"/>
        <updated>2025-04-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Unified contrastive learning framework for hand gesture recognition with curriculum-based augmentation]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/major-fyp-2024" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Hand%20Gesture%20Recognition-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://github.com/ash3327/major-fyp-2024/blob/rework-1/docs/KTL2401_1155175983_1155174636_final_report_term2.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Report-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="Report" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&amp;logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Computer_Vision-00BFFF?style=flat" alt="Computer Vision" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Contrastive_Learning-FF69B4?style=flat" alt="Contrastive Learning" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Sep%202024-Apr%202025-4285F4?style=flat" alt="Duration" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>This project aims to create a unified, semi-supervised contrastive-learning framework for hand gesture recognition. The framework is designed to adapt efficiently to various downstream tasks, such as human-computer interaction and sign language recognition, with minimal retraining or fine-tuning.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="scope-and-applications">Scope and Applications<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#scope-and-applications" class="hash-link" aria-label="Direct link to Scope and Applications" title="Direct link to Scope and Applications">​</a></h2>
<blockquote>
<p>[!NOTE]
This section is a summary generated from the <a href="https://github.com/ash3327/major-fyp-2024/blob/rework-1/docs/KTL2401_1155175983_1155174636_final_report_term2.pdf" target="_blank" rel="noopener noreferrer">report</a> by Grok. The contents have been double-checked by the author.</p>
<p>Only this section covers the main content of the report and the remaining sections are about the details of setting up the project and the purpose of specific scripts within the repository.</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-areas-explored">Key Areas Explored<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#key-areas-explored" class="hash-link" aria-label="Direct link to Key Areas Explored" title="Direct link to Key Areas Explored">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-pose-representation-learning">Static-Pose Representation Learning<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#static-pose-representation-learning" class="hash-link" aria-label="Direct link to Static-Pose Representation Learning" title="Direct link to Static-Pose Representation Learning">​</a></h4>
<ul>
<li><strong>Objective</strong>: Map hand landmark inputs (shape <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>21</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">21 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">21</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span>) into feature embeddings (size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span>).</li>
<li><strong>Approach</strong>: Compared three encoder architectures:<!-- -->
<ul>
<li>Multi-layer Perceptron (MLP)</li>
<li>Graph Convolutional Network (GCN)</li>
<li>Graph Attention Network (GAT)</li>
</ul>
</li>
<li><strong>Hypotheses Tested</strong>:<!-- -->
<ol>
<li>Graph-based models (GCN and GAT), which leverage edge information, outperform MLP in accuracy and convergence speed. This was evaluated using supervised contrastive loss on the Lexset dataset.</li>
<li>Incorporating a large unlabelled dataset (synthetic MANO data) with curriculum-based augmentations enhances model generalization.</li>
</ol>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="extension-to-dynamic-gesture-recognition">Extension to Dynamic Gesture Recognition<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#extension-to-dynamic-gesture-recognition" class="hash-link" aria-label="Direct link to Extension to Dynamic Gesture Recognition" title="Direct link to Extension to Dynamic Gesture Recognition">​</a></h4>
<ul>
<li><strong>Objective</strong>: Extend the contrastive learning approach to recognize dynamic gestures.</li>
<li><strong>Approach</strong>: Utilize sequential architectures like Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) units to model temporal dependencies in gesture sequences.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h3>
<p>While not fully achieved the original goals, our key findings include:</p>
<ul>
<li>
<p><strong>Static Gesture Recognition</strong>:</p>
<ul>
<li>Graph-based networks (e.g., GCN, GAT) are more effective, leveraging hand skeletal connections for improved accuracy and faster convergence.</li>
<li>Using large unlabelled datasets with curriculum learning enhances model generalization to new datasets and unseen gesture classes (which is tested by observing the cosine similarities of the output feature vectors).<!-- -->
<img src="https://ash3327.github.io/personalblog/img/docs/fyp/curriculum-compare-plot.png" width="600">
</li>
<li>Curriculum Learning with such a shallow model used produces degraded performance when the magnitude of augmentation exceeded a certain value<!-- -->
<img src="https://ash3327.github.io/personalblog/img/docs/fyp/curriculum-compare.png" height="400">
</li>
</ul>
</li>
<li>
<p><strong>Dynamic Gesture Recognition</strong>:</p>
<ul>
<li>Hierarchical and part-wise architectures improve understanding of gesture structures.</li>
<li>Contrastive learning showed limited improvement over existing methods, indicating a need for more complex approaches.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="future-work">Future Work<a href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition#future-work" class="hash-link" aria-label="Direct link to Future Work" title="Direct link to Future Work">​</a></h3>
<ul>
<li>Develop a general hand gesture encoder capturing rotation- and scale-invariant features for rapid adaptation to tasks like dynamic gesture recognition.</li>
<li>Investigate joint training of static and dynamic datasets using curriculum and contrastive learning to improve robustness.</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Contrastive Learning" term="Contrastive Learning"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: Event-Planning App "Oasis"]]></title>
        <id>https://ash3327.github.io/personalblog/blog/oasis-event-planning-app</id>
        <link href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app"/>
        <updated>2024-06-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Android event planning app with robust notification system and SQL database]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/OasisPlanner/tree/development" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Event--Planning%20App%20'Oasis'-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p>Android event planning app written in Java, with the following features:</p>
<ul>
<li>Alarm-setting and Event Organization with SQL via RoomDatabase</li>
<li>Allows the user to add (1) a large number of dates to each task, and (2) switch to the "same day-of-week" view for simple date-picking especially for recurring events</li>
<li>Notification System</li>
</ul>
<img src="https://github.com/ash3327/ash3327/assets/86100752/3548ccde-c41b-440f-af3d-4f35303066e4" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/43f12dbf-4ab5-45ec-9f50-6086b3f7e601" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/82471662-dfac-44f0-bbc5-c06190d2a21e" width="200">
<blockquote>
<p>[!INFO]</p>
<ul>
<li>Distributable version not delivered yet.</li>
<li>The development process is halted to make room for the author to focus on learning ML.</li>
</ul>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="our-goals">Our Goals<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#our-goals" class="hash-link" aria-label="Direct link to Our Goals" title="Direct link to Our Goals">​</a></h2>
<p>Oasis: where you find calm <em>in the past, present, and future.</em></p>
<p>We aim to provide a convenient and simple interface for users to effortlessly jot down and analyze their everyday productive work, without putting too much stress on yourself.</p>
<p>We believe:</p>
<ul>
<li>Visualizing past work done can improve confidence</li>
<li>Striking a balance between work and relaxing is crucial to boosting long-term productivity</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="environment-setup">Environment Setup<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#environment-setup" class="hash-link" aria-label="Direct link to Environment Setup" title="Direct link to Environment Setup">​</a></h2>
<ul>
<li><strong>JDK</strong>: Version 1.8 (Java 8). Download from <a href="https://www.oracle.com/java/technologies/javase-jdk8-downloads.html" target="_blank" rel="noopener noreferrer">Oracle JDK 8</a> or use <a href="https://sdkman.io/" target="_blank" rel="noopener noreferrer">SDKMAN!</a>.</li>
<li><strong>Gradle</strong>: Use the included Gradle Wrapper (v7.3.3). Run <code>./gradlew</code> (Unix) or <code>gradlew.bat</code> (Windows).</li>
<li><strong>OS</strong>: Tested on Windows 11. Should work on macOS/Linux with JDK installed.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="code-maintenance">Code Maintenance<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#code-maintenance" class="hash-link" aria-label="Direct link to Code Maintenance" title="Direct link to Code Maintenance">​</a></h2>
<p><em>Most part of the features are still under development.</em></p>
<ul>
<li><a href="https://github.com/ash3327/OasisPlanner/tree/development/markdowns/code_structure.md" target="_blank" rel="noopener noreferrer">Design Reference on Code Structure</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="design">Design<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#design" class="hash-link" aria-label="Direct link to Design" title="Direct link to Design">​</a></h2>
<ul>
<li><a href="https://github.com/ash3327/OasisPlanner/tree/development/markdowns/first_draft_design_doc.png" target="_blank" rel="noopener noreferrer">Design Document</a></li>
<li>Note: In the deisgn document, some feature designs are copied from existing apps. Such designs will ONLY BE USED AS A BASIC REFERENCE, and the final product WILL NOT include the exact same design.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="our-plan">Our Plan<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#our-plan" class="hash-link" aria-label="Direct link to Our Plan" title="Direct link to Our Plan">​</a></h2>
<ul>
<li>Complete basic functionalities as described in our goals before September 2024.</li>
<li>We are currently working on:<!-- -->
<ul>
<li>Basic Functionalities</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="recent-updates">Recent Updates<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#recent-updates" class="hash-link" aria-label="Direct link to Recent Updates" title="Direct link to Recent Updates">​</a></h2>
<ul>
<li>Our recent update added support to devices of Android version 13+. [16 JUN 2024]</li>
<li>Added "Quick Add" and "Quick Edit" function - users can now add new events with just one click!</li>
<li>Added UI for Home and Projects fragments.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="functionalities">Functionalities<a href="https://ash3327.github.io/personalblog/blog/oasis-event-planning-app#functionalities" class="hash-link" aria-label="Direct link to Functionalities" title="Direct link to Functionalities">​</a></h2>
<ul>
<li>
<p>Preview of work:</p>
<img src="https://github.com/ash3327/ash3327/assets/86100752/3548ccde-c41b-440f-af3d-4f35303066e4" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/73996de9-525e-4c91-a27d-f76b8054de93" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/43f12dbf-4ab5-45ec-9f50-6086b3f7e601" width="200">
<img src="https://github.com/ash3327/ash3327/assets/86100752/82471662-dfac-44f0-bbc5-c06190d2a21e" width="200">
</li>
<li>
<p>Relevant documents will be uploaded later.</p>
</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="Java" term="Java"/>
        <category label="Android" term="Android"/>
        <category label="RoomDB" term="RoomDB"/>
        <category label="Mobile Development" term="Mobile Development"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: P2P Communication App]]></title>
        <id>https://ash3327.github.io/personalblog/blog/p2p-communication-app</id>
        <link href="https://ash3327.github.io/personalblog/blog/p2p-communication-app"/>
        <updated>2024-04-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Real-time audio/video streaming with optimized packet synchronization]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/Peer-to-Peer-Communication-App" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-P2P%20Communication%20App-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://github.com/ash3327/Peer-to-Peer-Communication-App" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/public%20repo-github-%2324292e.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/last%20commit-Apr%202024-%23FFA500.svg?style=for-the-badge&amp;logo=git&amp;logoColor=white" alt="Last Commit" class="img_ev3q"></p>
<p>A fork of the project in the semester 2023-24 Term 2, creating a peer-to-peer communication app supporting audio recording, waveform display and editing, and also screen share function.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-features">Key Features<a href="https://ash3327.github.io/personalblog/blog/p2p-communication-app#key-features" class="hash-link" aria-label="Direct link to Key Features" title="Direct link to Key Features">​</a></h2>
<ul>
<li>Peer-to-peer communication within local area network</li>
<li>Functions:<!-- -->
<ul>
<li>Create, join and leave chat rooms</li>
<li>Audio recording</li>
<li>Screen sharing</li>
</ul>
</li>
<li>Synchronization and handling of audio and video streams from multiple users so that they do not hear their own voices</li>
</ul>
<table><tbody><tr><td>Chatroom Creation &amp; User Alias</td><td>Audio Recording</td></tr><tr><td><img src="https://ash3327.github.io/personalblog/img/docs/p2p/image-1.png" alt="Chatroom Creation &amp; User Alias"></td><td><img src="https://ash3327.github.io/personalblog/img/docs/p2p/image-2.png" alt="Audio Recording"></td></tr><tr><td colspan="2">Screen Sharing</td></tr><tr><td colspan="2"><img src="https://ash3327.github.io/personalblog/img/docs/p2p/image-3.png" alt="Screen Sharing"></td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="https://ash3327.github.io/personalblog/blog/p2p-communication-app#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h2>
<ul>
<li>Consists of ONE server and multiple clients</li>
<li>The server and clients can be runned on the same machine, or on different machines within the same local area network</li>
</ul>
<p>The calls between server and clients, visualized:
<img decoding="async" loading="lazy" src="https://ash3327.github.io/personalblog/assets/images/gui-533c3f5b8d7f0b78f7a4ad1134bc3771.jpeg" width="800" height="504" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notes">Notes<a href="https://ash3327.github.io/personalblog/blog/p2p-communication-app#notes" class="hash-link" aria-label="Direct link to Notes" title="Direct link to Notes">​</a></h2>
<ul>
<li>Functions that allows usage over the internet are not implemented yet due to problems in port forwarding that requires extra care</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="Python" term="Python"/>
        <category label="Networking" term="Networking"/>
        <category label="Real-time Communication" term="Real-time Communication"/>
        <category label="P2P" term="P2P"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: Vision Transformer Analysis]]></title>
        <id>https://ash3327.github.io/personalblog/blog/vision-transformer-analysis</id>
        <link href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis"/>
        <updated>2024-04-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Comparative study of Vision Transformers vs CNNs on small datasets]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/proj-vision-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Vision%20Transformer%20Analysis-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-UNet-green?logo=arxiv&amp;color=green" alt="UNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ResNet-green?logo=arxiv&amp;color=green" alt="ResNet" class="img_ev3q"></a>
<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Paper-ViT-green?logo=arxiv&amp;color=green" alt="ViT" class="img_ev3q"></a>
<a href="https://github.com/facebookresearch/deit" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-DeiT-orange?logo=github&amp;color=orange" alt="DeiT" class="img_ev3q"></a>
<a href="https://github.com/yitu-opensource/T2T-ViT" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Model-T2T-orange?logo=github&amp;color=orange" alt="T2T" class="img_ev3q"></a>
<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-CIFAR10-blue.svg" alt="Dataset | CIFAR10" class="img_ev3q"></a>
<a href="https://cs.stanford.edu/~acoates/stl10/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-STL10-blue.svg" alt="Dataset | STL10" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes-blue.svg" alt="Dataset | Cityscapes" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Update-April%202024-green.svg" alt="Last Update | April 2024" class="img_ev3q"></p>
<p>This is the final project for the course <strong>AIST4010</strong>. More details on the project can be found in the report. This project is done in April 2024.</p>
<p><strong>Report</strong>: <a href="https://github.com/ash3327/proj-vision-transformer/blob/master/project-final-report-1155175983.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Final%20Report-blue.svg" alt="Report" class="img_ev3q"></a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-goals">Project Goals<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#project-goals" class="hash-link" aria-label="Direct link to Project Goals" title="Direct link to Project Goals">​</a></h3>
<p>The project investigates the <strong>generalizability of Vision Transformers (ViTs)</strong> compared to Convolutional Neural Networks (CNNs) for <strong>small-scale computer vision tasks</strong>. While ViTs excel in large datasets, they struggle with smaller ones. This work evaluates and compares the performance of models like ResNet, ViT, DeiT, and T2T-ViT on classification tasks using small subsets of CIFAR-10 and STL-10 datasets.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-contributions">Key Contributions<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#key-contributions" class="hash-link" aria-label="Direct link to Key Contributions" title="Direct link to Key Contributions">​</a></h3>
<ol>
<li><strong>Scalability Analysis</strong>: Demonstrated performance degradation of ViTs with reduced dataset sizes, showing CNNs are more effective for small datasets.</li>
<li><strong>Computational Efficiency</strong>: Analyzed training iterations and time-to-convergence, highlighting that ViTs, while converging faster, still lack efficiency due to lower accuracy on small datasets.</li>
<li><strong>Comparison of Architectures</strong>: Implemented and trained models with similar parameter counts for fair performance evaluations.</li>
</ol>
<p>Note: The above overview is generated by ChatGPT from the project report, which itself is not written by ChatGPT. For more details, please refer to the report.</p>
<p>Sections below are not generated by ChatGPT.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="models-used">Models Used<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#models-used" class="hash-link" aria-label="Direct link to Models Used" title="Direct link to Models Used">​</a></h2>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/image3-98190f450375afbd637ff97c3ff80d39.png" width="602" height="155" class="img_ev3q"></p>
<p>The models used have approximately the same number of parameters. The sources of the models have been provided in both the report and the header of this readme.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-results">Experimental Results<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#experimental-results" class="hash-link" aria-label="Direct link to Experimental Results" title="Direct link to Experimental Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalability-performance">Scalability Performance<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#scalability-performance" class="hash-link" aria-label="Direct link to Scalability Performance" title="Direct link to Scalability Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/image2-00c2236a1b942ee94ceb9b2a7ed7dfc2.png" width="640" height="480" class="img_ev3q"></p>
<p><strong>Findings:</strong> Transformer-based models perform poorly on small datasets.</p>
<ul>
<li>For models with the same input size, transformer-based models achieve significantly lower accuracy.</li>
<li>The accuracy gap widens significantly for input shape 224x224 (the dotted lines) under a decrease of the training set size, where DeiT (red) and T2T-ViT (purple) underperforms the ResNet (green).</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="computational-efficiency">Computational Efficiency<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#computational-efficiency" class="hash-link" aria-label="Direct link to Computational Efficiency" title="Direct link to Computational Efficiency">​</a></h3>
<table><thead><tr><th>Against #iterations</th><th>Against time in second-P100</th></tr></thead><tbody><tr><td><img src="https://ash3327.github.io/personalblog/img/docs/vit/image4.png" width="300"></td><td><img src="https://ash3327.github.io/personalblog/img/docs/vit/image5.png" width="300"></td></tr></tbody></table>
<p><strong>Findings:</strong> Transformer-based models seemed to remain computationally less efficient compared to convolution-based models over significantly small datasets.</p>
<ul>
<li>Note that it is an unfair comparison if we compare all models directly since they don't have the same accuracy.</li>
<li>We can see that DeiT-S with input size 224x224 (red), which have a performance (accuracy) comparable to ResNet-34 with input size 64x64 (orange) while taking significantly more time to converge.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-task">Image Segmentation Task<a href="https://ash3327.github.io/personalblog/blog/vision-transformer-analysis#image-segmentation-task" class="hash-link" aria-label="Direct link to Image Segmentation Task" title="Direct link to Image Segmentation Task">​</a></h2>
<p>This segment explores <strong>UNet-based architectures</strong> for image segmentation tasks. Related code is in the <code>models_archive/</code> folder. Not part of the final report.</p>
<table><tbody><tr><th>mAP</th><th>IoU</th></tr><tr><td><img src="https://ash3327.github.io/personalblog/img/docs/vit/acc/model_1711376232.h5_accs.png" width="300"></td><td><img src="https://ash3327.github.io/personalblog/img/docs/vit/acc/model_1711376232.h5_ious.png" width="300"></td></tr></tbody></table>
<p>Resultant output:</p>
<p><img decoding="async" loading="lazy" src="https://ash3327.github.io/personalblog/assets/images/image7-db5e9bed77228f4a4f62f69fe6623530.png" width="630" height="116" class="img_ev3q"></p>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Vision Transformers" term="Vision Transformers"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: ARG Prediction with Transformers]]></title>
        <id>https://ash3327.github.io/personalblog/blog/arg-prediction-transformers</id>
        <link href="https://ash3327.github.io/personalblog/blog/arg-prediction-transformers"/>
        <updated>2024-03-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Fine-tuned ProtTrans model for antibiotic resistance gene classification with 0.94 F-score]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-ARG%20Prediction%20with%20Transformers-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p>Fine-tuned ProtTrans model for antibiotic resistance gene classification achieving 0.94 F-score.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-achievements">Key Achievements<a href="https://ash3327.github.io/personalblog/blog/arg-prediction-transformers#key-achievements" class="hash-link" aria-label="Direct link to Key Achievements" title="Direct link to Key Achievements">​</a></h2>
<ul>
<li>0.94 F-score on ARG classification</li>
<li>Fine-tuned ProtTrans model</li>
<li>Robust bioinformatics pipeline</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/personalblog/blog/arg-prediction-transformers#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<ul>
<li>
<p>An assignment.</p>
</li>
<li>
<p>Competition Link and Resources: <a href="https://www.kaggle.com/competitions/aist4010-spring2024-a2/leaderboard?tab=public" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/competitions/aist4010-spring2024-a2/leaderboard?tab=public</a></p>
</li>
<li>
<p>This repository is posted just for reference of myself.</p>
</li>
<li>
<p>Code style may not be nice if you're trying to use this as your own reference for learning.</p>
</li>
<li>
<p>Here is the report: <a href="https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer/blob/main/report.pdf" target="_blank" rel="noopener noreferrer">Report</a></p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="technologies-used">Technologies Used<a href="https://ash3327.github.io/personalblog/blog/arg-prediction-transformers#technologies-used" class="hash-link" aria-label="Direct link to Technologies Used" title="Direct link to Technologies Used">​</a></h2>
<ul>
<li>Transformers</li>
<li>ProtTrans</li>
<li>Bioinformatics tools</li>
<li>Python</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="procedures">Procedures<a href="https://ash3327.github.io/personalblog/blog/arg-prediction-transformers#procedures" class="hash-link" aria-label="Direct link to Procedures" title="Direct link to Procedures">​</a></h2>
<ol>
<li>
<p>Download the dataset either by direct downloading, or through kaggle by the following:</p>
<p>a. Generate the Kaggle api key from kaggle / accounts / generate api key.</p>
<p>b. Put the kaggle.json generated into the folder specified by the error message generated when you execute <code>kaggle competitions download -c aist4010-spring2024-a2</code>.</p>
<p>c. Execute <code>kaggle competitions download -c aist4010-spring2024-a2</code> in command prompt and unzip the file in any manner. Make sure you unzipped it with root folder containing the directory <code>aist4010-spring2024-a2/data</code>.</p>
<p>*Note: For replacement, you can also place the <code>data/</code> directory from the dataset under the directory <code>(root)/aist4010-spring2024-a2</code>. You may also change the <code>paths</code> variable under the section <code>Parameters and Settings</code>.</p>
</li>
<li>
<p>Execute <code>pip3 install -r requirements.txt</code> in command prompt, and also install PyTorch that matches your needs. You may want to install PyTorch versions compatible with the CUDA and GPU you're using.</p>
</li>
<li>
<p>Open the Jupyter notebook main.ipynb.</p>
<p>a. For the first time of training, you would have to prepare the embeddings by setting <code>LOAD = False</code>. This way, the embeddings generated are placed in the directory <code>(root)/cache</code> or <code>(root)/cache_2</code>. You can then load the embeddings generated by setting <code>LOAD = True</code>.</p>
<p>b. You can change any parameters under the sections with header <code>Parameters and Settings</code>, and you are NOT advised to change any code from other sections. The names of the variables in the section that users can modify should be self-explanatory.</p>
<p>c. Run the code.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="github-repository">GitHub Repository<a href="https://ash3327.github.io/personalblog/blog/arg-prediction-transformers#github-repository" class="hash-link" aria-label="Direct link to GitHub Repository" title="Direct link to GitHub Repository">​</a></h2>
<p>You can find the complete source code and implementation details on <a href="https://github.com/ash3327/aist4010-coursework-asm2-protein-transformer" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>]]></content>
        <category label="Project" term="Project"/>
        <category label="Transformers" term="Transformers"/>
        <category label="Bioinformatics" term="Bioinformatics"/>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Protein Analysis" term="Protein Analysis"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: YOLO Object Tracking]]></title>
        <id>https://ash3327.github.io/personalblog/blog/yolo-object-tracking</id>
        <link href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking"/>
        <updated>2023-06-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Improved instance tracking with custom algorithm from outputs of YOLOv8]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/ObjectDetection-v1" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-YOLO%20Object%20Tracking-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://github.com/ultralytics" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Ultralytics-00875A.svg?logo=ultralytics&amp;logoColor=white" alt="Ultralytics" class="img_ev3q"></a>
<a href="https://github.com/ultralytics/yolov5" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/YOLO-FF69B4.svg?logo=yolo&amp;logoColor=white" alt="YOLO" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Object%20Detection-EE4C2C.svg?logo=object-detection&amp;logoColor=white" alt="Object Detection" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-June%202023-green.svg" alt="Last Updated" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>This is a backup of an old project that focused on object detection and tracking over videos using <strong>YOLOv8</strong>.</p>
<p>Applied Abrewley's Sort Library and self-implemented instance label assigning function based on maximum overlap to stablize the labelled id on each car moving across frames.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-june-2023">Backup of Old Project (June 2023)<a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#backup-of-old-project-june-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (June 2023)" title="Direct link to Backup of Old Project (June 2023)">​</a></h2>
<p>This is a backup of an old project that focused on object detection and tracking over videos using <strong>YOLOv8</strong>. The project was based on the following tutorials:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=WgPbbWmnXJ8&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI" target="_blank" rel="noopener noreferrer">Murtaza's Workshop - Robotics and AI's Object Detection Tutorial</a></li>
<li><a href="https://www.computervision.zone/courses/object-detection-course/" target="_blank" rel="noopener noreferrer">Computer Vision Zone's Object Detection Course</a></li>
</ul>
<p>The project was written in Python and uses the YOLOv8 object detection model.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-project--key-features">About the Project / Key Features<a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#about-the-project--key-features" class="hash-link" aria-label="Direct link to About the Project / Key Features" title="Direct link to About the Project / Key Features">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="static-image-detection-section-32"><strong>Static Image Detection</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.2-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.2" class="img_ev3q"><a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#static-image-detection-section-32" class="hash-link" aria-label="Direct link to static-image-detection-section-32" title="Direct link to static-image-detection-section-32">​</a></h3>
<ul>
<li>Applying YOLOv8 on static image given by path specified in the "Parameters" section.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/image-b1c62bc4e9d1bea54cd9a83744b9b221.png" width="1200" height="703" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="video-detection-section-34"><strong>Video Detection</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.4-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.4" class="img_ev3q"><a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#video-detection-section-34" class="hash-link" aria-label="Direct link to video-detection-section-34" title="Direct link to video-detection-section-34">​</a></h3>
<ul>
<li>Applying YOLOv8 on video given by path specified in the "Parameters" section.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/vid1-24f111c7aa665024b482883cfb9f0491.gif" width="800" height="450" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort"><strong>Instance Tracking with Abrewlay Sort</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.5.1-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.5.1" class="img_ev3q"> <a href="https://github.com/abewley/sort" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Abrewlay%20Sort-007EC6.svg?logo=github&amp;logoColor=white" alt="Abrewlay Sort" class="img_ev3q"></a><a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort" class="hash-link" aria-label="Direct link to instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort" title="Direct link to instance-tracking-with-abrewlay-sort-section-351-abrewlay-sort">​</a></h3>
<ul>
<li>Instance identification with Abrewlay Sort library for tracking objects.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/vid2-1c9c4db30069b35d42478de78c21cf9c.gif" width="800" height="450" class="img_ev3q"></p>
<p><strong>Notes</strong></p>
<ul>
<li>The color of the box indicates the INSTANCE ID</li>
</ul>
<p><strong>Problems</strong></p>
<ol>
<li><strong>Inconsistent IDs</strong>: Occurs under occlusion or label changes<!-- -->
<ul>
<li>Observe that the id of the truck 457 on the rightmost lane (cyan, with label "car") changed to id 473 (purple, with label "truck")</li>
<li>Indicates that the library cannot provide a consistent ID for the same object across frames</li>
</ul>
</li>
<li><strong>Multiple bounding boxes for the same object</strong>
<ul>
<li>As artifacts of the original YOLOv8 detection (due to using the nano model)</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="custom-tracking-algorithm-section-352"><strong>Custom Tracking Algorithm</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.5.2-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.5.2" class="img_ev3q"><a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#custom-tracking-algorithm-section-352" class="hash-link" aria-label="Direct link to custom-tracking-algorithm-section-352" title="Direct link to custom-tracking-algorithm-section-352">​</a></h3>
<ul>
<li>Instance identification with custom-implemented algorithm for tracking objects.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/vid3-95dcb36459fc32eac62ef0ced26f3d9d.gif" width="800" height="450" class="img_ev3q"></p>
<p><strong>Goals</strong></p>
<ul>
<li>Implementing a version of the object tracking algorithm that is more resistent to lost frames and flickering compared to the Abrewlay Sort library.</li>
</ul>
<p><strong>Features</strong></p>
<ol>
<li><strong>Label Accumulation</strong>
<ul>
<li>Can accumulate labels from previous frames.</li>
<li>The ids (colors of the frames) of the objects are more consistent, which can be checked visually</li>
<li>The same truck now got a consistent id (no change in color indicates that)</li>
</ul>
</li>
<li><strong>Bounding Box Merging</strong>
<ul>
<li>Finds the closest bounding box to the previous frames (stored in a dictionary)</li>
<li>Dictionary is cleared after a certain number of frames</li>
<li>Only bounding boxes within a certain change in size and aspect ratio are merged by overwritting the same instance id</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="car-counter-section-36"><strong>Car Counter</strong> <img decoding="async" loading="lazy" src="https://img.shields.io/badge/Section%203.6-228B22.svg?logo=section&amp;logoColor=white" alt="Section 3.6" class="img_ev3q"><a href="https://ash3327.github.io/personalblog/blog/yolo-object-tracking#car-counter-section-36" class="hash-link" aria-label="Direct link to car-counter-section-36" title="Direct link to car-counter-section-36">​</a></h3>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/vid4-d03ca387a586325bd24979201d986374.gif" width="800" height="450" class="img_ev3q"></p>
<ul>
<li>Simple algorithm for counting cars across the line by masking the image before detection.</li>
<li>Utilized the algorithm in sectino 3.5.2 for tracking objects to ensure that a car is not detected twice.</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="YOLO" term="YOLO"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Tracking" term="Tracking"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: GAN Generation]]></title>
        <id>https://ash3327.github.io/personalblog/blog/gan-generation</id>
        <link href="https://ash3327.github.io/personalblog/blog/gan-generation"/>
        <updated>2023-06-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[WGAN implementation on MNIST dataset]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/GAN-self-learn-v1" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-GAN%20Generation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/GAN-Generative%20Adversarial%20Networks-blueviolet.svg" alt="Generative Adversarial Networks" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-MNIST-blue.svg" alt="MNIST Dataset" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-August%202022-green.svg" alt="Last Updated: August 2022" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-gan-learning-project-august-2022">Backup of GAN Learning Project (August 2022)<a href="https://ash3327.github.io/personalblog/blog/gan-generation#backup-of-gan-learning-project-august-2022" class="hash-link" aria-label="Direct link to Backup of GAN Learning Project (August 2022)" title="Direct link to Backup of GAN Learning Project (August 2022)">​</a></h2>
<blockquote>
<p>[!NOTE]
The project explores various GAN architectures and improvements through iterative versions.</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is a personal learning exercise in understanding and implementing different GAN techniques.</p>
</blockquote>
<p>This project re-implemented GAN, WGAN and conditional GAN and explored the typical problems that occurred with GAN-based architectures like mode collapse and sensitivity to hyperparameters.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="https://ash3327.github.io/personalblog/blog/gan-generation#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2>
<ul>
<li>The YouTube video series "Generative Adversarial Networks (GANs)", Aladdin Persson, at <a href="https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va" target="_blank" rel="noopener noreferrer">https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setups">Setups<a href="https://ash3327.github.io/personalblog/blog/gan-generation#setups" class="hash-link" aria-label="Direct link to Setups" title="Direct link to Setups">​</a></h2>
<p><strong>Environment</strong>:</p>
<ul>
<li>Python version: 3.x</li>
<li>Framework: PyTorch</li>
<li>Dataset: MNIST</li>
<li>Runned on Google Colab</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="https://ash3327.github.io/personalblog/blog/gan-generation#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mnist-digit-generation">MNIST Digit Generation<a href="https://ash3327.github.io/personalblog/blog/gan-generation#mnist-digit-generation" class="hash-link" aria-label="Direct link to MNIST Digit Generation" title="Direct link to MNIST Digit Generation">​</a></h3>
<p>Explored various GAN architectures:</p>
<ul>
<li>Standard GAN</li>
<li>Wasserstein GAN (WGAN)</li>
<li>Conditional WGAN</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-experiments">Key Experiments<a href="https://ash3327.github.io/personalblog/blog/gan-generation#key-experiments" class="hash-link" aria-label="Direct link to Key Experiments" title="Direct link to Key Experiments">​</a></h3>
<ul>
<li>Experimented with different learning rates</li>
<li>Observed the phoenomenon of mode collapse and the sensitivity of the GAN architecture to the learning rate</li>
<li>Understanding the architecture of GAN, improvements made by WGAN, and also the principles of providing class conditions to GANs</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-results">Experiment Results<a href="https://ash3327.github.io/personalblog/blog/gan-generation#experiment-results" class="hash-link" aria-label="Direct link to Experiment Results" title="Direct link to Experiment Results">​</a></h3>
<ol>
<li>
<p>Vanilla GAN
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208011748_GAN_mnist_v2%20good/202208011748_GAN_mnist_v2_final.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v2-blue.svg" alt="Version v2" class="img_ev3q"></a>
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208021155_GAN_mnist_v3%20good%2Cinterupted/202208021155_GAN_mnist_v3.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v3-blue.svg" alt="Version v3" class="img_ev3q"></a>
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208041411_GAN_mnist_v4%20faster%20GAN/202208031401_GAN_mnist_v4_epoch100_ed.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v4-blue.svg" alt="Version v4" class="img_ev3q"></a></p>
<table><tbody><tr><th>v2 (100 epochs)</th><th>v3 (35 epochs *early stopped)</th><th>v4 (100 epochs)</th></tr><tr><td><img src="https://ash3327.github.io/personalblog/img/docs/gan/v2.png" width="200" height="200"></td><td><img src="https://ash3327.github.io/personalblog/img/docs/gan/v3.png" width="200" height="200"></td><td><img src="https://ash3327.github.io/personalblog/img/docs/gan/v4.gif" width="200" height="200"></td></tr></tbody></table>
</li>
<li>
<p>Wasserstein GAN (WGAN)
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208051901_GAN_mnist_v5_WGAN/202208051901_GAN_mnist_v5_WGAN%20epoch100.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v5-blue.svg" alt="Version v5" class="img_ev3q"></a></p>
<table><tbody><tr><th>v5 (100 epochs)</th></tr><tr><td><img src="https://ash3327.github.io/personalblog/img/docs/gan/v5.gif" width="200" height="200"></td></tr></tbody></table>
</li>
<li>
<p>Conditional Wasserstein GAN (incomplete)
<a href="https://github.com/ash3327/GAN-self-learn-v1/blob/main/202208061306_GAN_mnist_v6_conditional%20WGAN/202208061306_GAN_mnist_v6_Conditional_WGAN.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Version-v6-blue.svg" alt="Version v6" class="img_ev3q"></a></p>
</li>
</ol>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="GAN" term="GAN"/>
        <category label="Deep Learning" term="Deep Learning"/>
        <category label="Generative Models" term="Generative Models"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: Deep Q-Learning Agent]]></title>
        <id>https://ash3327.github.io/personalblog/blog/deep-q-learning-agent</id>
        <link href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent"/>
        <updated>2022-12-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning agent achieving 30× higher performance in custom Gym environment]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/SnowFight" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Deep%20Q--Learning%20Agent-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB?style=flat&amp;logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://gymnasium.farama.org/index.html" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Gymnasium-8B9467?style=flat&amp;logo=openai" alt="Gymnasium" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Reinforcement_Learning-00BFFF?style=flat" alt="Reinforcement Learning" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Group_Project-FF9900?style=flat" alt="Group Project" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last_Updated-December_2022-green?style=flat" alt="Last Updated" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<ul>
<li>Created a Gym environment of a simple third-person shooter game in Python</li>
<li>Implemented a simple Deep-Q Network with PyTorch to train agents to master at the game (left image)</li>
<li>Fine-tuned the hyperparameters of the agent, achieving average kill streak of 7 (right image, top) and lengthend the survival duration by 4 times (right image, bottom), which significantly better than the random baseline of 0.22 kills on average.</li>
<li>Explored how deep-Q learning models handle a variable quantity of moving objects, i.e. the bullets and enemies, and relevant adjustments to the reward functions and representations of the observation space needed.</li>
</ul>
<p><strong>Report:</strong>&nbsp;<a href="https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Report-4285F4?style=flat&amp;logo=github&amp;logoColor=white&amp;link=https://github.com/ash3327/SnowFight/blob/master/project%20report%20-%20group%205.pdf" alt="Report" class="img_ev3q"></a></p>
<img src="https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715" width="300" height="300">
<img src="https://github.com/ash3327/ash3327/assets/86100752/9ac9a3e3-8e36-436c-bbd9-48b80c06e2d6" width="400">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="game-design">Game Design<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#game-design" class="hash-link" aria-label="Direct link to Game Design" title="Direct link to Game Design">​</a></h2>
<ul>
<li>Survive a zombie apocalypse by controlling a snowball-throwing character.</li>
<li>Goal: Survive as long as possible while killing zombies.</li>
<li>Player has limited vision range.</li>
<li>Game ends when a zombie touches the player.</li>
<li>Image: Human Gameplay; Art: Myself; AI training: Myself; Observation and Reward Design: Myself &amp; Jerry; Game Code: Jerry &amp; Myself</li>
</ul>
<img src="https://ash3327.github.io/personalblog/img/docs/snowfight/gameplay-human-1.gif" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model">Model<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#model" class="hash-link" aria-label="Direct link to Model" title="Direct link to Model">​</a></h2>
<ul>
<li>Deep Q-Network (DQN) model for agent training.</li>
<li>3-layer feedforward neural network in TensorFlow.</li>
<li>Uses experience replay with batch training.</li>
<li>Epsilon-decay strategy for interaction.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2>
<ul>
<li>Tested multiple decay factors (gamma).</li>
<li>Adjusted rewards to improve agent's learning on long-term dependencies.</li>
<li>Achieving an average kill streak of 7 and quadrupling survival time, far surpassing the random baseline of 0.22 kills on average.</li>
</ul>
<img src="https://ash3327.github.io/personalblog/img/docs/snowfight/results-1.png" width="400" height="400">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-behaviors-1">Notable Behaviors (1)<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#notable-behaviors-1" class="hash-link" aria-label="Direct link to Notable Behaviors (1)" title="Direct link to Notable Behaviors (1)">​</a></h2>
<ul>
<li>AI learns to control its orientation for precise shooting.</li>
<li>AI refines orientation control to improve shooting accuracy.</li>
</ul>
<img src="https://github.com/ash3327/ash3327/assets/86100752/60f36fa1-d6fd-490b-b275-19bb1cbe9715" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-behaviors-2">Notable Behaviors (2)<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#notable-behaviors-2" class="hash-link" aria-label="Direct link to Notable Behaviors (2)" title="Direct link to Notable Behaviors (2)">​</a></h2>
<ul>
<li>AI learns to evade zombies by retreating to the map corner.</li>
<li>AI retreats to a corner for better firing coverage.</li>
</ul>
<img src="https://ash3327.github.io/personalblog/img/docs/snowfight/results-2.gif" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-behaviors-3">Notable Behaviors (3)<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#notable-behaviors-3" class="hash-link" aria-label="Direct link to Notable Behaviors (3)" title="Direct link to Notable Behaviors (3)">​</a></h2>
<ul>
<li>AI adopts a spinning and frequent shooting strategy.</li>
<li>AI spins and shoots frequently to maximize hits.</li>
</ul>
<img src="https://ash3327.github.io/personalblog/img/docs/snowfight/results-3.gif" width="300" height="300">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-directions">Future Directions<a href="https://ash3327.github.io/personalblog/blog/deep-q-learning-agent#future-directions" class="hash-link" aria-label="Direct link to Future Directions" title="Direct link to Future Directions">​</a></h2>
<ul>
<li>Further training needed due to time constraints of this project.</li>
<li>Interest in refining rewards and exploring new mechanics in near future.</li>
</ul>]]></content>
        <category label="Project" term="Project"/>
        <category label="Python" term="Python"/>
        <category label="Gymnasium" term="Gymnasium"/>
        <category label="Reinforcement Learning" term="Reinforcement Learning"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project: U-Net Segmentation]]></title>
        <id>https://ash3327.github.io/personalblog/blog/unet-segmentation</id>
        <link href="https://ash3327.github.io/personalblog/blog/unet-segmentation"/>
        <updated>2022-08-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[99.55% pixel accuracy on Carvana dataset]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/ash3327/ImageSegmentation-UNet" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-U--Net%20Segmentation-4285F4?style=flat&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a></p>
<p><a href="https://www.python.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Python-3776AB.svg?logo=python&amp;logoColor=white" alt="Python" class="img_ev3q"></a>
<a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Artificial%20Intelligence%20(AI)-orange.svg?logo=ai&amp;logoColor=white" alt="Artificial Intelligence (AI)" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Image%20Segmentation-red.svg?logo=segmentation&amp;logoColor=white" alt="Image Segmentation" class="img_ev3q">
<a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a>
<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Last%20Updated-December%202023-green.svg" alt="Last Updated: December 2023" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backup-of-old-project-december-2023">Backup of Old Project (December 2023)<a href="https://ash3327.github.io/personalblog/blog/unet-segmentation#backup-of-old-project-december-2023" class="hash-link" aria-label="Direct link to Backup of Old Project (December 2023)" title="Direct link to Backup of Old Project (December 2023)">​</a></h2>
<p>This is a backup of an old project focused on training a U-Net model from scratch for semantic segmentation from scratch on the Cityscapes dataset and Carvana dataset. The images are DOWNSCALED to speed up the training process for learning purposes.</p>
<p>The model has been trained and tested with the following results:</p>
<blockquote>
<p>[!WARNING]
If you are looking for a high-quality model, this is NOT the place. This is only a practice exercise when I was in year 2.</p>
</blockquote>
<blockquote>
<p>[!NOTE]
Training is done long ago and some parameters recorded, such as the number of epochs, may not be accurate</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]
This project is done few years ago from the point of writing this documentation. Back then, the training details are not documented properly and the jupyter notebook results are not saved for each experiment. The results are not excellent either. Sorry for the inconvenience.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="https://ash3327.github.io/personalblog/blog/unet-segmentation#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">Carvana Dataset (Carvana Image Masking Challenge Dataset) <a href="https://www.kaggle.com/competitions/carvana-image-masking-challenge" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Kaggle-Carvana%20Image%20Masking%20Challenge-blue.svg?logo=kaggle&amp;logoColor=white" alt="Carvana Image Masking Challenge" class="img_ev3q"></a><a href="https://ash3327.github.io/personalblog/blog/unet-segmentation#carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" class="hash-link" aria-label="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge" title="Direct link to carvana-dataset-carvana-image-masking-challenge-dataset-carvana-image-masking-challenge">​</a></h3>
<p>Dataset information</p>
<ul>
<li>Original image dimension: 1918 x 1280</li>
<li>Training image dimension: 160 x 240 and 320 x 480</li>
</ul>
<p>Validation accuracies (highest)</p>
<ul>
<li>Validation Pixel Accuracy: <strong>0.9955</strong></li>
<li>Validation Dice Score: <strong>0.9911</strong></li>
</ul>
<h3 align="center"> Results </h3>
<p>Results on test set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/image-041267cdfd77e346a9d0e0d7d422bb48.png" width="520" height="341" class="img_ev3q"></p>
<p>Results on train set: (Top: Prediction, Bottom: Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/image2-78057b4210e74b5ab7ccbaae64eb9529.png" width="518" height="341" class="img_ev3q"></p>
<p>Inspection of intermediate layers: (In the order: Prediction, Output of Downsampling Block 1, Output of Bottleneck Block, Output of Upsampling Block 1, Reference)</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="https://ash3327.github.io/personalblog/assets/images/image4-341eb0e84099144c4c517f3738e0f883.png" width="1008" height="501" class="img_ev3q"></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Details of the experiments</summary><div><div class="collapsibleContent_i85q"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Normalization: mean 0, std 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Augments:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomHorizontalFlip(p=0.5),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomVerticalFlip(p=0.1),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        transforms.RandomRotation(degrees=35),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**29/12/2023 17:15: result12_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 5e-7</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: StepLR: step_size=5, gamma=0.85</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 42</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 90.9%, Avg loss: 0.403125, Test recall: 0.5831877589225769, precision: 0.980972170829773</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 08:40: result13_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 50</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final: Test accuracy: 81.0%, Avg loss: 0.775568, Test recall: 0.1010913997888565, precision: 0.9727051258087158</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 11:33: result14_**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: ReduceLROnPlateau: patience=5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Epochs: 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 96.6%, Avg loss: 0.210099, Test recall: 0.9129086136817932, precision: 0.927651584148407</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 15:47: 1703922437**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 160 x 240</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 16</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.5%, Avg loss: 0.013175, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9918522238731384, precision: 0.9872172474861145, dice_score: 0.9895293116569519</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**30/12/2023 16:49: 1703926149**</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Downscaled image dimension: 320 x 480</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size: 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate: 1e-4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Decay: None</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loss: BCEWithLogitsLoss</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test accuracy: 99.6%, Avg loss: 0.009994, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test recall: 0.9942919611930847, precision: 0.9879629611968994, dice_score: 0.9911173582077026</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">Cityscapes Dataset (Cityscapes Dataset) <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Dataset-Cityscapes%20Dataset-00BFFF.svg?logo=data:image/png;base64,iVBORw0KGg&amp;logoColor=white" alt="Cityscapes Dataset" class="img_ev3q"></a><a href="https://ash3327.github.io/personalblog/blog/unet-segmentation#cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" class="hash-link" aria-label="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset" title="Direct link to cityscapes-dataset-cityscapes-dataset-cityscapes-dataset">​</a></h3>
<ul>
<li>Test Pixel Accuracy: <strong>0.8669</strong></li>
<li>The code for evaluating the mAP for the cityscape dataset has been lost. The code in this repository does not reflect the true results.</li>
</ul>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">1704279189_cityscapes/model_1704284109.h5: </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">**Documented:** </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test acc: 0.8402184247970581, Test loss: 0.6274673556908965</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>]]></content>
        <category label="Project" term="Project"/>
        <category label="PyTorch" term="PyTorch"/>
        <category label="Segmentation" term="Segmentation"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
</feed>