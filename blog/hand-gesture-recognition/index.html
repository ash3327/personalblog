<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Project: General Hand Gesture Recognition | Sam&#x27;s Portfolio</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Project: General Hand Gesture Recognition | Sam&#x27;s Portfolio"><meta data-rh="true" name="description" content="Unified contrastive learning framework for hand gesture recognition with curriculum-based augmentation"><meta data-rh="true" property="og:description" content="Unified contrastive learning framework for hand gesture recognition with curriculum-based augmentation"><meta data-rh="true" property="og:image" content="https://ash3327.github.io/personalblog/img/docs/fyp/image.png"><meta data-rh="true" name="twitter:image" content="https://ash3327.github.io/personalblog/img/docs/fyp/image.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-04-15T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Project,PyTorch,Computer Vision,Contrastive Learning,Deep Learning"><link data-rh="true" rel="icon" href="/personalblog/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition" hreflang="en"><link data-rh="true" rel="alternate" href="https://ash3327.github.io/personalblog/blog/hand-gesture-recognition" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://ash3327.github.io/personalblog/blog/hand-gesture-recognition","mainEntityOfPage":"https://ash3327.github.io/personalblog/blog/hand-gesture-recognition","url":"https://ash3327.github.io/personalblog/blog/hand-gesture-recognition","headline":"Project: General Hand Gesture Recognition","name":"Project: General Hand Gesture Recognition","description":"Unified contrastive learning framework for hand gesture recognition with curriculum-based augmentation","datePublished":"2025-04-15T00:00:00.000Z","author":[],"image":{"@type":"ImageObject","@id":"https://ash3327.github.io/personalblog/img/docs/fyp/image.png","url":"https://ash3327.github.io/personalblog/img/docs/fyp/image.png","contentUrl":"https://ash3327.github.io/personalblog/img/docs/fyp/image.png","caption":"title image for the blog post: Project: General Hand Gesture Recognition"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://ash3327.github.io/personalblog/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/personalblog/blog/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog/atom.xml" title="Sam&#39;s Portfolio Atom Feed">




<link rel="alternate" type="application/rss+xml" href="/personalblog/blog_old/rss.xml" title="Sam&#39;s Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/personalblog/blog_old/atom.xml" title="Sam&#39;s Portfolio Atom Feed">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"><link rel="stylesheet" href="/personalblog/assets/css/styles.e22aa8d7.css">
<script src="/personalblog/assets/js/runtime~main.13459c8a.js" defer="defer"></script>
<script src="/personalblog/assets/js/main.fe594dd9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/personalblog/img/docs/fyp/curriculum-compare-plot.png"><link rel="preload" as="image" href="/personalblog/img/docs/fyp/curriculum-compare.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/personalblog/"><b class="navbar__title text--truncate">SamKHT</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/personalblog/blog">Blog 博客</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ash3327" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-github fa-brands fa-xl" aria-label="GitHub"></a><a href="https://linkedin.com/in/khtam-51a008256" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link fa-linkedin fa-brands fa-xl" aria-label="LinkedIn"></a><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/personalblog/blog/hand-gesture-recognition">Project: General Hand Gesture Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/ner-project">Project: Named Entity Recognition</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/oasis-event-planning-app">Project: Event-Planning App &quot;Oasis&quot;</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/p2p-communication-app">Project: P2P Communication App</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/vision-transformer-analysis">Project: Vision Transformer Analysis</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/arg-prediction-transformers">Project: ARG Prediction with Transformers</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/unet-segmentation">Project: U-Net Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/yolo-object-tracking">Project: YOLO Object Tracking</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/deep-q-learning-agent">Project: Deep Q-Learning Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/personalblog/blog/gan-generation">Project: GAN Generation</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Project: General Hand Gesture Recognition</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-04-15T00:00:00.000Z">April 15, 2025</time> · <!-- -->3 min read</div></header><div id="__blog-post-container" class="markdown"><p><a href="https://github.com/ash3327/major-fyp-2024" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/View_Project-Hand%20Gesture%20Recognition-4285F4?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="View Project" class="img_ev3q"></a> <a href="https://github.com/ash3327/major-fyp-2024/blob/rework-1/docs/KTL2401_1155175983_1155174636_final_report_term2.pdf" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Report-4285F4?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="Report" class="img_ev3q"></a></p>
<p><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&amp;logo=pytorch&amp;logoColor=white" alt="PyTorch" class="img_ev3q"></a>
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Computer_Vision-00BFFF?style=flat" alt="Computer Vision" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Contrastive_Learning-FF69B4?style=flat" alt="Contrastive Learning" class="img_ev3q">
<img decoding="async" loading="lazy" src="https://img.shields.io/badge/Sep%202024-Apr%202025-4285F4?style=flat" alt="Duration" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>This project aims to create a unified, semi-supervised contrastive-learning framework for hand gesture recognition. The framework is designed to adapt efficiently to various downstream tasks, such as human-computer interaction and sign language recognition, with minimal retraining or fine-tuning.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="scope-and-applications">Scope and Applications<a href="#scope-and-applications" class="hash-link" aria-label="Direct link to Scope and Applications" title="Direct link to Scope and Applications">​</a></h2>
<blockquote>
<p>[!NOTE]
This section is a summary generated from the <a href="https://github.com/ash3327/major-fyp-2024/blob/rework-1/docs/KTL2401_1155175983_1155174636_final_report_term2.pdf" target="_blank" rel="noopener noreferrer">report</a> by Grok. The contents have been double-checked by the author.</p>
<p>Only this section covers the main content of the report and the remaining sections are about the details of setting up the project and the purpose of specific scripts within the repository.</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-areas-explored">Key Areas Explored<a href="#key-areas-explored" class="hash-link" aria-label="Direct link to Key Areas Explored" title="Direct link to Key Areas Explored">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-pose-representation-learning">Static-Pose Representation Learning<a href="#static-pose-representation-learning" class="hash-link" aria-label="Direct link to Static-Pose Representation Learning" title="Direct link to Static-Pose Representation Learning">​</a></h4>
<ul>
<li><strong>Objective</strong>: Map hand landmark inputs (shape <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>21</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">21 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">21</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span>) into feature embeddings (size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span>).</li>
<li><strong>Approach</strong>: Compared three encoder architectures:<!-- -->
<ul>
<li>Multi-layer Perceptron (MLP)</li>
<li>Graph Convolutional Network (GCN)</li>
<li>Graph Attention Network (GAT)</li>
</ul>
</li>
<li><strong>Hypotheses Tested</strong>:<!-- -->
<ol>
<li>Graph-based models (GCN and GAT), which leverage edge information, outperform MLP in accuracy and convergence speed. This was evaluated using supervised contrastive loss on the Lexset dataset.</li>
<li>Incorporating a large unlabelled dataset (synthetic MANO data) with curriculum-based augmentations enhances model generalization.</li>
</ol>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="extension-to-dynamic-gesture-recognition">Extension to Dynamic Gesture Recognition<a href="#extension-to-dynamic-gesture-recognition" class="hash-link" aria-label="Direct link to Extension to Dynamic Gesture Recognition" title="Direct link to Extension to Dynamic Gesture Recognition">​</a></h4>
<ul>
<li><strong>Objective</strong>: Extend the contrastive learning approach to recognize dynamic gestures.</li>
<li><strong>Approach</strong>: Utilize sequential architectures like Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) units to model temporal dependencies in gesture sequences.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h3>
<p>While not fully achieved the original goals, our key findings include:</p>
<ul>
<li>
<p><strong>Static Gesture Recognition</strong>:</p>
<ul>
<li>Graph-based networks (e.g., GCN, GAT) are more effective, leveraging hand skeletal connections for improved accuracy and faster convergence.</li>
<li>Using large unlabelled datasets with curriculum learning enhances model generalization to new datasets and unseen gesture classes (which is tested by observing the cosine similarities of the output feature vectors).<!-- -->
<img src="/personalblog/img/docs/fyp/curriculum-compare-plot.png" width="600">
</li>
<li>Curriculum Learning with such a shallow model used produces degraded performance when the magnitude of augmentation exceeded a certain value<!-- -->
<img src="/personalblog/img/docs/fyp/curriculum-compare.png" height="400">
</li>
</ul>
</li>
<li>
<p><strong>Dynamic Gesture Recognition</strong>:</p>
<ul>
<li>Hierarchical and part-wise architectures improve understanding of gesture structures.</li>
<li>Contrastive learning showed limited improvement over existing methods, indicating a need for more complex approaches.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="future-work">Future Work<a href="#future-work" class="hash-link" aria-label="Direct link to Future Work" title="Direct link to Future Work">​</a></h3>
<ul>
<li>Develop a general hand gesture encoder capturing rotation- and scale-invariant features for rapid adaptation to tasks like dynamic gesture recognition.</li>
<li>Investigate joint training of static and dynamic datasets using curriculum and contrastive learning to improve robustness.</li>
</ul></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/project">Project</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/pytorch">PyTorch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/contrastive-learning">Contrastive Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/personalblog/blog/tags/deep-learning">Deep Learning</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-04-15-hand-gesture-recognition.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/personalblog/blog/ner-project"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Project: Named Entity Recognition</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#scope-and-applications" class="table-of-contents__link toc-highlight">Scope and Applications</a><ul><li><a href="#key-areas-explored" class="table-of-contents__link toc-highlight">Key Areas Explored</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li><li><a href="#future-work" class="table-of-contents__link toc-highlight">Future Work</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Sam K. H. Tam. Built with Docusaurus and assistance of Cursor with Claude.</div></div></div></footer></div>
</body>
</html>